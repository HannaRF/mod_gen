{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFOGAN with metrics\n",
    "\n",
    "* souce: https://github.com/openai/InfoGAN/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting numpy\n",
      "  Using cached numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting pandas\n",
      "  Downloading pandas-2.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
      "Collecting tqdm\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Collecting scipy\n",
      "  Downloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (60 kB)\n",
      "Collecting scikit-learn\n",
      "  Using cached scikit_learn-1.6.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
      "Collecting tensorflow\n",
      "  Downloading tensorflow-2.19.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
      "Collecting pillow\n",
      "  Using cached pillow-11.3.0-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (9.0 kB)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in ./.venv/lib/python3.9/site-packages (from pandas) (2.9.0.post0)\n",
      "Collecting pytz>=2020.1 (from pandas)\n",
      "  Using cached pytz-2025.2-py2.py3-none-any.whl.metadata (22 kB)\n",
      "Collecting tzdata>=2022.7 (from pandas)\n",
      "  Using cached tzdata-2025.2-py2.py3-none-any.whl.metadata (1.4 kB)\n",
      "Collecting joblib>=1.2.0 (from scikit-learn)\n",
      "  Using cached joblib-1.5.1-py3-none-any.whl.metadata (5.6 kB)\n",
      "Collecting threadpoolctl>=3.1.0 (from scikit-learn)\n",
      "  Using cached threadpoolctl-3.6.0-py3-none-any.whl.metadata (13 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow)\n",
      "  Using cached absl_py-2.3.1-py3-none-any.whl.metadata (3.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow)\n",
      "  Using cached flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl.metadata (5.2 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in ./.venv/lib/python3.9/site-packages (from tensorflow) (25.0)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow)\n",
      "  Using cached protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl.metadata (592 bytes)\n",
      "Collecting requests<3,>=2.21.0 (from tensorflow)\n",
      "  Using cached requests-2.32.4-py3-none-any.whl.metadata (4.9 kB)\n",
      "Requirement already satisfied: setuptools in ./.venv/lib/python3.9/site-packages (from tensorflow) (58.1.0)\n",
      "Requirement already satisfied: six>=1.12.0 in ./.venv/lib/python3.9/site-packages (from tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow)\n",
      "  Using cached termcolor-3.1.0-py3-none-any.whl.metadata (6.4 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in ./.venv/lib/python3.9/site-packages (from tensorflow) (4.14.1)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow)\n",
      "  Downloading wrapt-1.17.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.4 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow)\n",
      "  Using cached grpcio-1.73.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (3.8 kB)\n",
      "Collecting tensorboard~=2.19.0 (from tensorflow)\n",
      "  Using cached tensorboard-2.19.0-py3-none-any.whl.metadata (1.8 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow)\n",
      "  Using cached keras-3.10.0-py3-none-any.whl.metadata (6.0 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow)\n",
      "  Downloading h5py-3.14.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (2.7 kB)\n",
      "Collecting ml-dtypes<1.0.0,>=0.5.1 (from tensorflow)\n",
      "  Downloading ml_dtypes-0.5.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (21 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow)\n",
      "  Downloading tensorflow_io_gcs_filesystem-0.37.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\n",
      "Collecting charset_normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached charset_normalizer-3.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (35 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached urllib3-2.5.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow)\n",
      "  Using cached certifi-2025.7.14-py3-none-any.whl.metadata (2.4 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached markdown-3.8.2-py3-none-any.whl.metadata (5.1 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow)\n",
      "  Using cached rich-14.0.0-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow)\n",
      "  Using cached namex-0.1.0-py3-none-any.whl.metadata (322 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow)\n",
      "  Downloading optree-0.16.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (30 kB)\n",
      "Requirement already satisfied: importlib-metadata>=4.4 in ./.venv/lib/python3.9/site-packages (from markdown>=2.6.8->tensorboard~=2.19.0->tensorflow) (8.7.0)\n",
      "Requirement already satisfied: zipp>=3.20 in ./.venv/lib/python3.9/site-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard~=2.19.0->tensorflow) (3.23.0)\n",
      "Collecting MarkupSafe>=2.1.1 (from werkzeug>=1.0.1->tensorboard~=2.19.0->tensorflow)\n",
      "  Using cached MarkupSafe-3.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.0 kB)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in ./.venv/lib/python3.9/site-packages (from rich->keras>=3.5.0->tensorflow) (2.19.2)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached numpy-2.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.5 MB)\n",
      "Downloading pandas-2.3.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m69.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Downloading scipy-1.13.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (38.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m38.6/38.6 MB\u001b[0m \u001b[31m55.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0ma \u001b[36m0:00:01\u001b[0m\n",
      "\u001b[?25hUsing cached scikit_learn-1.6.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.5 MB)\n",
      "Downloading tensorflow-2.19.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (644.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m644.8/644.8 MB\u001b[0m \u001b[31m77.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hUsing cached grpcio-1.73.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (6.0 MB)\n",
      "Downloading ml_dtypes-0.5.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.7 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.7/4.7 MB\u001b[0m \u001b[31m90.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached protobuf-5.29.5-cp38-abi3-manylinux2014_x86_64.whl (319 kB)\n",
      "Using cached requests-2.32.4-py3-none-any.whl (64 kB)\n",
      "Using cached charset_normalizer-3.4.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (149 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached tensorboard-2.19.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-manylinux_2_31_x86_64.whl (6.6 MB)\n",
      "Using cached urllib3-2.5.0-py3-none-any.whl (129 kB)\n",
      "Using cached pillow-11.3.0-cp39-cp39-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (6.6 MB)\n",
      "Using cached absl_py-2.3.1-py3-none-any.whl (135 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Using cached certifi-2025.7.14-py3-none-any.whl (162 kB)\n",
      "Using cached flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Downloading h5py-3.14.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (4.6 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.6/4.6 MB\u001b[0m \u001b[31m89.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached joblib-1.5.1-py3-none-any.whl (307 kB)\n",
      "Using cached keras-3.10.0-py3-none-any.whl (1.4 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-manylinux2010_x86_64.whl (24.5 MB)\n",
      "Using cached markdown-3.8.2-py3-none-any.whl (106 kB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached pytz-2025.2-py2.py3-none-any.whl (509 kB)\n",
      "Downloading tensorflow_io_gcs_filesystem-0.37.1-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.1 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m5.1/5.1 MB\u001b[0m \u001b[31m89.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25hUsing cached termcolor-3.1.0-py3-none-any.whl (7.7 kB)\n",
      "Using cached threadpoolctl-3.6.0-py3-none-any.whl (18 kB)\n",
      "Using cached tzdata-2025.2-py2.py3-none-any.whl (347 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached MarkupSafe-3.0.2-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (20 kB)\n",
      "Downloading wrapt-1.17.2-cp39-cp39-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (82 kB)\n",
      "Using cached namex-0.1.0-py3-none-any.whl (5.9 kB)\n",
      "Downloading optree-0.16.0-cp39-cp39-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (405 kB)\n",
      "Using cached rich-14.0.0-py3-none-any.whl (243 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: pytz, namex, libclang, flatbuffers, wrapt, wheel, urllib3, tzdata, tqdm, threadpoolctl, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, pillow, optree, opt-einsum, numpy, mdurl, MarkupSafe, joblib, idna, grpcio, google-pasta, gast, charset_normalizer, certifi, absl-py, werkzeug, scipy, requests, pandas, ml-dtypes, markdown-it-py, markdown, h5py, astunparse, tensorboard, scikit-learn, rich, keras, tensorflow\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42/42\u001b[0m [tensorflow]nsorflow]ras]-learn]-gcs-filesystem]\n",
      "\u001b[1A\u001b[2KSuccessfully installed MarkupSafe-3.0.2 absl-py-2.3.1 astunparse-1.6.3 certifi-2025.7.14 charset_normalizer-3.4.2 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.73.1 h5py-3.14.0 idna-3.10 joblib-1.5.1 keras-3.10.0 libclang-18.1.1 markdown-3.8.2 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.5.1 namex-0.1.0 numpy-2.0.2 opt-einsum-3.4.0 optree-0.16.0 pandas-2.3.1 pillow-11.3.0 protobuf-5.29.5 pytz-2025.2 requests-2.32.4 rich-14.0.0 scikit-learn-1.6.1 scipy-1.13.1 tensorboard-2.19.0 tensorboard-data-server-0.7.2 tensorflow-2.19.0 tensorflow-io-gcs-filesystem-0.37.1 termcolor-3.1.0 threadpoolctl-3.6.0 tqdm-4.67.1 tzdata-2025.2 urllib3-2.5.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.2\n"
     ]
    }
   ],
   "source": [
    "!pip install numpy pandas tqdm scipy scikit-learn tensorflow pillow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting prettytensor\n",
      "  Using cached prettytensor-0.7.4-py3-none-any.whl.metadata (846 bytes)\n",
      "Collecting enum34>=1.0.0 (from prettytensor)\n",
      "  Using cached enum34-1.1.10-py3-none-any.whl.metadata (1.6 kB)\n",
      "Requirement already satisfied: six>=1.10.0 in ./.venv/lib/python3.9/site-packages (from prettytensor) (1.17.0)\n",
      "Using cached prettytensor-0.7.4-py3-none-any.whl (276 kB)\n",
      "Using cached enum34-1.1.10-py3-none-any.whl (11 kB)\n",
      "Installing collected packages: enum34, prettytensor\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [prettytensor]\n",
      "\u001b[1A\u001b[2KSuccessfully installed enum34-1.1.10 prettytensor-0.7.4\n"
     ]
    }
   ],
   "source": [
    "!pip install prettytensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting progressbar\n",
      "  Using cached progressbar-2.5.tar.gz (10 kB)\n",
      "  Preparing metadata (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25hBuilding wheels for collected packages: progressbar\n",
      "\u001b[33m  DEPRECATION: Building 'progressbar' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'progressbar'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
      "\u001b[0m  Building wheel for progressbar (setup.py) ... \u001b[?25ldone\n",
      "\u001b[?25h  Created wheel for progressbar: filename=progressbar-2.5-py3-none-any.whl size=12081 sha256=28bf7c874df44b1b053664ee6c5316c25962f7a8c1c41e6731746412fa4cd52b\n",
      "  Stored in directory: /home/hanna/.cache/pip/wheels/d7/d9/89/a3f31c76ff6d51dc3b1575628f59afe59e4ceae3f2748cd7ad\n",
      "Successfully built progressbar\n",
      "Installing collected packages: progressbar\n",
      "Successfully installed progressbar-2.5\n"
     ]
    }
   ],
   "source": [
    "!pip install progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: python-dateutil in ./.venv/lib/python3.9/site-packages (2.9.0.post0)\n",
      "Requirement already satisfied: six>=1.5 in ./.venv/lib/python3.9/site-packages (from python-dateutil) (1.17.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install python-dateutil"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sem atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, math, json, errno, time\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.linalg import sqrtm\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from PIL import Image\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Utils ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------\n",
    "TINY = 1e-8\n",
    "floatX = np.float32\n",
    "\n",
    "def mkdir_p(path: str):\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Dataset loader (CelebA sem atributos) --------------------------------------\n",
    "# ----------------------------------------------------------------------------\n",
    "class CelebADataset:\n",
    "    \"\"\"Loader simplificado da CelebA sem atributos.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir: str = '.', image_shape=(64, 64, 3), split_ratio=0.9):\n",
    "        self.image_shape = image_shape\n",
    "        self.image_dim = int(np.prod(image_shape))\n",
    "        base = Path(root_dir)\n",
    "\n",
    "        # Encontra todas as imagens no diretório\n",
    "        img_dir = base / 'img_align_celeba'\n",
    "        sub = img_dir / 'img_align_celeba'\n",
    "        if sub.is_dir():\n",
    "            img_dir = sub\n",
    "            \n",
    "        self.files = sorted([f for f in img_dir.glob('*.jpg')])\n",
    "        \n",
    "        # Split train\n",
    "        n_train = int(len(self.files) * split_ratio)\n",
    "        self.train_idx = np.arange(n_train)\n",
    "        np.random.shuffle(self.train_idx)\n",
    "        self.ptr = 0\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        if self.ptr + batch_size > len(self.train_idx):\n",
    "            np.random.shuffle(self.train_idx)\n",
    "            self.ptr = 0\n",
    "        sel = self.train_idx[self.ptr:self.ptr + batch_size]\n",
    "        self.ptr += batch_size\n",
    "        imgs = []\n",
    "        for i in sel:\n",
    "            img = Image.open(self.files[i]).resize(self.image_shape[:2])\n",
    "            imgs.append(np.asarray(img, np.float32) / 127.5 - 1.0)\n",
    "        x = np.stack(imgs).reshape(batch_size, -1)\n",
    "        return x\n",
    "\n",
    "    def inverse_transform(self, flat):\n",
    "        imgs = flat.reshape((-1,) + self.image_shape)\n",
    "        return ((imgs + 1.) * 127.5).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Distributions (TF-2.x) -----------------------------------------------------\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "TINY = 1e-8\n",
    "floatX = np.float32\n",
    "\n",
    "class Distribution:\n",
    "    \"\"\"Classe base abstrata para distribuições latentes no InfoGAN.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def dim(self) -> int:\n",
    "        \"\"\"Dimensão da variável aleatória (tamanho do vetor de saída).\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @property\n",
    "    def dist_flat_dim(self) -> int:\n",
    "        \"\"\"Dimensão do vetor plano de parâmetros da distribuição.\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @property\n",
    "    def effective_dim(self) -> int:\n",
    "        \"\"\"Dimensão efetiva para cálculo de mutual information.\"\"\"\n",
    "        return self.dim\n",
    "        \n",
    "    def logli(self, x_var: tf.Tensor, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Calcula o log-likelihood log p(x|θ) para cada amostra no batch.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def sample(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Gera amostras da distribuição parametrizada por dist_info.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def sample_prior(self, batch_size: int) -> tf.Tensor:\n",
    "        \"\"\"Gera amostras da distribuição prévia (prior).\"\"\"\n",
    "        return self.sample(self.prior_dist_info(batch_size))\n",
    "        \n",
    "    def prior_dist_info(self, batch_size: int) -> Dict[str, tf.Tensor]:\n",
    "        \"\"\"\n",
    "        Retorna os parâmetros da distribuição prévia (prior).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def entropy(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Calcula a entropia H[p(x|θ)] para cada amostra no batch.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def marginal_entropy(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Calcula a entropia marginal (média sobre o batch).\n",
    "        \"\"\"\n",
    "        # Implementação padrão: média dos parâmetros no batch\n",
    "        avg_dist_info = {\n",
    "            k: tf.tile(tf.reduce_mean(v, axis=0, keepdims=True), \n",
    "            [tf.shape(v)[0], *[1]*(len(v.shape)-1)])\n",
    "            for k, v in dist_info.items()\n",
    "        }\n",
    "        return self.entropy(avg_dist_info)\n",
    "        \n",
    "    def marginal_logli(self, x_var: tf.Tensor, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Log-likelihood usando parâmetros marginais (média no batch).\n",
    "        \"\"\"\n",
    "        avg_dist_info = {\n",
    "            k: tf.tile(tf.reduce_mean(v, axis=0, keepdims=True), \n",
    "            [tf.shape(v)[0], *[1]*(len(v.shape)-1)])\n",
    "            for k, v in dist_info.items()\n",
    "        }\n",
    "        return self.logli(x_var, avg_dist_info)\n",
    "        \n",
    "    def kl(self, p: Dict[str, tf.Tensor], q: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Calcula a divergência KL entre duas distribuições (KL(p||q)).\n",
    "        Implementação padrão usando entropia cruzada e entropia:\n",
    "        KL(p||q) = H(p,q) - H(p)\n",
    "        \"\"\"\n",
    "        cross_entropy = -self.logli(p['samples'], q)\n",
    "        entropy = self.entropy(p)\n",
    "        return cross_entropy - entropy\n",
    "        \n",
    "    def dist_info_keys(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Lista de chaves no dicionário de parâmetros da distribuição.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def activate_dist(self, flat_dist: tf.Tensor) -> Dict[str, tf.Tensor]:\n",
    "        \"\"\"\n",
    "        Converte um vetor plano de parâmetros em parâmetros da distribuição.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def nonreparam_logli(self, x_var: tf.Tensor, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Log-likelihood para distribuições sem reparameterization trick.\n",
    "        \"\"\"\n",
    "        return tf.zeros_like(x_var[:, 0])\n",
    "\n",
    "class Categorical(Distribution):\n",
    "    \"\"\"Distribuição categórica (one-hot) para variáveis latentes discretas.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int):\n",
    "        self._dim = dim\n",
    "        \n",
    "    @property\n",
    "    def dim(self) -> int:\n",
    "        return self._dim\n",
    "        \n",
    "    @property\n",
    "    def dist_flat_dim(self) -> int:\n",
    "        return self._dim\n",
    "        \n",
    "    @property\n",
    "    def effective_dim(self) -> int:\n",
    "        return 1  # Apesar de ter N categorias, a dimensão efetiva é 1\n",
    "        \n",
    "    def logli(self, x_var: tf.Tensor, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        prob = dist_info['prob']\n",
    "        return tf.reduce_sum(x_var * tf.math.log(prob + TINY), axis=1)\n",
    "        \n",
    "    def sample(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        prob = dist_info['prob']\n",
    "        # Gera amostras usando Gumbel-Softmax trick para diferenciabilidade\n",
    "        logits = tf.math.log(prob + TINY)\n",
    "        gumbel_noise = -tf.math.log(-tf.math.log(tf.random.uniform(tf.shape(logits), dtype=floatX)))\n",
    "        samples = tf.nn.softmax((logits + gumbel_noise) / 1.0)  # temperatura=1.0\n",
    "        return samples\n",
    "        \n",
    "    def prior_dist_info(self, batch_size: int) -> Dict[str, tf.Tensor]:\n",
    "        prob = tf.ones((batch_size, self.dim), dtype=floatX) / self.dim\n",
    "        return {'prob': prob}\n",
    "        \n",
    "    def entropy(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        prob = dist_info['prob']\n",
    "        return -tf.reduce_sum(prob * tf.math.log(prob + TINY), axis=1)\n",
    "        \n",
    "    def kl(self, p: Dict[str, tf.Tensor], q: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        p_prob, q_prob = p['prob'], q['prob']\n",
    "        return tf.reduce_sum(p_prob * (tf.math.log(p_prob + TINY) - tf.math.log(q_prob + TINY)), axis=1)\n",
    "        \n",
    "    def dist_info_keys(self) -> List[str]:\n",
    "        return ['prob']\n",
    "        \n",
    "    def activate_dist(self, flat_dist: tf.Tensor) -> Dict[str, tf.Tensor]:\n",
    "        return {'prob': tf.nn.softmax(flat_dist)}\n",
    "\n",
    "class Gaussian(Distribution):\n",
    "    \"\"\"Distribuição gaussiana para variáveis latentes contínuas.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, fix_std: bool = False):\n",
    "        self._dim = dim\n",
    "        self._fix_std = fix_std\n",
    "        \n",
    "    @property\n",
    "    def dim(self) -> int:\n",
    "        return self._dim\n",
    "        \n",
    "    @property\n",
    "    def dist_flat_dim(self) -> int:\n",
    "        return self._dim * 2 if not self._fix_std else self._dim\n",
    "        \n",
    "    def logli(self, x_var: tf.Tensor, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        mean, std = dist_info['mean'], dist_info['stddev']\n",
    "        z = (x_var - mean) / (std + TINY)\n",
    "        return tf.reduce_sum(-0.5 * (np.log(2 * np.pi) + tf.math.log(std + TINY) + 0.5 * tf.square(z)), axis=1)\n",
    "        \n",
    "    def sample(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        mean, std = dist_info['mean'], dist_info['stddev']\n",
    "        return mean + std * tf.random.normal(tf.shape(mean))\n",
    "        \n",
    "    def prior_dist_info(self, batch_size: int) -> Dict[str, tf.Tensor]:\n",
    "        mean = tf.zeros((batch_size, self.dim), dtype=floatX)\n",
    "        std = tf.ones((batch_size, self.dim), dtype=floatX)\n",
    "        return {'mean': mean, 'stddev': std}\n",
    "        \n",
    "    def entropy(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        std = dist_info['stddev']\n",
    "        return tf.reduce_sum(0.5 * np.log(2 * np.pi * np.e) + tf.math.log(std + TINY), axis=1)\n",
    "        \n",
    "    def kl(self, p: Dict[str, tf.Tensor], q: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        p_mean, p_std = p['mean'], p['stddev']\n",
    "        q_mean, q_std = q['mean'], q['stddev']\n",
    "        return tf.reduce_sum(\n",
    "            tf.math.log(q_std + TINY) - tf.math.log(p_std + TINY) + \n",
    "            (tf.square(p_std) + tf.square(p_mean - q_mean)) / (2 * tf.square(q_std + TINY)) - 0.5,\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "    def dist_info_keys(self) -> List[str]:\n",
    "        return ['mean', 'stddev']\n",
    "        \n",
    "    def activate_dist(self, flat_dist: tf.Tensor) -> Dict[str, tf.Tensor]:\n",
    "        mean = flat_dist[:, :self.dim]\n",
    "        if self._fix_std:\n",
    "            std = tf.ones_like(mean)\n",
    "        else:\n",
    "            std = tf.sqrt(tf.exp(flat_dist[:, self.dim:]))\n",
    "        return {'mean': mean, 'stddev': std}\n",
    "\n",
    "class Product(Distribution):\n",
    "    \"\"\"Produto de distribuições para combinar diferentes tipos de variáveis latentes.\"\"\"\n",
    "    \n",
    "    def __init__(self, dists: List[Distribution]):\n",
    "        self._dists = dists\n",
    "        \n",
    "    @property\n",
    "    def dim(self) -> int:\n",
    "        return sum(d.dim for d in self._dists)\n",
    "        \n",
    "    @property\n",
    "    def dist_flat_dim(self) -> int:\n",
    "        return sum(d.dist_flat_dim for d in self._dists)\n",
    "    @property\n",
    "    def dists(self):  \n",
    "        return self._dists\n",
    "    \n",
    "    @property\n",
    "    def effective_dim(self) -> int:\n",
    "        return sum(d.effective_dim for d in self._dists)\n",
    "        \n",
    "    def split_dist_info(self, dist_info: Dict[str, tf.Tensor]) -> List[Dict[str, tf.Tensor]]:\n",
    "        \"\"\"Divide um dicionário de parâmetros combinado em dicionários por distribuição.\"\"\"\n",
    "        split_infos = []\n",
    "        for i, dist in enumerate(self._dists):\n",
    "            info = {}\n",
    "            for key in dist.dist_info_keys():\n",
    "                info[key] = dist_info[f'dist{i}_{key}']\n",
    "            split_infos.append(info)\n",
    "        return split_infos\n",
    "        \n",
    "    def join_dist_infos(self, dist_infos: List[Dict[str, tf.Tensor]]) -> Dict[str, tf.Tensor]:\n",
    "        \"\"\"Combina dicionários de parâmetros em um único dicionário.\"\"\"\n",
    "        joint_info = {}\n",
    "        for i, (dist, info) in enumerate(zip(self._dists, dist_infos)):\n",
    "            for key in dist.dist_info_keys():\n",
    "                joint_info[f'dist{i}_{key}'] = info[key]\n",
    "        return joint_info\n",
    "        \n",
    "    def logli(self, x_var: tf.Tensor, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        split_infos = self.split_dist_info(dist_info)\n",
    "        dims = [d.dim for d in self._dists]\n",
    "        split_x = tf.split(x_var, dims, axis=1)\n",
    "        return tf.add_n([d.logli(x, i) for d, x, i in zip(self._dists, split_x, split_infos)])\n",
    "        \n",
    "    def sample(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        split_infos = self.split_dist_info(dist_info)\n",
    "        samples = [d.sample(i) for d, i in zip(self._dists, split_infos)]\n",
    "        return tf.concat(samples, axis=1)\n",
    "        \n",
    "    def prior_dist_info(self, batch_size: int) -> Dict[str, tf.Tensor]:\n",
    "        dist_infos = [d.prior_dist_info(batch_size) for d in self._dists]\n",
    "        return self.join_dist_infos(dist_infos)\n",
    "        \n",
    "    def entropy(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        split_infos = self.split_dist_info(dist_info)\n",
    "        return tf.add_n([d.entropy(i) for d, i in zip(self._dists, split_infos)])\n",
    "        \n",
    "    def kl(self, p: Dict[str, tf.Tensor], q: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        p_split = self.split_dist_info(p)\n",
    "        q_split = self.split_dist_info(q)\n",
    "        return tf.add_n([d.kl(pi, qi) for d, pi, qi in zip(self._dists, p_split, q_split)])\n",
    "        \n",
    "    def dist_info_keys(self) -> List[str]:\n",
    "        keys = []\n",
    "        for i, dist in enumerate(self._dists):\n",
    "            for key in dist.dist_info_keys():\n",
    "                keys.append(f'dist{i}_{key}')\n",
    "        return keys\n",
    "        \n",
    "    def activate_dist(self, flat_dist: tf.Tensor) -> Dict[str, tf.Tensor]:\n",
    "        dist_infos = []\n",
    "        sizes = [d.dist_flat_dim for d in self._dists]\n",
    "        split_flat = tf.split(flat_dist, sizes, axis=1)\n",
    "        for dist, flat in zip(self._dists, split_flat):\n",
    "            dist_infos.append(dist.activate_dist(flat))\n",
    "        return self.join_dist_infos(dist_infos)\n",
    "\n",
    "    def prior_dist_info(self, batch_size: int) -> Dict[str, tf.Tensor]:\n",
    "        infos = [d.prior_dist_info(batch_size) for d in self._dists]  # Usando _dists\n",
    "        return self.join_dist_infos(infos)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Modelos Keras --------------------------------------------------------------\n",
    "# (Mantido igual ao original)\n",
    "# ----------------------------------------------------------------------------\n",
    "#Gerador DCGAN-64: projeção → reshape → 4 transposed‐convs com batch norm + ReLU, final em tanh.\n",
    "def build_generator(z_dim: int, img_shape):\n",
    "    \"\"\"Gerador DCGAN‑64 clássico (4× upsampling → 64×64).\"\"\"\n",
    "    h, w, c = img_shape  # h==w==64\n",
    "\n",
    "    inp = layers.Input(shape=(z_dim,))\n",
    "\n",
    "    # 1) projeção + reshape → 4×4×512\n",
    "    x = layers.Dense(4 * 4 * 512, use_bias=False)(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Reshape((4, 4, 512))(x)\n",
    "\n",
    "    # 2) 8×8×256\n",
    "    x = layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # 3) 16×16×128\n",
    "    x = layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # 4) 32×32×64\n",
    "    x = layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # 5) 64×64×c\n",
    "    x = layers.Conv2DTranspose(c, kernel_size=4, strides=2, padding='same', activation='tanh')(x)\n",
    "\n",
    "    return Model(inp, x, name='Generator')\n",
    "\n",
    "\n",
    "def build_discriminator_q(img_shape, cat_dim, cont_dim):\n",
    "    inp = layers.Input(shape=img_shape)\n",
    "    x = layers.Conv2D(64, 4, 2, 'same')(inp)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    x = layers.Conv2D(128, 4, 2, 'same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(1024)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    # discriminator\n",
    "    d_out = layers.Dense(1, activation='sigmoid', name='d_out')(x)\n",
    "\n",
    "    # Q‐network: duas saídas\n",
    "    q_cat_logits = layers.Dense(cat_dim, name='q_cat_logits')(x)\n",
    "    q_cont_params = layers.Dense(cont_dim * 2, name='q_cont_params')(x)\n",
    "\n",
    "    return Model(inp, [d_out, q_cat_logits, q_cont_params], name='Discriminator_Q')\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Métricas (Simplificadas - apenas FID) --------------------------------------\n",
    "# ----------------------------------------------------------------------------\n",
    "_inception = tf.keras.applications.InceptionV3(include_top=False, pooling='avg', input_shape=(299, 299, 3))\n",
    "\n",
    "FID_BATCH = 256  # nº de imagens por forward pass no Inception (evita OOM)\n",
    "\n",
    "def _get_inception_activations(img_uint8, bs: int = FID_BATCH):\n",
    "    \"\"\"Extrai ativações do pool-3 da Inception em minibatches para não estourar RAM.\"\"\"\n",
    "    acts = []\n",
    "    for i in range(0, len(img_uint8), bs):\n",
    "        batch = img_uint8[i:i + bs]\n",
    "        batch = tf.image.resize(batch, (299, 299))\n",
    "        batch = tf.keras.applications.inception_v3.preprocess_input(tf.cast(batch, tf.float32))\n",
    "        acts.append(_inception(batch, training=False))\n",
    "    return tf.concat(acts, axis=0).numpy()\n",
    "\n",
    "def fid_np(real_uint8, gen_uint8):\n",
    "    act1, act2 = _get_inception_activations(real_uint8), _get_inception_activations(gen_uint8)\n",
    "    mu1, mu2 = act1.mean(0), act2.mean(0)\n",
    "    sigma1, sigma2 = np.cov(act1, rowvar=False), np.cov(act2, rowvar=False)\n",
    "    covmean = sqrtm(sigma1 @ sigma2)\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    return float(np.sum((mu1 - mu2) ** 2) + np.trace(sigma1 + sigma2 - 2 * covmean))\n",
    "\n",
    "\n",
    "# --- HSIC / MIG / SAP / quase-ortogonal -----------------------------------\n",
    "\n",
    "def _hsic(K, L):\n",
    "    n = K.shape[0]\n",
    "    H = np.eye(n) - np.ones((n, n)) / n\n",
    "    HKH, HLH = H @ K @ H, H @ L @ H\n",
    "    return np.trace(HKH @ HLH) / ((n - 1) ** 2)\n",
    "\n",
    "# def metric_hsic(z: np.ndarray, attr: np.ndarray):\n",
    "#     n, d = z.shape\n",
    "#     hsic_vals = []\n",
    "#     for k in range(d):\n",
    "#         zk = z[:, k:k + 1]\n",
    "#         K = np.exp(-squareform(pdist(zk, 'sqeuclidean')) / (np.median(zk) ** 2 + 1e-8))\n",
    "#         for j in range(attr.shape[1]):\n",
    "#             aj = attr[:, j:j + 1]\n",
    "#             L = (aj == aj.T).astype(np.float32)\n",
    "#             hsic_vals.append(_hsic(K, L))\n",
    "#     return float(np.mean(hsic_vals))\n",
    "\n",
    "def metric_hsic(z: np.ndarray):\n",
    "    # HSIC sem atributos\n",
    "    n, d = z.shape\n",
    "    hsic_vals = []\n",
    "    for k in range(d):\n",
    "        zk = z[:, k:k + 1]\n",
    "        K = np.exp(-squareform(pdist(zk, 'sqeuclidean')) / (np.median(zk) ** 2 + 1e-8))\n",
    "        for j in range(k + 1, d):\n",
    "            zj = z[:, j:j + 1]\n",
    "            L = np.exp(-squareform(pdist(zj, 'sqeuclidean')) / (np.median(zj) ** 2 + 1e-8))\n",
    "            hsic_vals.append(_hsic(K, L))\n",
    "    return float(np.mean(hsic_vals))\n",
    "\n",
    "\n",
    "# def metric_mutual_info(z, attr):\n",
    "#     n_lat = z.shape[1]\n",
    "#     mi = np.zeros((n_lat, attr.shape[1]))\n",
    "#     for i in range(n_lat):\n",
    "#         zi_disc = np.digitize(z[:, i], np.histogram(z[:, i], bins=20)[1][:-1])\n",
    "#         for j in range(attr.shape[1]):\n",
    "#             mi[i, j] = mutual_info_score(zi_disc, attr[:, j])\n",
    "#     return mi\n",
    "\n",
    "# def metric_mig(z, attr):\n",
    "#     mi = metric_mutual_info(z, attr)\n",
    "#     entropy_attr = np.array([mutual_info_score(attr[:, j], attr[:, j]) for j in range(attr.shape[1])])\n",
    "#     sorted_mi = -np.sort(-mi, axis=0)\n",
    "#     gap = (sorted_mi[0] - sorted_mi[1]) / (entropy_attr + 1e-12)\n",
    "#     return float(np.mean(gap))\n",
    "\n",
    "# def metric_sap(z, attr):\n",
    "#     mi = metric_mutual_info(z, attr)\n",
    "#     sorted_mi = -np.sort(-mi, axis=0)\n",
    "#     return float(np.mean(sorted_mi[0] - sorted_mi[1]))\n",
    "\n",
    "def quasi_orthogonality(z):\n",
    "    zc = z - z.mean(0)\n",
    "    cov = np.cov(zc, rowvar=False)\n",
    "    off = cov - np.diag(np.diag(cov))\n",
    "    max_abs = np.max(np.abs(off))\n",
    "    return float(max_abs < 1e-5), float(max_abs)\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# InfoGAN Trainer (Adaptado para dataset sem atributos) -----------------------\n",
    "# ----------------------------------------------------------------------------\n",
    "class InfoGANTrainer:\n",
    "    def __init__(self, G, DQ, latent_dist: Product, dataset: CelebADataset, batch_size=64,\n",
    "             info_coeff=1.0, log_dir='logs', ckpt_dir='ckpt', snapshot=1000, max_iter=100_000,\n",
    "             noise_dim=62, cat_dim=10, cont_dim=2):\n",
    "        self.G, self.DQ = G, DQ\n",
    "        self.latent_dist, self.dataset = latent_dist, dataset\n",
    "        self.bs = batch_size\n",
    "        self.info_coeff = info_coeff\n",
    "        self.snapshot = snapshot\n",
    "        self.max_iter = max_iter\n",
    "        self.noise_dim = noise_dim\n",
    "        self.cat_dim   = cat_dim\n",
    "        self.cont_dim  = cont_dim\n",
    "\n",
    "        self.log_dir, self.ckpt_dir = Path(log_dir), Path(ckpt_dir)\n",
    "        mkdir_p(self.log_dir); mkdir_p(self.ckpt_dir)\n",
    "\n",
    "        self.d_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        self.g_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        self.q_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        \n",
    "        tf.config.optimizer.set_jit(True)  \n",
    "        # tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n",
    "\n",
    "        # Filtra os pesos da Q-head pelo nome\n",
    "        self.q_vars = [\n",
    "            v for v in self.DQ.trainable_variables\n",
    "            if 'q_cat_logits' in v.name or 'q_cont_params' in v.name\n",
    "        ]\n",
    "\n",
    "        # Métricas simplificadas (apenas FID)\n",
    "        self.metric_hist = {'iter': [], 'FID': []}\n",
    "        self.metric_path = self.log_dir / 'metrics.csv'\n",
    "\n",
    "    def _mi_loss(self, z_reg, q_cat_logits, q_cont_params):\n",
    "        # separa z_reg em z_cat (one-hot) e z_cont (gaussiano)\n",
    "        z_cat  = z_reg[:, :self.cat_dim]\n",
    "        z_cont = z_reg[:, self.cat_dim:]\n",
    "\n",
    "        # 1) Cross-entropy (categórico)\n",
    "        cat_loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=z_cat,\n",
    "                logits=q_cat_logits\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 2) Log-likelihood gaussiano (contínuo)\n",
    "        mean_pred    = q_cont_params[:, :self.cont_dim]\n",
    "        log_std_pred = q_cont_params[:, self.cont_dim:]\n",
    "        std_pred     = tf.exp(log_std_pred)\n",
    "\n",
    "        eps = (z_cont - mean_pred) / (std_pred + TINY)\n",
    "        logli_per_dim = -0.5 * (tf.math.log(2. * np.pi) + 2. * log_std_pred + tf.square(eps))\n",
    "        logli = tf.reduce_sum(logli_per_dim, axis=1)\n",
    "        cont_loss = -tf.reduce_mean(logli)  # NLL\n",
    "\n",
    "        return cat_loss + cont_loss\n",
    "\n",
    "    \n",
    "    @tf.function\n",
    "    def _train_step(self, real_imgs):\n",
    "\n",
    "        # 1) Amostra z\n",
    "        z = self.latent_dist.sample_prior(self.bs)\n",
    "\n",
    "        # Atualiza Discriminador\n",
    "        with tf.GradientTape() as d_tape:\n",
    "            fake = self.G(z, training=True)\n",
    "            d_real, q_cat_logits, q_cont_params = self.DQ(real_imgs, training=True)\n",
    "            d_fake, _, _ = self.DQ(fake, training=True)\n",
    "            d_loss = -tf.reduce_mean(tf.math.log(d_real + TINY) + tf.math.log(1 - d_fake + TINY))\n",
    "\n",
    "        # Calcula gradientes apenas para o Discriminador (excluindo Q)\n",
    "        d_vars = [v for v in self.DQ.trainable_variables \n",
    "                  if 'q_cat_logits' not in v.name and 'q_cont_params' not in v.name]\n",
    "        d_grads = d_tape.gradient(d_loss, d_vars)\n",
    "\n",
    "        self.d_opt.apply_gradients(zip(d_grads, d_vars))\n",
    "\n",
    "        # Atualiza Gerador e Rede Q juntos\n",
    "        with tf.GradientTape(persistent=True) as g_tape:\n",
    "            fake = self.G(z, training=True)\n",
    "            d_fake, q_cat_logits, q_cont_params = self.DQ(fake, training=True)\n",
    "\n",
    "            # Loss adversarial\n",
    "            g_adv_loss = -tf.reduce_mean(tf.math.log(d_fake + TINY))\n",
    "\n",
    "            # Loss de informação mútua\n",
    "            z_reg = z[:, self.noise_dim:]\n",
    "            mi_loss = self._mi_loss(z_reg, q_cat_logits, q_cont_params)\n",
    "\n",
    "            g_loss = g_adv_loss + self.info_coeff * mi_loss\n",
    "\n",
    "\n",
    "\n",
    "        # Gradientes para o Gerador\n",
    "        g_grads = g_tape.gradient(g_loss, self.G.trainable_variables)\n",
    "        \n",
    "        self.g_opt.apply_gradients(zip(g_grads, self.G.trainable_variables))\n",
    "\n",
    "        # Gradientes para a Rede Q (apenas MI loss)\n",
    "        q_grads = g_tape.gradient(mi_loss, self.q_vars)\n",
    "        self.q_opt.apply_gradients(zip(q_grads, self.q_vars))\n",
    "\n",
    "        del g_tape  # Importante quando persistent=True\n",
    "\n",
    "        return d_loss, g_adv_loss, mi_loss\n",
    "\n",
    "    def _evaluate_metrics(self):\n",
    "        #Calcula FID (±1 k amostras) + métricas de disentanglement sem estourar memória.\"\"\"\n",
    "        N_EVAL = 1000  # amostras para FID / HSIC / etc.\n",
    "        # real_flat, real_attr = self.dataset.next_batch(N_EVAL)\n",
    "        real_flat = self.dataset.next_batch(N_EVAL)\n",
    "        z = self.latent_dist.sample_prior(N_EVAL)\n",
    "        gen_imgs = self.G(z, training=False).numpy()\n",
    "\n",
    "        fid = fid_np(\n",
    "                self.dataset.inverse_transform(real_flat),\n",
    "                self.dataset.inverse_transform(gen_imgs.reshape(real_flat.shape))\n",
    "            )\n",
    "\n",
    "        z_np = z.numpy()\n",
    "        # hsic = metric_hsic(z_np, real_attr)\n",
    "        # mig = metric_mig(z_np, real_attr)\n",
    "        # sap = metric_sap(z_np, real_attr)\n",
    "        hsic = metric_hsic(z_np)\n",
    "        _, omax = quasi_orthogonality(z_np)\n",
    "        return fid, hsic, omax\n",
    "\n",
    "    def train(self):\n",
    "        for step in tqdm(range(1, self.max_iter + 1)):\n",
    "            real_flat = self.dataset.next_batch(self.bs)\n",
    "            real_imgs = real_flat.reshape((-1,) + self.dataset.image_shape)\n",
    "            d_loss, g_loss, mi = self._train_step(real_imgs)\n",
    "\n",
    "            if step % self.snapshot == 0:\n",
    "                fid, hsic, omax = self._evaluate_metrics()\n",
    "                self.metric_hist['iter'].append(step)\n",
    "                self.metric_hist['FID'].append(fid)\n",
    "                self.metric_hist['HSIC'].append(hsic)\n",
    "                # self.metric_hist['MIG'].append(mig)\n",
    "                # self.metric_hist['SAP'].append(sap)\n",
    "                self.metric_hist['OrthoMax'].append(omax)\n",
    "                pd.DataFrame(self.metric_hist).to_csv(self.metric_path, index=False)\n",
    "                ckpt_path = str(self.ckpt_dir / f'ckpt_{step}')\n",
    "                self.ckpt.save(ckpt_path)\n",
    "                print(f\"Step {step}: FID {fid:.1f} | HSIC {hsic:.4f} | max|off‑diag| {omax:.3e}\")\n",
    " \n",
    "\n",
    "    def generate_samples(self, n_samples: int = 8, output_dir: str = 'samples', prefix: str = 'sample'):\n",
    "        \"\"\"Gera e salva amostras como imagens PNG.\"\"\"\n",
    "        mkdir_p(output_dir)\n",
    "        z = self.latent_dist.sample_prior(n_samples)\n",
    "        gen = self.G(z, training=False).numpy()\n",
    "        imgs_uint8 = self.dataset.inverse_transform(gen.reshape(n_samples, -1))\n",
    "\n",
    "        for i, img in enumerate(imgs_uint8):\n",
    "            path = os.path.join(output_dir, f\"{prefix}_{i}.png\")\n",
    "            Image.fromarray(img).save(path)\n",
    "\n",
    "        print(f\"Geradas e salvas {n_samples} amostras em '{output_dir}/'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300000 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_2943434/2732370642.py\", line 611, in _train_step  *\n        self.q_opt.apply_gradients(zip(q_grads, self.q_vars))\n    File \"/home/hanna/Github/mod_gen/.venv/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\", line 462, in apply_gradients  **\n        \n\n    ValueError: not enough values to unpack (expected 2, got 0)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[28]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     12\u001b[39m data = CelebADataset(\u001b[33m'\u001b[39m\u001b[33mdata/celeba\u001b[39m\u001b[33m'\u001b[39m, IMG_SHAPE)\n\u001b[32m     14\u001b[39m trainer = InfoGANTrainer(G, DQ, latent_dist, data, batch_size=BATCH, max_iter=\u001b[32m300000\u001b[39m, snapshot=\u001b[32m1000\u001b[39m, \n\u001b[32m     15\u001b[39m                              noise_dim=noise_dim,cat_dim=cat_dim, cont_dim=cont_dim)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[27]\u001b[39m\u001b[32m, line 642\u001b[39m, in \u001b[36mInfoGANTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    640\u001b[39m real_flat = \u001b[38;5;28mself\u001b[39m.dataset.next_batch(\u001b[38;5;28mself\u001b[39m.bs)\n\u001b[32m    641\u001b[39m real_imgs = real_flat.reshape((-\u001b[32m1\u001b[39m,) + \u001b[38;5;28mself\u001b[39m.dataset.image_shape)\n\u001b[32m--> \u001b[39m\u001b[32m642\u001b[39m d_loss, g_loss, mi = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_imgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    644\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m step % \u001b[38;5;28mself\u001b[39m.snapshot == \u001b[32m0\u001b[39m:\n\u001b[32m    645\u001b[39m     fid, hsic, omax = \u001b[38;5;28mself\u001b[39m._evaluate_metrics()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/mod_gen/.venv/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    155\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/tmp/__autograph_generated_fileqh2z6b8a.py:30\u001b[39m, in \u001b[36mouter_factory.<locals>.inner_factory.<locals>.tf___train_step\u001b[39m\u001b[34m(self, real_imgs)\u001b[39m\n\u001b[32m     28\u001b[39m ag__.converted_call(ag__.ld(\u001b[38;5;28mself\u001b[39m).g_opt.apply_gradients, (ag__.converted_call(ag__.ld(\u001b[38;5;28mzip\u001b[39m), (ag__.ld(g_grads), ag__.ld(\u001b[38;5;28mself\u001b[39m).G.trainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[32m     29\u001b[39m q_grads = ag__.converted_call(ag__.ld(g_tape).gradient, (ag__.ld(mi_loss), ag__.ld(\u001b[38;5;28mself\u001b[39m).q_vars), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mq_opt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mapply_gradients\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mconverted_call\u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_grads\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mag__\u001b[49m\u001b[43m.\u001b[49m\u001b[43mld\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mq_vars\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfscope\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m g_tape = ag__.Undefined(\u001b[33m'\u001b[39m\u001b[33mg_tape\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     32\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/mod_gen/.venv/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:462\u001b[39m, in \u001b[36mBaseOptimizer.apply_gradients\u001b[39m\u001b[34m(self, grads_and_vars)\u001b[39m\n\u001b[32m    461\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[32m--> \u001b[39m\u001b[32m462\u001b[39m     grads, trainable_variables = \u001b[38;5;28mzip\u001b[39m(*grads_and_vars)\n\u001b[32m    463\u001b[39m     \u001b[38;5;28mself\u001b[39m.apply(grads, trainable_variables)\n\u001b[32m    464\u001b[39m     \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: in user code:\n\n    File \"/tmp/ipykernel_2943434/2732370642.py\", line 611, in _train_step  *\n        self.q_opt.apply_gradients(zip(q_grads, self.q_vars))\n    File \"/home/hanna/Github/mod_gen/.venv/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\", line 462, in apply_gradients  **\n        \n\n    ValueError: not enough values to unpack (expected 2, got 0)\n"
     ]
    }
   ],
   "source": [
    "BATCH = 64\n",
    "IMG_SHAPE = (64, 64, 3)\n",
    "    \n",
    "#Parametros para latente (dimensão do vetor \\Re^(noise_dim + cat_dim + cont_dim))\n",
    "noise_dim, cat_dim, cont_dim = 62, 10, 2\n",
    "    \n",
    "# O espaço latente modelado pelo objeto Product que agrupa três distribuições:\n",
    "latent_dist = Product([Gaussian(noise_dim, fix_std=True),Categorical(cat_dim),Gaussian(cont_dim)])\n",
    "\n",
    "G = build_generator(noise_dim + cat_dim + cont_dim, IMG_SHAPE)\n",
    "DQ = build_discriminator_q(IMG_SHAPE, cat_dim, cont_dim)\n",
    "data = CelebADataset('data/celeba', IMG_SHAPE)\n",
    "\n",
    "trainer = InfoGANTrainer(G, DQ, latent_dist, data, batch_size=BATCH, max_iter=300000, snapshot=1000, \n",
    "                             noise_dim=noise_dim,cat_dim=cat_dim, cont_dim=cont_dim)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
