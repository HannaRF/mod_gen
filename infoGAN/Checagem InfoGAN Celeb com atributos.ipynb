{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFOGAN with metrics\n",
    "\n",
    "* souce: https://github.com/openai/InfoGAN/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.19.0\n",
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 17 03:01:27 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A4000               Off | 00000000:55:00.0  On |                  Off |\n",
      "| 41%   36C    P8              29W / 140W |     54MiB / 16376MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   2814199      G   /usr/lib/xorg/Xorg                           40MiB |\n",
      "|    0   N/A  N/A   2814248      G   /usr/bin/gnome-shell                          8MiB |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Fri_Jan__6_16:45:21_PST_2023\n",
      "Cuda compilation tools, release 12.0, V12.0.140\n",
      "Build cuda_12.0.r12.0/compiler.32267302_0\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!nvcc --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow>=2.13 scipy scikit-learn pandas pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install prettytensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dateutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "InfoGAN em TensorFlow 2 / Keras – versão 2\n",
    "-----------------------------------------\n",
    "Compatível com Python ≥ 3.9 e TensorFlow ≥ 2.13 (eager por default).\n",
    "Principais mudanças em relação ao script original:\n",
    "• Todas as chamadas TF‑1.x (tf.random_normal, tf.multinomial, tf.pack, etc.)\n",
    "  foram substituídas pelas equivalentes em TF‑2.x.\n",
    "• Classes de distribuição (Gaussian, Categorical, Product) portadas para a nova API.\n",
    "• Ajuste no cálculo de MI: em vez de depender de `Product.reg_z`, selecionamos\n",
    "  as últimas dimensões do vetor latente (categorical + continuous).\n",
    "• Pequenas correções antipandas/NumPy para garantir execução em Windows + Anaconda.\n",
    "\n",
    "Requisitos:\n",
    " pip install tensorflow>=2.13 scipy scikit-learn pandas pillow tqdm\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import os, math, json, errno, time\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.linalg import sqrtm\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from PIL import Image\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Utils ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------\n",
    "TINY = 1e-8\n",
    "floatX = np.float32\n",
    "\n",
    "def mkdir_p(path: str):\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Dataset loader (CelebA + atributos) ----------------------------------------\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Dataset loader (CelebA + atributos) ----------------------------------------\n",
    "# ----------------------------------------------------------------------------\n",
    "class CelebAWithAttr:\n",
    "    \"\"\"Loader mínimo da CelebA com 40 atributos binários.\"\"\"\n",
    "\n",
    "    def __init__(self, path: str, image_shape=(64, 64, 3), split_ratio=0.9):\n",
    "        self.image_shape = image_shape\n",
    "        self.image_dim = int(np.prod(image_shape))\n",
    "        self.ptr = 0\n",
    "        \n",
    "        base = Path(path)\n",
    "        # encontra arquivo de atributos\n",
    "        attr_files = list(base.glob('list_attr_celeba.*'))\n",
    "        attr_txt_path = attr_files[0]\n",
    "\n",
    "        with open(attr_txt_path, 'r') as f:\n",
    "            lines = f.readlines()\n",
    "\n",
    "        # Pula a primeira linha e usa a segunda como nomes dos atributos\n",
    "        self.attr_names = lines[1].strip().split()\n",
    "        img_dir = Path(attr_txt_path).parent / 'img_align_celeba'\n",
    "\n",
    "        image_paths = []\n",
    "        attributes = []\n",
    "\n",
    "        for line in lines[2:]:  # Começa da 3ª linha\n",
    "            parts = line.strip().split()\n",
    "            image_path = img_dir / parts[0]\n",
    "            image_paths.append(str(image_path))  # importante: converte Path para string\n",
    "            attributes.append([int(x) for x in parts[1:]])\n",
    "\n",
    "        self.files = np.array(image_paths)\n",
    "        self.attrs = (np.array(attributes) == 1).astype(np.int8)  # binariza: -1 → 0, 1 → 1\n",
    "\n",
    "        # Split de treino\n",
    "        n_total = len(self.files)\n",
    "        n_train = int(n_total * split_ratio)\n",
    "        self.train_idx = np.arange(n_train)\n",
    "        np.random.shuffle(self.train_idx)\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        if self.ptr + batch_size > len(self.train_idx):\n",
    "            np.random.shuffle(self.train_idx)\n",
    "            self.ptr = 0\n",
    "\n",
    "        sel = self.train_idx[self.ptr:self.ptr + batch_size]\n",
    "        self.ptr += batch_size\n",
    "\n",
    "        imgs, atts = [], []\n",
    "        for i in sel:\n",
    "            img = Image.open(self.files[i]).resize(self.image_shape[:2])\n",
    "            img = np.asarray(img, np.float32) / 127.5 - 1.0\n",
    "            imgs.append(img)\n",
    "            atts.append(self.attrs[i])\n",
    "\n",
    "        x = np.stack(imgs).reshape(batch_size, -1)\n",
    "        a = np.stack(atts)\n",
    "        return x, a\n",
    "\n",
    "    def inverse_transform(self, flat):\n",
    "        imgs = flat.reshape((-1,) + self.image_shape)\n",
    "        return ((imgs + 1.) * 127.5).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "# class CelebAWithAttr:\n",
    "#     \"\"\"Loader mínimo da CelebA com 40 atributos binários.\"\"\"\n",
    "\n",
    "#     def __init__(self, root_dir: str = '.', image_shape=(64, 64, 3), split_ratio=0.9):\n",
    "#         self.image_shape = image_shape\n",
    "#         self.image_dim = int(np.prod(image_shape))\n",
    "#         base = Path(root_dir)\n",
    "\n",
    "#         # encontra arquivo de atributos\n",
    "#         attr_files = list(base.glob('list_attr_celeba.*'))\n",
    "#         attr_path = attr_files[0]\n",
    "\n",
    "#         # leitura com pandas (auto-separador)\n",
    "#         df = pd.read_csv(attr_path, sep=' ',skiprows=1, engine='python')\n",
    "#         fname_col = df.columns[0]\n",
    "        \n",
    "#         self.attr_names = [c for c in df.columns if c != fname_col]\n",
    "\n",
    "#         img_dir = base / 'img_align_celeba'\n",
    "#         sub = img_dir / 'img_align_celeba'\n",
    "#         if sub.is_dir():\n",
    "#             img_dir = sub\n",
    "\n",
    "#         self.files = df[fname_col].apply(lambda fn: img_dir / fn).values\n",
    "#         attrs = df[self.attr_names].replace(-1, 0).values.astype(np.int8)\n",
    "#         self.attrs = attrs\n",
    "\n",
    "#         # split train\n",
    "#         n_train = int(len(self.files) * split_ratio)\n",
    "#         self.train_idx = np.arange(n_train)\n",
    "#         np.random.shuffle(self.train_idx)\n",
    "#         self.ptr = 0\n",
    "\n",
    "#     def next_batch(self, batch_size):\n",
    "#         if self.ptr + batch_size > len(self.train_idx):\n",
    "#             np.random.shuffle(self.train_idx)\n",
    "#             self.ptr = 0\n",
    "#         sel = self.train_idx[self.ptr:self.ptr + batch_size]\n",
    "#         self.ptr += batch_size\n",
    "#         imgs, atts = [], []\n",
    "#         for i in sel:\n",
    "#             img = Image.open(self.files[i]).resize(self.image_shape[:2])\n",
    "#             imgs.append(np.asarray(img, np.float32) / 127.5 - 1.0)\n",
    "#             atts.append(self.attrs[i])\n",
    "#         x = np.stack(imgs).reshape(batch_size, -1)\n",
    "#         a = np.stack(atts)\n",
    "#         return x, a\n",
    "\n",
    "#     def inverse_transform(self, flat):\n",
    "#         imgs = flat.reshape((-1,) + self.image_shape)\n",
    "#         return ((imgs + 1.) * 127.5).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Distributions (TF‑2.x) -----------------------------------------------------\n",
    "# ----------------------------------------------------------------------------\n",
    "class Distribution:\n",
    "    @property\n",
    "    def dist_flat_dim(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def effective_dim(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def logli(self, x_var, dist_info):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, dist_info):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample_prior(self, batch_size):\n",
    "        return self.sample(self.prior_dist_info(batch_size))\n",
    "\n",
    "    def prior_dist_info(self, batch_size):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # helpers usados pelo InfoGAN ------------------------------------------\n",
    "    def entropy(self, dist_info):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def marginal_entropy(self, dist_info):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def marginal_logli(self, x_var, dist_info):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def kl(self, p, q):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def dist_info_keys(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def activate_dist(self, flat_dist):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Categorical(Distribution):\n",
    "    def __init__(self, dim: int):\n",
    "        self._dim = dim\n",
    "\n",
    "    # --- propriedades ------------------------------------------------------\n",
    "    @property\n",
    "    def dim(self): return self._dim\n",
    "    @property\n",
    "    def dist_flat_dim(self): return self._dim\n",
    "    @property\n",
    "    def effective_dim(self): return 1\n",
    "    @property\n",
    "    def dist_info_keys(self): return ['prob']\n",
    "\n",
    "    # --- likelihood / KL ---------------------------------------------------\n",
    "    def logli(self, x_var, dist_info):\n",
    "        prob = dist_info['prob']\n",
    "        return tf.reduce_sum(tf.math.log(prob + TINY) * x_var, axis=1)\n",
    "\n",
    "    def kl(self, p, q):\n",
    "        p_prob, q_prob = p['prob'], q['prob']\n",
    "        return tf.reduce_sum(p_prob * (tf.math.log(p_prob + TINY) - tf.math.log(q_prob + TINY)), axis=1)\n",
    "\n",
    "    # --- sampling ----------------------------------------------------------\n",
    "    def sample(self, dist_info):\n",
    "        prob = dist_info['prob']\n",
    "        ids = tf.random.categorical(tf.math.log(prob + TINY), 1)[:, 0]\n",
    "        onehot = tf.eye(self.dim, dtype=tf.float32)\n",
    "        return tf.nn.embedding_lookup(onehot, ids)\n",
    "\n",
    "    def sample_prior(self, batch_size):\n",
    "        prob = tf.ones([batch_size, self.dim], dtype=floatX) / self.dim\n",
    "        return self.sample(dict(prob=prob))\n",
    "\n",
    "    # --- helpers -----------------------------------------------------------\n",
    "    def activate_dist(self, flat_dist):\n",
    "        return {'prob': tf.nn.softmax(flat_dist)}\n",
    "\n",
    "    def entropy(self, dist_info):\n",
    "        prob = dist_info['prob']\n",
    "        return -tf.reduce_sum(prob * tf.math.log(prob + TINY), axis=1)\n",
    "\n",
    "    def marginal_entropy(self, dist_info):\n",
    "        prob = dist_info['prob']\n",
    "        avg_prob = tf.tile(tf.reduce_mean(prob, axis=0, keepdims=True), [tf.shape(prob)[0], 1])\n",
    "        return self.entropy({'prob': avg_prob})\n",
    "\n",
    "    def marginal_logli(self, x_var, dist_info):\n",
    "        prob = dist_info['prob']\n",
    "        avg_prob = tf.tile(tf.reduce_mean(prob, axis=0, keepdims=True), [tf.shape(prob)[0], 1])\n",
    "        return self.logli(x_var, {'prob': avg_prob})\n",
    "\n",
    "    def nonreparam_logli(self, x_var, dist_info):\n",
    "        return self.logli(x_var, dist_info)\n",
    "\n",
    "\n",
    "class Gaussian(Distribution):\n",
    "    def __init__(self, dim: int, fix_std: bool = False):\n",
    "        self._dim = dim\n",
    "        self._fix_std = fix_std\n",
    "\n",
    "    @property\n",
    "    def dim(self): return self._dim\n",
    "    @property\n",
    "    def dist_flat_dim(self): return self._dim * 2\n",
    "    @property\n",
    "    def effective_dim(self): return self._dim\n",
    "    @property\n",
    "    def dist_info_keys(self): return ['mean', 'stddev']\n",
    "\n",
    "    # --- likelihood / KL ---------------------------------------------------\n",
    "    def logli(self, x_var, dist_info):\n",
    "        mean, std = dist_info['mean'], dist_info['stddev']\n",
    "        eps = (x_var - mean) / (std + TINY)\n",
    "        return tf.reduce_sum(-0.5 * np.log(2 * np.pi) - tf.math.log(std + TINY) - 0.5 * tf.square(eps), axis=1)\n",
    "\n",
    "    def kl(self, p, q):\n",
    "        μ1, σ1 = p['mean'], p['stddev']\n",
    "        μ2, σ2 = q['mean'], q['stddev']\n",
    "        num = tf.square(μ1 - μ2) + tf.square(σ1) - tf.square(σ2)\n",
    "        den = 2. * tf.square(σ2)\n",
    "        return tf.reduce_sum(num / (den + TINY) + tf.math.log(σ2 + TINY) - tf.math.log(σ1 + TINY), axis=1)\n",
    "\n",
    "    # --- sampling ----------------------------------------------------------\n",
    "    def sample(self, dist_info):\n",
    "        mean, std = dist_info['mean'], dist_info['stddev']\n",
    "        eps = tf.random.normal(tf.shape(mean))\n",
    "        return mean + eps * std\n",
    "\n",
    "    def sample_prior(self, batch_size):\n",
    "        return tf.random.normal([batch_size, self.dim])\n",
    "\n",
    "    # --- helpers -----------------------------------------------------------\n",
    "    def activate_dist(self, flat):\n",
    "        mean = flat[:, :self.dim]\n",
    "        if self._fix_std:\n",
    "            std = tf.ones_like(mean)\n",
    "        else:\n",
    "            std = tf.sqrt(tf.exp(flat[:, self.dim:]))\n",
    "        return {'mean': mean, 'stddev': std}\n",
    "\n",
    "    def entropy(self, dist_info):\n",
    "        std = dist_info['stddev']\n",
    "        return tf.reduce_sum(0.5 * np.log(2 * np.pi * np.e) + tf.math.log(std + TINY), axis=1)\n",
    "\n",
    "    def marginal_entropy(self, dist_info):\n",
    "        return self.entropy(dist_info)\n",
    "\n",
    "    def marginal_logli(self, x_var, dist_info):\n",
    "        return self.logli(x_var, dist_info)\n",
    "\n",
    "    def nonreparam_logli(self, x_var, dist_info):\n",
    "        return tf.zeros_like(x_var[:, 0])\n",
    "\n",
    "    def prior_dist_info(self, batch_size):\n",
    "        mean = tf.zeros([batch_size, self.dim])\n",
    "        std = tf.ones([batch_size, self.dim])\n",
    "        return {'mean': mean, 'stddev': std}\n",
    "\n",
    "# Espaço latente\n",
    "class Product(Distribution):\n",
    "    def __init__(self, dists: list[Distribution]):\n",
    "        self._dists = dists\n",
    "\n",
    "    # --- short‑cuts --------------------------------------------------------\n",
    "    @property\n",
    "    def dists(self): return list(self._dists)\n",
    "    @property\n",
    "    def dim(self): return sum(d.dim for d in self.dists)\n",
    "    @property\n",
    "    def effective_dim(self): return sum(d.effective_dim for d in self.dists)\n",
    "    @property\n",
    "    def dist_flat_dim(self): return sum(d.dist_flat_dim for d in self.dists)\n",
    "\n",
    "    def dims(self):\n",
    "        return [d.dim for d in self.dists]\n",
    "\n",
    "    def dist_flat_dims(self):\n",
    "        return [d.dist_flat_dim for d in self.dists]\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    def dist_info_keys(self):\n",
    "        keys = []\n",
    "        for idx, dist in enumerate(self.dists):\n",
    "            for k in dist.dist_info_keys:\n",
    "                keys.append(f'id_{idx}_{k}')\n",
    "        return keys\n",
    "\n",
    "    def split_dist_info(self, dist_info):\n",
    "        ret = []\n",
    "        for idx, dist in enumerate(self.dists):\n",
    "            cur = {k: dist_info[f'id_{idx}_{k}'] for k in dist.dist_info_keys}\n",
    "            ret.append(cur)\n",
    "        return ret\n",
    "\n",
    "    def join_dist_infos(self, infos):\n",
    "        ret = {}\n",
    "        for idx, dist, info in zip(itertools.count(), self.dists, infos):\n",
    "            for k in dist.dist_info_keys:\n",
    "                ret[f'id_{idx}_{k}'] = info[k]\n",
    "        return ret\n",
    "\n",
    "    def split_var(self, x):\n",
    "        cum = np.cumsum([d.dim for d in self.dists])\n",
    "        outs, start = [], 0\n",
    "        for end in cum:\n",
    "            outs.append(x[:, start:end])\n",
    "            start = end\n",
    "        return outs\n",
    "\n",
    "    def split_dist_flat(self, flat):\n",
    "        cum = np.cumsum([d.dist_flat_dim for d in self.dists])\n",
    "        outs, start = [], 0\n",
    "        for end in cum:\n",
    "            outs.append(flat[:, start:end])\n",
    "            start = end\n",
    "        return outs\n",
    "\n",
    "    # --- sampling ----------------------------------------------------------\n",
    "    def sample(self, dist_info):\n",
    "        parts = [tf.cast(d.sample(i), tf.float32) for d, i in zip(self.dists, self.split_dist_info(dist_info))]\n",
    "        return tf.concat(parts, axis=1)\n",
    "\n",
    "    def sample_prior(self, batch_size):\n",
    "        parts = [tf.cast(d.sample_prior(batch_size), tf.float32) for d in self.dists]\n",
    "        return tf.concat(parts, axis=1)\n",
    "\n",
    "    # --- likelihood / entropy / etc. --------------------------------------\n",
    "    def logli(self, x_var, dist_info):\n",
    "        return tf.add_n([d.logli(xi, di) for d, xi, di in zip(self.dists, self.split_var(x_var), self.split_dist_info(dist_info))])\n",
    "\n",
    "    def marginal_logli(self, x_var, dist_info):\n",
    "        return tf.add_n([d.marginal_logli(xi, di) for d, xi, di in zip(self.dists, self.split_var(x_var), self.split_dist_info(dist_info))])\n",
    "\n",
    "    def entropy(self, dist_info):\n",
    "        return tf.add_n([d.entropy(di) for d, di in zip(self.dists, self.split_dist_info(dist_info))])\n",
    "\n",
    "    def marginal_entropy(self, dist_info):\n",
    "        return tf.add_n([d.marginal_entropy(di) for d, di in zip(self.dists, self.split_dist_info(dist_info))])\n",
    "\n",
    "    def nonreparam_logli(self, x_var, dist_info):\n",
    "        return tf.add_n([d.nonreparam_logli(xi, di) for d, xi, di in zip(self.dists, self.split_var(x_var), self.split_dist_info(dist_info))])\n",
    "\n",
    "    def kl(self, p, q):\n",
    "        return tf.add_n([d.kl(pi, qi) for d, pi, qi in zip(self.dists, self.split_dist_info(p), self.split_dist_info(q))])\n",
    "\n",
    "    def activate_dist(self, flat):\n",
    "        ret = {}\n",
    "        for idx, d, f in zip(itertools.count(), self.dists, self.split_dist_flat(flat)):\n",
    "            info = d.activate_dist(f)\n",
    "            for k, v in info.items():\n",
    "                ret[f'id_{idx}_{k}'] = v\n",
    "        return ret\n",
    "\n",
    "    def prior_dist_info(self, batch_size):\n",
    "        infos = [d.prior_dist_info(batch_size) for d in self.dists]\n",
    "        return self.join_dist_infos(infos)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Modelos Keras --------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "#Gerador DCGAN-64: projeção → reshape → 4 transposed‐convs com batch norm + ReLU, final em tanh.\n",
    "def build_generator(z_dim: int, img_shape):\n",
    "    \"\"\"Gerador DCGAN‑64 clássico (4× upsampling → 64×64).\"\"\"\n",
    "    h, w, c = img_shape  # h==w==64\n",
    "\n",
    "    inp = layers.Input(shape=(z_dim,))\n",
    "\n",
    "    # 1) projeção + reshape → 4×4×512\n",
    "    x = layers.Dense(4 * 4 * 512, use_bias=False)(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Reshape((4, 4, 512))(x)\n",
    "\n",
    "    # 2) 8×8×256\n",
    "    x = layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # 3) 16×16×128\n",
    "    x = layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # 4) 32×32×64\n",
    "    x = layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # 5) 64×64×c\n",
    "    x = layers.Conv2DTranspose(c, kernel_size=4, strides=2, padding='same', activation='tanh')(x)\n",
    "\n",
    "    return Model(inp, x, name='Generator')\n",
    "\n",
    "\n",
    "def build_discriminator_q(img_shape, cat_dim, cont_dim):\n",
    "    inp = layers.Input(shape=img_shape)\n",
    "    x = layers.Conv2D(64, 4, 2, 'same')(inp)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    x = layers.Conv2D(128, 4, 2, 'same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(1024)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    # discriminator\n",
    "    d_out = layers.Dense(1, activation='sigmoid', name='d_out')(x)\n",
    "\n",
    "    # Q‐network: duas saídas\n",
    "    q_cat_logits = layers.Dense(cat_dim, name='q_cat_logits')(x)\n",
    "    q_cont_params = layers.Dense(cont_dim * 2, name='q_cont_params')(x)\n",
    "\n",
    "    return Model(inp, [d_out, q_cat_logits, q_cont_params], name='Discriminator_Q')\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Métricas\n",
    "\n",
    "_inception = tf.keras.applications.InceptionV3(include_top=False, pooling='avg', input_shape=(299, 299, 3))\n",
    "\n",
    "# ------------------------- FID helpers (stream‑safe) -------------------------\n",
    "FID_BATCH = 256  # nº de imagens por forward pass no Inception (evita OOM)\n",
    "\n",
    "\n",
    "def _get_inception_activations(img_uint8, bs: int = FID_BATCH):\n",
    "    \"\"\"Extrai ativações do pool-3 da Inception em minibatches para não estourar RAM.\"\"\"\n",
    "    acts = []\n",
    "    for i in range(0, len(img_uint8), bs):\n",
    "        batch = img_uint8[i:i + bs]\n",
    "        batch = tf.image.resize(batch, (299, 299))\n",
    "        batch = tf.keras.applications.inception_v3.preprocess_input(tf.cast(batch, tf.float32))\n",
    "        acts.append(_inception(batch, training=False))\n",
    "    # Retorna só o array numpy resultante\n",
    "    return tf.concat(acts, axis=0).numpy()\n",
    "\n",
    "\n",
    "def fid_np(real_uint8, gen_uint8):\n",
    "    act1, act2 = _get_inception_activations(real_uint8), _get_inception_activations(gen_uint8)\n",
    "    mu1, mu2 = act1.mean(0), act2.mean(0)\n",
    "    sigma1, sigma2 = np.cov(act1, rowvar=False), np.cov(act2, rowvar=False)\n",
    "    covmean = sqrtm(sigma1 @ sigma2)\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    return float(np.sum((mu1 - mu2) ** 2) + np.trace(sigma1 + sigma2 - 2 * covmean))\n",
    "\n",
    "# --- HSIC / MIG / SAP / quase-ortogonal -----------------------------------\n",
    "\n",
    "def _hsic(K, L):\n",
    "    n = K.shape[0]\n",
    "    H = np.eye(n) - np.ones((n, n)) / n\n",
    "    HKH, HLH = H @ K @ H, H @ L @ H\n",
    "    return np.trace(HKH @ HLH) / ((n - 1) ** 2)\n",
    "\n",
    "def metric_hsic(z: np.ndarray, attr: np.ndarray):\n",
    "    n, d = z.shape\n",
    "    hsic_vals = []\n",
    "    for k in range(d):\n",
    "        zk = z[:, k:k + 1]\n",
    "        K = np.exp(-squareform(pdist(zk, 'sqeuclidean')) / (np.median(zk) ** 2 + 1e-8))\n",
    "        for j in range(attr.shape[1]):\n",
    "            aj = attr[:, j:j + 1]\n",
    "            L = (aj == aj.T).astype(np.float32)\n",
    "            hsic_vals.append(_hsic(K, L))\n",
    "    return float(np.mean(hsic_vals))\n",
    "\n",
    "def metric_mutual_info(z, attr):\n",
    "    n_lat = z.shape[1]\n",
    "    mi = np.zeros((n_lat, attr.shape[1]))\n",
    "    for i in range(n_lat):\n",
    "        zi_disc = np.digitize(z[:, i], np.histogram(z[:, i], bins=20)[1][:-1])\n",
    "        for j in range(attr.shape[1]):\n",
    "            mi[i, j] = mutual_info_score(zi_disc, attr[:, j])\n",
    "    return mi\n",
    "\n",
    "def metric_mig(z, attr):\n",
    "    mi = metric_mutual_info(z, attr)\n",
    "    entropy_attr = np.array([mutual_info_score(attr[:, j], attr[:, j]) for j in range(attr.shape[1])])\n",
    "    sorted_mi = -np.sort(-mi, axis=0)\n",
    "    gap = (sorted_mi[0] - sorted_mi[1]) / (entropy_attr + 1e-12)\n",
    "    return float(np.mean(gap))\n",
    "\n",
    "def metric_sap(z, attr):\n",
    "    mi = metric_mutual_info(z, attr)\n",
    "    sorted_mi = -np.sort(-mi, axis=0)\n",
    "    return float(np.mean(sorted_mi[0] - sorted_mi[1]))\n",
    "\n",
    "def quasi_orthogonality(z):\n",
    "    zc = z - z.mean(0)\n",
    "    cov = np.cov(zc, rowvar=False)\n",
    "    off = cov - np.diag(np.diag(cov))\n",
    "    max_abs = np.max(np.abs(off))\n",
    "    return float(max_abs < 1e-5), float(max_abs)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# InfoGAN Trainer \n",
    "\n",
    "class InfoGANTrainer:\n",
    "    def __init__(self, G, DQ, latent_dist: Product, dataset: CelebAWithAttr, batch_size=64,\n",
    "                 info_coeff=1.0, log_dir='logs', ckpt_dir='ckpt', snapshot=1000, max_iter=100_000,\n",
    "                 noise_dim=62, cat_dim=10, cont_dim=2):\n",
    "        self.G, self.DQ = G, DQ\n",
    "        self.latent_dist, self.dataset = latent_dist, dataset\n",
    "        self.bs = batch_size\n",
    "        self.info_coeff = info_coeff\n",
    "        self.snapshot = snapshot\n",
    "        self.max_iter = max_iter\n",
    "        self.noise_dim = noise_dim\n",
    "        self.cat_dim   = cat_dim\n",
    "        self.cont_dim  = cont_dim\n",
    "\n",
    "        self.log_dir, self.ckpt_dir = Path(log_dir), Path(ckpt_dir)\n",
    "        # mkdir_p(self.log_dir); mkdir_p(self.ckpt_dir)\n",
    "\n",
    "        self.d_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        self.g_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        # adicionamos um otimizador separado para a cabeça Q\n",
    "        self.q_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "        # filtramos os pesos da Q-head pelo nome\n",
    "        self.q_vars = [\n",
    "            v for v in self.DQ.trainable_variables\n",
    "            if 'q_cat_logits' in v.name or 'q_cont_params' in v.name\n",
    "        ]\n",
    "        \n",
    "        if self.ckpt_dir.is_dir():\n",
    "            # carrega o checkpoint se existir\n",
    "            self.ckpt = tf.train.Checkpoint(G=self.G, DQ=self.DQ, d_opt=self.d_opt, g_opt=self.g_opt, q_opt=self.q_opt)\n",
    "            self.ckpt_manager = tf.train.CheckpointManager(self.ckpt, str(self.ckpt_dir), max_to_keep=5)\n",
    "            latest_ckpt = self.ckpt_manager.latest_checkpoint\n",
    "            if latest_ckpt:\n",
    "                print(f\"Restaurando checkpoint de {latest_ckpt}...\")\n",
    "                self.ckpt.restore(latest_ckpt)\n",
    "            else:\n",
    "                print(\"Nenhum checkpoint encontrado, iniciando do zero.\")\n",
    "                mkdir_p(self.ckpt_dir)\n",
    "            \n",
    "        self.metric_path = self.log_dir / 'info_gan_metrics.csv'\n",
    "        if self.log_dir.is_dir():\n",
    "            # carrega histórico de métricas se existir\n",
    "            self.metric_hist = pd.read_csv(self.metric_path).to_dict(orient='list')\n",
    "        else:\n",
    "            self.metric_hist = {'iter': [], 'FID': [], 'HSIC': [], 'MIG': [], 'SAP': [], 'OrthoMax': []} \n",
    "            mkdir_p(self.log_dir)\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    def _mi_loss(self, z_reg, q_cat_logits, q_cont_params):\n",
    "        # separa z_reg em z_cat (one‐hot) e z_cont (gaussiano)\n",
    "        z_cat  = z_reg[:, :self.cat_dim]\n",
    "        z_cont = z_reg[:, self.cat_dim:]\n",
    "\n",
    "        # --- 1) Cross‐entropy (categórico) -----------------------------\n",
    "        cat_loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=z_cat,\n",
    "                logits=q_cat_logits\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # --- 2) Log‐likelihood gaussiano (contínuo) ------------------\n",
    "        # previsão de mean e log‐std\n",
    "        mean_pred    = q_cont_params[:, :self.cont_dim]\n",
    "        log_std_pred = q_cont_params[:, self.cont_dim:]\n",
    "        std_pred     = tf.exp(log_std_pred)\n",
    "\n",
    "        # log‐likelihood de cada dimensão\n",
    "        eps = (z_cont - mean_pred) / (std_pred + TINY)\n",
    "        # log N(z;μ,σ) = −½·(log(2π) + 2·logσ + ε²)\n",
    "        logli_per_dim = -0.5 * (tf.math.log(2. * np.pi) + 2. * log_std_pred + tf.square(eps))\n",
    "        logli = tf.reduce_sum(logli_per_dim, axis=1)  # soma sobre dims\n",
    "\n",
    "        cont_loss = -tf.reduce_mean(logli)  # NLL\n",
    "\n",
    "        return cat_loss + cont_loss\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    # \n",
    "    @tf.function\n",
    "    def _train_step(self, real_imgs):\n",
    "        # 1) Amostra z do espaço latente\n",
    "        z = self.latent_dist.sample_prior(self.bs)\n",
    "\n",
    "        # 2) Treina o Discriminador\n",
    "        with tf.GradientTape() as d_tape:\n",
    "            fake = self.G(z, training=True)\n",
    "            d_real, _, _ = self.DQ(real_imgs, training=True)\n",
    "            d_fake, _, _ = self.DQ(fake,    training=True)\n",
    "\n",
    "            d_loss = -tf.reduce_mean(\n",
    "                tf.math.log(d_real + TINY) +\n",
    "                tf.math.log(1 - d_fake + TINY)\n",
    "            )\n",
    "\n",
    "        d_grads = d_tape.gradient(d_loss, self.DQ.trainable_variables)\n",
    "        self.d_opt.apply_gradients(zip(d_grads, self.DQ.trainable_variables))\n",
    "\n",
    "        # 3) Treina o Gerador (adversarial + MI)\n",
    "        with tf.GradientTape() as g_tape:\n",
    "            fake = self.G(z, training=True)\n",
    "            d_fake, q_cat_logits, q_cont_params = self.DQ(fake, training=True)\n",
    "\n",
    "            g_adv_loss = -tf.reduce_mean(tf.math.log(d_fake + TINY))\n",
    "            z_reg = z[:, self.noise_dim:]\n",
    "            mi_loss = self._mi_loss(z_reg, q_cat_logits, q_cont_params)\n",
    "\n",
    "            g_loss = g_adv_loss + self.info_coeff * mi_loss\n",
    "\n",
    "        g_grads = g_tape.gradient(g_loss, self.G.trainable_variables)\n",
    "        self.g_opt.apply_gradients(zip(g_grads, self.G.trainable_variables))\n",
    "\n",
    "        # 4) Treina a cabeça Q (apenas MI)\n",
    "        with tf.GradientTape() as q_tape:\n",
    "            fake = self.G(z, training=False)  # G congelado aqui\n",
    "            _, q_cat_logits, q_cont_params = self.DQ(fake, training=True)\n",
    "            z_reg = z[:, self.noise_dim:]\n",
    "            mi_loss = self._mi_loss(z_reg, q_cat_logits, q_cont_params)\n",
    "\n",
    "        q_grads = q_tape.gradient(mi_loss, self.q_vars)\n",
    "\n",
    "        # Protege contra gradientes None\n",
    "        grads_and_vars = [(g, v) for g, v in zip(q_grads, self.q_vars) if g is not None]\n",
    "        if not grads_and_vars:\n",
    "            tf.print(\"Nenhum gradiente foi propagado para a cabeça Q. Verifique as conexões do modelo.\")\n",
    "        else:\n",
    "            self.q_opt.apply_gradients(grads_and_vars)\n",
    "\n",
    "        return d_loss, g_adv_loss, mi_loss\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    def _evaluate_metrics(self):\n",
    "        #Calcula FID (±1 k amostras) + métricas de disentanglement sem estourar memória.\"\"\"\n",
    "        N_EVAL = 1000  # amostras para FID / HSIC / etc.\n",
    "        real_flat, real_attr = self.dataset.next_batch(N_EVAL)\n",
    "        z = self.latent_dist.sample_prior(N_EVAL)\n",
    "        gen_imgs = self.G(z, training=False).numpy()\n",
    "\n",
    "        fid = fid_np(\n",
    "                self.dataset.inverse_transform(real_flat),\n",
    "                self.dataset.inverse_transform(gen_imgs.reshape(real_flat.shape))\n",
    "            )\n",
    "\n",
    "        z_np = z.numpy()\n",
    "        hsic = metric_hsic(z_np, real_attr)\n",
    "        mig = metric_mig(z_np, real_attr)\n",
    "        sap = metric_sap(z_np, real_attr)\n",
    "        _, omax = quasi_orthogonality(z_np)\n",
    "        return fid, hsic, mig, sap, omax\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    def train(self):\n",
    "        for step in tqdm(range(1, self.max_iter + 1)):\n",
    "            real_flat, _ = self.dataset.next_batch(self.bs)\n",
    "            real_imgs = real_flat.reshape((-1,) + self.dataset.image_shape)\n",
    "\n",
    "            print(f\"Step {step}: real_imgs shape {real_imgs.shape}, batch size {self.bs}\")\n",
    "\n",
    "            d_loss, g_loss, mi = self._train_step(real_imgs)\n",
    "\n",
    "            if step % self.snapshot == 0:\n",
    "                fid, hsic, mig, sap, omax = self._evaluate_metrics()\n",
    "                self.metric_hist['iter'].append(step)\n",
    "                self.metric_hist['FID'].append(fid)\n",
    "                self.metric_hist['HSIC'].append(hsic)\n",
    "                self.metric_hist['MIG'].append(mig)\n",
    "                self.metric_hist['SAP'].append(sap)\n",
    "                self.metric_hist['OrthoMax'].append(omax)\n",
    "                pd.DataFrame(self.metric_hist).to_csv(self.metric_path, index=False)\n",
    "                ckpt_path = str(self.ckpt_dir / f'ckpt_{step}')\n",
    "                self.ckpt.save(ckpt_path)\n",
    "                print(f\"Step {step}: FID {fid:.1f} | HSIC {hsic:.4f} | MIG {mig:.4f} | SAP {sap:.4f} | max|off‑diag| {omax:.3e}\")\n",
    "    \n",
    "    def generate_samples(self,\n",
    "                         n_samples: int = 8,\n",
    "                         output_dir: str = 'samples',\n",
    "                         prefix: str = 'sample'):\n",
    "        \"\"\"\n",
    "        Gera n amostras usando o G treinado e salva como PNG em output_dir.\n",
    "        Args:\n",
    "            n_samples: número de imagens a gerar (padrão 8).\n",
    "            output_dir: pasta onde os arquivos serão salvos.\n",
    "            prefix: prefixo do nome de cada arquivo (ex: 'sample_0.png').\n",
    "        \"\"\"\n",
    "        # garante que a pasta existe\n",
    "        mkdir_p(output_dir)\n",
    "\n",
    "        # amostra do espaço latente\n",
    "        z = self.latent_dist.sample_prior(n_samples)\n",
    "        # gera imagens\n",
    "        gen = self.G(z, training=False).numpy()\n",
    "        # converte do formato flat [-1,1] para uint8 [0,255]\n",
    "        imgs_uint8 = self.dataset.inverse_transform(gen.reshape(n_samples, -1))\n",
    "\n",
    "        # salva cada imagem\n",
    "        for i, img in enumerate(imgs_uint8):\n",
    "            path = os.path.join(output_dir, f\"{prefix}_{i}.png\")\n",
    "            Image.fromarray(img).save(path)\n",
    "\n",
    "        print(f\"Geradas e salvas {n_samples} amostras em '{output_dir}/'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/300000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 1: real_imgs shape (64, 64, 64, 3), batch size 64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py:855: UserWarning: Gradients do not exist for variables ['q_cat_logits/kernel', 'q_cat_logits/bias', 'q_cont_params/kernel', 'q_cont_params/bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nenhum gradiente foi propagado para a cabeça Q. Verifique as conexões do modelo.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "I0000 00:00:1752753409.618889 2818284 cuda_dnn.cc:529] Loaded cuDNN version 90501\n",
      "2025-07-17 08:56:49.813142: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2025-07-17 08:56:49.815320: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2025-07-17 08:56:49.818114: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2025-07-17 08:56:49.819955: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2025-07-17 08:56:49.823291: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2025-07-17 08:56:49.826186: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2025-07-17 08:56:49.828544: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "2025-07-17 08:56:49.831214: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "error: libdevice not found at ./libdevice.10.bc\n",
      "error: libdevice not found at ./libdevice.10.bc\n",
      "error: libdevice not found at ./libdevice.10.bc\n",
      "error: libdevice not found at ./libdevice.10.bc\n",
      "error: libdevice not found at ./libdevice.10.bc\n",
      "error: libdevice not found at ./libdevice.10.bc\n",
      "error: libdevice not found at ./libdevice.10.bc\n",
      "error: libdevice not found at ./libdevice.10.bc\n",
      "2025-07-17 08:56:49.832165: E tensorflow/compiler/mlir/tools/kernel_gen/tf_framework_c_interface.cc:227] INTERNAL: Generating device code failed.\n",
      "2025-07-17 08:56:49.834562: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834591: I tensorflow/core/framework/local_rendezvous.cc:407] Local rendezvous is aborting with status: UNKNOWN: JIT compilation failed.\n",
      "\t [[{{node adam/Pow_23}}]]\n",
      "2025-07-17 08:56:49.834622: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834636: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834643: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834651: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834657: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834665: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834671: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834680: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834686: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834693: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834699: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834703: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834707: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834711: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834714: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834719: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834722: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834726: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834730: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834735: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834738: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834742: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834745: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834749: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834753: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834757: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834761: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834765: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834769: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834773: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834776: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834780: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834784: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834787: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834791: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834795: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834799: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834802: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834806: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834810: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834814: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834817: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834821: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834826: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834830: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834833: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834837: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834841: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834844: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834848: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:49.834851: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:50.141778: W external/local_xla/xla/service/gpu/llvm_gpu_backend/nvptx_backend.cc:110] libdevice is required by this HLO module but was not found at ./libdevice.10.bc\n",
      "error: libdevice not found at ./libdevice.10.bc\n",
      "2025-07-17 08:56:50.141977: E tensorflow/compiler/mlir/tools/kernel_gen/tf_framework_c_interface.cc:227] INTERNAL: Generating device code failed.\n",
      "2025-07-17 08:56:50.142892: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:50.142961: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:50.142998: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "2025-07-17 08:56:50.143033: W tensorflow/core/framework/op_kernel.cc:1844] UNKNOWN: JIT compilation failed.\n",
      "  0%|          | 0/300000 [00:03<?, ?it/s]\n"
     ]
    },
    {
     "ename": "UnknownError",
     "evalue": "Graph execution error:\n\nDetected at node adam/Pow_23 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n\n  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n\n  File \"/tmp/ipykernel_2817945/1108313581.py\", line 16, in <module>\n\n  File \"/tmp/ipykernel_2817945/3003019814.py\", line 736, in train\n\n  File \"/tmp/ipykernel_2817945/3003019814.py\", line 670, in _train_step\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\", line 463, in apply_gradients\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\", line 527, in apply\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\", line 593, in _backend_apply_gradients\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 120, in _backend_update_step\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 134, in _distributed_tf_update_step\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 131, in apply_grad_to_update_var\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/keras/src/optimizers/adam.py\", line 110, in update_step\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/keras/src/ops/numpy.py\", line 6391, in power\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/keras/src/backend/tensorflow/numpy.py\", line 2653, in power\n\nJIT compilation failed.\n\t [[{{node adam/Pow_23}}]] [Op:__inference__train_step_82149]",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mUnknownError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[50]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     12\u001b[39m data = CelebAWithAttr(\u001b[33m'\u001b[39m\u001b[33m/home/hanna/Github/mod_gen/data/celeba\u001b[39m\u001b[33m'\u001b[39m, IMG_SHAPE)\n\u001b[32m     14\u001b[39m trainer = InfoGANTrainer(G, DQ, latent_dist, data, batch_size=BATCH, max_iter=\u001b[32m300000\u001b[39m, snapshot=\u001b[32m1000\u001b[39m, \n\u001b[32m     15\u001b[39m                              noise_dim=noise_dim,cat_dim=cat_dim, cont_dim=cont_dim)\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m \u001b[43mtrainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[48]\u001b[39m\u001b[32m, line 736\u001b[39m, in \u001b[36mInfoGANTrainer.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    732\u001b[39m real_imgs = real_flat.reshape((-\u001b[32m1\u001b[39m,) + \u001b[38;5;28mself\u001b[39m.dataset.image_shape)\n\u001b[32m    734\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mStep \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mstep\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m: real_imgs shape \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mreal_imgs.shape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, batch size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.bs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m--> \u001b[39m\u001b[32m736\u001b[39m d_loss, g_loss, mi = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_imgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    738\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m step % \u001b[38;5;28mself\u001b[39m.snapshot == \u001b[32m0\u001b[39m:\n\u001b[32m    739\u001b[39m     fid, hsic, mig, sap, omax = \u001b[38;5;28mself\u001b[39m._evaluate_metrics()\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/mod_gen/mod-gen/lib/python3.12/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m    152\u001b[39m   filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m--> \u001b[39m\u001b[32m153\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    154\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    155\u001b[39m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Github/mod_gen/mod-gen/lib/python3.12/site-packages/tensorflow/python/eager/execute.py:53\u001b[39m, in \u001b[36mquick_execute\u001b[39m\u001b[34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[39m\n\u001b[32m     51\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m     52\u001b[39m   ctx.ensure_initialized()\n\u001b[32m---> \u001b[39m\u001b[32m53\u001b[39m   tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[32m     54\u001b[39m                                       inputs, attrs, num_outputs)\n\u001b[32m     55\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m core._NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[32m     56\u001b[39m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[31mUnknownError\u001b[39m: Graph execution error:\n\nDetected at node adam/Pow_23 defined at (most recent call last):\n  File \"<frozen runpy>\", line 198, in _run_module_as_main\n\n  File \"<frozen runpy>\", line 88, in _run_code\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/ipykernel_launcher.py\", line 18, in <module>\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/traitlets/config/application.py\", line 1075, in launch_instance\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/ipykernel/kernelapp.py\", line 739, in start\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/tornado/platform/asyncio.py\", line 211, in start\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 641, in run_forever\n\n  File \"/usr/lib/python3.12/asyncio/base_events.py\", line 1987, in _run_once\n\n  File \"/usr/lib/python3.12/asyncio/events.py\", line 88, in _run\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 545, in dispatch_queue\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 534, in process_one\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 437, in dispatch_shell\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 362, in execute_request\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/ipykernel/kernelbase.py\", line 778, in execute_request\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/ipykernel/ipkernel.py\", line 449, in do_execute\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/ipykernel/zmqshell.py\", line 549, in run_cell\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3116, in run_cell\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3171, in _run_cell\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/IPython/core/async_helpers.py\", line 128, in _pseudo_sync_runner\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3394, in run_cell_async\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3639, in run_ast_nodes\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/IPython/core/interactiveshell.py\", line 3699, in run_code\n\n  File \"/tmp/ipykernel_2817945/1108313581.py\", line 16, in <module>\n\n  File \"/tmp/ipykernel_2817945/3003019814.py\", line 736, in train\n\n  File \"/tmp/ipykernel_2817945/3003019814.py\", line 670, in _train_step\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\", line 463, in apply_gradients\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\", line 527, in apply\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/keras/src/optimizers/base_optimizer.py\", line 593, in _backend_apply_gradients\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 120, in _backend_update_step\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 134, in _distributed_tf_update_step\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/keras/src/backend/tensorflow/optimizer.py\", line 131, in apply_grad_to_update_var\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/keras/src/optimizers/adam.py\", line 110, in update_step\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/keras/src/ops/numpy.py\", line 6391, in power\n\n  File \"/home/hanna/Github/mod_gen/mod-gen/lib/python3.12/site-packages/keras/src/backend/tensorflow/numpy.py\", line 2653, in power\n\nJIT compilation failed.\n\t [[{{node adam/Pow_23}}]] [Op:__inference__train_step_82149]"
     ]
    }
   ],
   "source": [
    "BATCH = 64\n",
    "IMG_SHAPE = (64, 64, 3)\n",
    "    \n",
    "#Parametros para latente (dimensão do vetor \\Re^(noise_dim + cat_dim + cont_dim))\n",
    "noise_dim, cat_dim, cont_dim = 62, 10, 2\n",
    "    \n",
    "# O espaço latente modelado pelo objeto Product que agrupa três distribuições:\n",
    "latent_dist = Product([Gaussian(noise_dim, fix_std=True),Categorical(cat_dim),Gaussian(cont_dim)])\n",
    "\n",
    "G = build_generator(noise_dim + cat_dim + cont_dim, IMG_SHAPE)\n",
    "DQ = build_discriminator_q(IMG_SHAPE, cat_dim, cont_dim)\n",
    "data = CelebAWithAttr('/home/hanna/Github/mod_gen/data/celeba', IMG_SHAPE)\n",
    "\n",
    "trainer = InfoGANTrainer(G, DQ, latent_dist, data, batch_size=BATCH, max_iter=300000, snapshot=1000, \n",
    "                             noise_dim=noise_dim,cat_dim=cat_dim, cont_dim=cont_dim)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array(['/home/hanna/Github/mod_gen/data/celeba/img_align_celeba/000001.jpg',\n",
       "       '/home/hanna/Github/mod_gen/data/celeba/img_align_celeba/000002.jpg',\n",
       "       '/home/hanna/Github/mod_gen/data/celeba/img_align_celeba/000003.jpg',\n",
       "       ...,\n",
       "       '/home/hanna/Github/mod_gen/data/celeba/img_align_celeba/202597.jpg',\n",
       "       '/home/hanna/Github/mod_gen/data/celeba/img_align_celeba/202598.jpg',\n",
       "       '/home/hanna/Github/mod_gen/data/celeba/img_align_celeba/202599.jpg'],\n",
       "      dtype='<U66')"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['5_o_Clock_Shadow',\n",
       " 'Arched_Eyebrows',\n",
       " 'Attractive',\n",
       " 'Bags_Under_Eyes',\n",
       " 'Bald',\n",
       " 'Bangs',\n",
       " 'Big_Lips',\n",
       " 'Big_Nose',\n",
       " 'Black_Hair',\n",
       " 'Blond_Hair',\n",
       " 'Blurry',\n",
       " 'Brown_Hair',\n",
       " 'Bushy_Eyebrows',\n",
       " 'Chubby',\n",
       " 'Double_Chin',\n",
       " 'Eyeglasses',\n",
       " 'Goatee',\n",
       " 'Gray_Hair',\n",
       " 'Heavy_Makeup',\n",
       " 'High_Cheekbones',\n",
       " 'Male',\n",
       " 'Mouth_Slightly_Open',\n",
       " 'Mustache',\n",
       " 'Narrow_Eyes',\n",
       " 'No_Beard',\n",
       " 'Oval_Face',\n",
       " 'Pale_Skin',\n",
       " 'Pointy_Nose',\n",
       " 'Receding_Hairline',\n",
       " 'Rosy_Cheeks',\n",
       " 'Sideburns',\n",
       " 'Smiling',\n",
       " 'Straight_Hair',\n",
       " 'Wavy_Hair',\n",
       " 'Wearing_Earrings',\n",
       " 'Wearing_Hat',\n",
       " 'Wearing_Lipstick',\n",
       " 'Wearing_Necklace',\n",
       " 'Wearing_Necktie',\n",
       " 'Young']"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.attr_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mod-gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
