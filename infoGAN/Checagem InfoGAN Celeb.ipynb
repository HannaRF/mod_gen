{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFOGAN with metrics\n",
    "\n",
    "* souce: https://github.com/openai/InfoGAN/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install prettytensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dateutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-17 21:33:39.290126: I tensorflow/core/util/port.cc:153] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-07-17 21:33:39.299604: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1752798819.311002 2958359 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1752798819.314107 2958359 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1752798819.322582 2958359 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752798819.322595 2958359 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752798819.322596 2958359 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1752798819.322597 2958359 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-17 21:33:39.326252: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI AVX512_BF16 AVX512_FP16 AVX_VNNI AMX_TILE AMX_INT8 AMX_BF16 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "W0000 00:00:1752798820.445553 2958359 gpu_device.cc:2341] Cannot dlopen some GPU libraries. Please make sure the missing libraries mentioned above are installed properly if you would like to use GPU. Follow the guide at https://www.tensorflow.org/install/gpu for how to download and setup the required libraries for your platform.\n",
      "Skipping registering GPU devices...\n"
     ]
    }
   ],
   "source": [
    "from __future__ import annotations\n",
    "import os, math, json, errno, time\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.linalg import sqrtm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from PIL import Image\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Utils ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------\n",
    "TINY = 1e-8\n",
    "floatX = np.float32\n",
    "\n",
    "def mkdir_p(path: str):\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Dataset loader (CelebA)\n",
    "class CelebADataset:\n",
    "    \"\"\"Loader simplificado da CelebA sem atributos.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir: str = '.', image_shape=(64, 64, 3), split_ratio=0.9):\n",
    "        self.image_shape = image_shape\n",
    "        self.image_dim = int(np.prod(image_shape))\n",
    "        base = Path(root_dir)\n",
    "\n",
    "        # Encontra todas as imagens no diretório\n",
    "        img_dir = base / 'img_align_celeba'\n",
    "        sub = img_dir / 'img_align_celeba'\n",
    "        if sub.is_dir():\n",
    "            img_dir = sub\n",
    "            \n",
    "        self.files = sorted([f for f in img_dir.glob('*.jpg')])\n",
    "        \n",
    "        # Split train\n",
    "        n_train = int(len(self.files) * split_ratio)\n",
    "        self.train_idx = np.arange(n_train)\n",
    "        np.random.shuffle(self.train_idx)\n",
    "        self.ptr = 0\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        if self.ptr + batch_size > len(self.train_idx):\n",
    "            np.random.shuffle(self.train_idx)\n",
    "            self.ptr = 0\n",
    "        sel = self.train_idx[self.ptr:self.ptr + batch_size]\n",
    "        self.ptr += batch_size\n",
    "        imgs = []\n",
    "        for i in sel:\n",
    "            img = Image.open(self.files[i]).resize(self.image_shape[:2])\n",
    "            imgs.append(np.asarray(img, np.float32) / 127.5 - 1.0)\n",
    "        x = np.stack(imgs).reshape(batch_size, -1)\n",
    "        return x\n",
    "\n",
    "    def inverse_transform(self, flat):\n",
    "        imgs = flat.reshape((-1,) + self.image_shape)\n",
    "        return ((imgs + 1.) * 127.5).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "# Distributions \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "TINY = 1e-8\n",
    "floatX = np.float32\n",
    "\n",
    "class Distribution:\n",
    "    \"\"\"Classe base abstrata para distribuições latentes no InfoGAN.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def dim(self) -> int:\n",
    "        \"\"\"Dimensão da variável aleatória (tamanho do vetor de saída).\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @property\n",
    "    def dist_flat_dim(self) -> int:\n",
    "        \"\"\"Dimensão do vetor plano de parâmetros da distribuição.\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @property\n",
    "    def effective_dim(self) -> int:\n",
    "        \"\"\"Dimensão efetiva para cálculo de mutual information.\"\"\"\n",
    "        return self.dim\n",
    "        \n",
    "    def logli(self, x_var: tf.Tensor, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Calcula o log-likelihood log p(x|θ) para cada amostra no batch.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def sample(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Gera amostras da distribuição parametrizada por dist_info.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def sample_prior(self, batch_size: int) -> tf.Tensor:\n",
    "        \"\"\"Gera amostras da distribuição prévia (prior).\"\"\"\n",
    "        return self.sample(self.prior_dist_info(batch_size))\n",
    "        \n",
    "    def prior_dist_info(self, batch_size: int) -> Dict[str, tf.Tensor]:\n",
    "        \"\"\"\n",
    "        Retorna os parâmetros da distribuição prévia (prior).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def entropy(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Calcula a entropia H[p(x|θ)] para cada amostra no batch.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def marginal_entropy(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Calcula a entropia marginal (média sobre o batch).\n",
    "        \"\"\"\n",
    "        # Implementação padrão: média dos parâmetros no batch\n",
    "        avg_dist_info = {\n",
    "            k: tf.tile(tf.reduce_mean(v, axis=0, keepdims=True), \n",
    "            [tf.shape(v)[0], *[1]*(len(v.shape)-1)])\n",
    "            for k, v in dist_info.items()\n",
    "        }\n",
    "        return self.entropy(avg_dist_info)\n",
    "        \n",
    "    def marginal_logli(self, x_var: tf.Tensor, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Log-likelihood usando parâmetros marginais (média no batch).\n",
    "        \"\"\"\n",
    "        avg_dist_info = {\n",
    "            k: tf.tile(tf.reduce_mean(v, axis=0, keepdims=True), \n",
    "            [tf.shape(v)[0], *[1]*(len(v.shape)-1)])\n",
    "            for k, v in dist_info.items()\n",
    "        }\n",
    "        return self.logli(x_var, avg_dist_info)\n",
    "        \n",
    "    def kl(self, p: Dict[str, tf.Tensor], q: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Calcula a divergência KL entre duas distribuições (KL(p||q)).\n",
    "        Implementação padrão usando entropia cruzada e entropia:\n",
    "        KL(p||q) = H(p,q) - H(p)\n",
    "        \"\"\"\n",
    "        cross_entropy = -self.logli(p['samples'], q)\n",
    "        entropy = self.entropy(p)\n",
    "        return cross_entropy - entropy\n",
    "        \n",
    "    def dist_info_keys(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Lista de chaves no dicionário de parâmetros da distribuição.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def activate_dist(self, flat_dist: tf.Tensor) -> Dict[str, tf.Tensor]:\n",
    "        \"\"\"\n",
    "        Converte um vetor plano de parâmetros em parâmetros da distribuição.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def nonreparam_logli(self, x_var: tf.Tensor, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Log-likelihood para distribuições sem reparameterization trick.\n",
    "        \"\"\"\n",
    "        return tf.zeros_like(x_var[:, 0])\n",
    "\n",
    "class Categorical(Distribution):\n",
    "    \"\"\"Distribuição categórica (one-hot) para variáveis latentes discretas.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int):\n",
    "        self._dim = dim\n",
    "        \n",
    "    @property\n",
    "    def dim(self) -> int:\n",
    "        return self._dim\n",
    "        \n",
    "    @property\n",
    "    def dist_flat_dim(self) -> int:\n",
    "        return self._dim\n",
    "        \n",
    "    @property\n",
    "    def effective_dim(self) -> int:\n",
    "        return 1  # Apesar de ter N categorias, a dimensão efetiva é 1\n",
    "        \n",
    "    def logli(self, x_var: tf.Tensor, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        prob = dist_info['prob']\n",
    "        return tf.reduce_sum(x_var * tf.math.log(prob + TINY), axis=1)\n",
    "        \n",
    "    def sample(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        prob = dist_info['prob']\n",
    "        # Gera amostras usando Gumbel-Softmax trick para diferenciabilidade\n",
    "        logits = tf.math.log(prob + TINY)\n",
    "        gumbel_noise = -tf.math.log(-tf.math.log(tf.random.uniform(tf.shape(logits), dtype=floatX)))\n",
    "        samples = tf.nn.softmax((logits + gumbel_noise) / 1.0)  # temperatura=1.0\n",
    "        return samples\n",
    "        \n",
    "    def prior_dist_info(self, batch_size: int) -> Dict[str, tf.Tensor]:\n",
    "        prob = tf.ones((batch_size, self.dim), dtype=floatX) / self.dim\n",
    "        return {'prob': prob}\n",
    "        \n",
    "    def entropy(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        prob = dist_info['prob']\n",
    "        return -tf.reduce_sum(prob * tf.math.log(prob + TINY), axis=1)\n",
    "        \n",
    "    def kl(self, p: Dict[str, tf.Tensor], q: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        p_prob, q_prob = p['prob'], q['prob']\n",
    "        return tf.reduce_sum(p_prob * (tf.math.log(p_prob + TINY) - tf.math.log(q_prob + TINY)), axis=1)\n",
    "        \n",
    "    def dist_info_keys(self) -> List[str]:\n",
    "        return ['prob']\n",
    "        \n",
    "    def activate_dist(self, flat_dist: tf.Tensor) -> Dict[str, tf.Tensor]:\n",
    "        return {'prob': tf.nn.softmax(flat_dist)}\n",
    "\n",
    "class Gaussian(Distribution):\n",
    "    \"\"\"Distribuição gaussiana para variáveis latentes contínuas.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, fix_std: bool = False):\n",
    "        self._dim = dim\n",
    "        self._fix_std = fix_std\n",
    "        \n",
    "    @property\n",
    "    def dim(self) -> int:\n",
    "        return self._dim\n",
    "        \n",
    "    @property\n",
    "    def dist_flat_dim(self) -> int:\n",
    "        return self._dim * 2 if not self._fix_std else self._dim\n",
    "        \n",
    "    def logli(self, x_var: tf.Tensor, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        mean, std = dist_info['mean'], dist_info['stddev']\n",
    "        z = (x_var - mean) / (std + TINY)\n",
    "        return tf.reduce_sum(-0.5 * (np.log(2 * np.pi) + tf.math.log(std + TINY) + 0.5 * tf.square(z)), axis=1)\n",
    "        \n",
    "    def sample(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        mean, std = dist_info['mean'], dist_info['stddev']\n",
    "        return mean + std * tf.random.normal(tf.shape(mean))\n",
    "        \n",
    "    def prior_dist_info(self, batch_size: int) -> Dict[str, tf.Tensor]:\n",
    "        mean = tf.zeros((batch_size, self.dim), dtype=floatX)\n",
    "        std = tf.ones((batch_size, self.dim), dtype=floatX)\n",
    "        return {'mean': mean, 'stddev': std}\n",
    "        \n",
    "    def entropy(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        std = dist_info['stddev']\n",
    "        return tf.reduce_sum(0.5 * np.log(2 * np.pi * np.e) + tf.math.log(std + TINY), axis=1)\n",
    "        \n",
    "    def kl(self, p: Dict[str, tf.Tensor], q: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        p_mean, p_std = p['mean'], p['stddev']\n",
    "        q_mean, q_std = q['mean'], q['stddev']\n",
    "        return tf.reduce_sum(\n",
    "            tf.math.log(q_std + TINY) - tf.math.log(p_std + TINY) + \n",
    "            (tf.square(p_std) + tf.square(p_mean - q_mean)) / (2 * tf.square(q_std + TINY)) - 0.5,\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "    def dist_info_keys(self) -> List[str]:\n",
    "        return ['mean', 'stddev']\n",
    "        \n",
    "    def activate_dist(self, flat_dist: tf.Tensor) -> Dict[str, tf.Tensor]:\n",
    "        mean = flat_dist[:, :self.dim]\n",
    "        if self._fix_std:\n",
    "            std = tf.ones_like(mean)\n",
    "        else:\n",
    "            std = tf.sqrt(tf.exp(flat_dist[:, self.dim:]))\n",
    "        return {'mean': mean, 'stddev': std}\n",
    "\n",
    "class Product(Distribution):\n",
    "    \"\"\"Produto de distribuições para combinar diferentes tipos de variáveis latentes.\"\"\"\n",
    "    \n",
    "    def __init__(self, dists: List[Distribution]):\n",
    "        self._dists = dists\n",
    "        \n",
    "    @property\n",
    "    def dim(self) -> int:\n",
    "        return sum(d.dim for d in self._dists)\n",
    "        \n",
    "    @property\n",
    "    def dist_flat_dim(self) -> int:\n",
    "        return sum(d.dist_flat_dim for d in self._dists)\n",
    "    @property\n",
    "    def dists(self):  \n",
    "        return self._dists\n",
    "    \n",
    "    @property\n",
    "    def effective_dim(self) -> int:\n",
    "        return sum(d.effective_dim for d in self._dists)\n",
    "        \n",
    "    def split_dist_info(self, dist_info: Dict[str, tf.Tensor]) -> List[Dict[str, tf.Tensor]]:\n",
    "        \"\"\"Divide um dicionário de parâmetros combinado em dicionários por distribuição.\"\"\"\n",
    "        split_infos = []\n",
    "        for i, dist in enumerate(self._dists):\n",
    "            info = {}\n",
    "            for key in dist.dist_info_keys():\n",
    "                info[key] = dist_info[f'dist{i}_{key}']\n",
    "            split_infos.append(info)\n",
    "        return split_infos\n",
    "        \n",
    "    def join_dist_infos(self, dist_infos: List[Dict[str, tf.Tensor]]) -> Dict[str, tf.Tensor]:\n",
    "        \"\"\"Combina dicionários de parâmetros em um único dicionário.\"\"\"\n",
    "        joint_info = {}\n",
    "        for i, (dist, info) in enumerate(zip(self._dists, dist_infos)):\n",
    "            for key in dist.dist_info_keys():\n",
    "                joint_info[f'dist{i}_{key}'] = info[key]\n",
    "        return joint_info\n",
    "        \n",
    "    def logli(self, x_var: tf.Tensor, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        split_infos = self.split_dist_info(dist_info)\n",
    "        dims = [d.dim for d in self._dists]\n",
    "        split_x = tf.split(x_var, dims, axis=1)\n",
    "        return tf.add_n([d.logli(x, i) for d, x, i in zip(self._dists, split_x, split_infos)])\n",
    "        \n",
    "    def sample(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        split_infos = self.split_dist_info(dist_info)\n",
    "        samples = [d.sample(i) for d, i in zip(self._dists, split_infos)]\n",
    "        return tf.concat(samples, axis=1)\n",
    "        \n",
    "    def prior_dist_info(self, batch_size: int) -> Dict[str, tf.Tensor]:\n",
    "        dist_infos = [d.prior_dist_info(batch_size) for d in self._dists]\n",
    "        return self.join_dist_infos(dist_infos)\n",
    "        \n",
    "    def entropy(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        split_infos = self.split_dist_info(dist_info)\n",
    "        return tf.add_n([d.entropy(i) for d, i in zip(self._dists, split_infos)])\n",
    "        \n",
    "    def kl(self, p: Dict[str, tf.Tensor], q: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        p_split = self.split_dist_info(p)\n",
    "        q_split = self.split_dist_info(q)\n",
    "        return tf.add_n([d.kl(pi, qi) for d, pi, qi in zip(self._dists, p_split, q_split)])\n",
    "        \n",
    "    def dist_info_keys(self) -> List[str]:\n",
    "        keys = []\n",
    "        for i, dist in enumerate(self._dists):\n",
    "            for key in dist.dist_info_keys():\n",
    "                keys.append(f'dist{i}_{key}')\n",
    "        return keys\n",
    "        \n",
    "    def activate_dist(self, flat_dist: tf.Tensor) -> Dict[str, tf.Tensor]:\n",
    "        dist_infos = []\n",
    "        sizes = [d.dist_flat_dim for d in self._dists]\n",
    "        split_flat = tf.split(flat_dist, sizes, axis=1)\n",
    "        for dist, flat in zip(self._dists, split_flat):\n",
    "            dist_infos.append(dist.activate_dist(flat))\n",
    "        return self.join_dist_infos(dist_infos)\n",
    "\n",
    "    def prior_dist_info(self, batch_size: int) -> Dict[str, tf.Tensor]:\n",
    "        infos = [d.prior_dist_info(batch_size) for d in self._dists]  \n",
    "        return self.join_dist_infos(infos)\n",
    "\n",
    "\n",
    "# Modelos Keras \n",
    "#Gerador DCGAN-64: projeção → reshape → 4 transposed‐convs com batch norm + ReLU, final em tanh.\n",
    "def build_generator(z_dim: int, img_shape):\n",
    "    \"\"\"Gerador DCGAN‑64 clássico (4× upsampling → 64×64).\"\"\"\n",
    "    h, w, c = img_shape  # h==w==64\n",
    "\n",
    "    inp = layers.Input(shape=(z_dim,))\n",
    "\n",
    "    # 1) projeção + reshape → 4×4×512\n",
    "    x = layers.Dense(4 * 4 * 512, use_bias=False)(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Reshape((4, 4, 512))(x)\n",
    "\n",
    "    # 2) 8×8×256\n",
    "    x = layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # 3) 16×16×128\n",
    "    x = layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # 4) 32×32×64\n",
    "    x = layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # 5) 64×64×c\n",
    "    x = layers.Conv2DTranspose(c, kernel_size=4, strides=2, padding='same', activation='tanh')(x)\n",
    "\n",
    "    return Model(inp, x, name='Generator')\n",
    "\n",
    "\n",
    "def build_discriminator_q(img_shape, cat_dim, cont_dim):\n",
    "    inp = layers.Input(shape=img_shape)\n",
    "    x = layers.Conv2D(64, 4, 2, 'same')(inp)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    x = layers.Conv2D(128, 4, 2, 'same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(1024)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    # discriminator\n",
    "    d_out = layers.Dense(1, activation='sigmoid', name='d_out')(x)\n",
    "\n",
    "    # Q‐network: duas saídas\n",
    "    q_cat_logits = layers.Dense(cat_dim, name='q_cat_logits')(x)\n",
    "    q_cont_params = layers.Dense(cont_dim * 2, name='q_cont_params')(x)\n",
    "\n",
    "    return Model(inp, [d_out, q_cat_logits, q_cont_params], name='Discriminator_Q')\n",
    "\n",
    "\n",
    "# FID\n",
    "_inception = tf.keras.applications.InceptionV3(include_top=False, pooling='avg', input_shape=(299, 299, 3))\n",
    "\n",
    "FID_BATCH = 1024  \n",
    "\n",
    "def _get_inception_activations(img_uint8, bs: int = FID_BATCH):\n",
    "    \"\"\"Extrai ativações do pool-3 da Inception em minibatches para não estourar RAM.\"\"\"\n",
    "    acts = []\n",
    "    for i in range(0, len(img_uint8), bs):\n",
    "        batch = img_uint8[i:i + bs]\n",
    "        batch = tf.image.resize(batch, (299, 299))\n",
    "        batch = tf.keras.applications.inception_v3.preprocess_input(tf.cast(batch, tf.float32))\n",
    "        acts.append(_inception(batch, training=False))\n",
    "    return tf.concat(acts, axis=0).numpy()\n",
    "\n",
    "def fid_np(real_uint8, gen_uint8):\n",
    "    act1, act2 = _get_inception_activations(real_uint8), _get_inception_activations(gen_uint8)\n",
    "    mu1, mu2 = act1.mean(0), act2.mean(0)\n",
    "    sigma1, sigma2 = np.cov(act1, rowvar=False), np.cov(act2, rowvar=False)\n",
    "    covmean = sqrtm(sigma1 @ sigma2)\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    return float(np.sum((mu1 - mu2) ** 2) + np.trace(sigma1 + sigma2 - 2 * covmean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#métricas\n",
    "def hsic(x: tf.Tensor, y: tf.Tensor):\n",
    "    \"\"\"Calcula o Hilbert-Schmidt Independence Criterion (HSIC)\"\"\"\n",
    "    m = tf.cast(tf.shape(x)[0], tf.float32)\n",
    "    K = tf.matmul(x, x, transpose_b=True)\n",
    "    L = tf.matmul(y, y, transpose_b=True)\n",
    "    H = tf.eye(m) - tf.ones((m, m)) / m\n",
    "    KH = tf.matmul(K, H)\n",
    "    LH = tf.matmul(L, H)\n",
    "    return tf.linalg.trace(tf.matmul(KH, LH)) / (m * m)\n",
    "\n",
    "def mig(codes, factors):\n",
    "    \"\"\"Calcula o Mutual Information Gap (MIG)\"\"\"\n",
    "    # Simplificada\n",
    "    mi_matrix = np.zeros((factors.shape[1], codes.shape[1]))\n",
    "    for i in range(factors.shape[1]):\n",
    "        for j in range(codes.shape[1]):\n",
    "            mi_matrix[i,j] = mutual_info_score(factors[:,i], codes[:,j])\n",
    "    \n",
    "    sorted_mi = np.sort(mi_matrix, axis=0)[::-1]\n",
    "    return np.mean((sorted_mi[0,:] - sorted_mi[1,:]) / sorted_mi[0,:])\n",
    "\n",
    "def sap_score(codes, factors):\n",
    "    \"\"\"Calcula o Separated Attribute Predictability (SAP)\"\"\"\n",
    "    # Simplificada\n",
    "    scores = []\n",
    "    for i in range(factors.shape[1]):\n",
    "        pred_scores = []\n",
    "        for j in range(codes.shape[1]):\n",
    "            pred_scores.append(np.abs(pearsonr(factors[:,i], codes[:,j])[0]))\n",
    "        scores.append(np.mean(pred_scores))\n",
    "    return np.mean(scores)\n",
    "\n",
    "def quase_ortogo(z: tf.Tensor, eps=1e-5):\n",
    "    \"\"\"Calcula métricas de quase-ortogonalidade para códigos latentes\"\"\"\n",
    "    # 1. Normalizar os vetores\n",
    "    zn = tf.math.l2_normalize(z, axis=0)\n",
    "    \n",
    "    # 2. Verificar desvio padrão\n",
    "    std = tf.math.reduce_std(zn, axis=0)\n",
    "    if tf.reduce_any(std < 1e-6):\n",
    "        return False, float(\"nan\"), float(\"nan\"), float(\"nan\")\n",
    "    \n",
    "    # 3. Calcular matriz de Gram\n",
    "    G = tf.matmul(zn, zn, transpose_a=True)\n",
    "    \n",
    "    # 4. Zerar diagonal\n",
    "    mask = tf.eye(tf.shape(G)[0], dtype=tf.bool)\n",
    "    G = tf.where(mask, tf.zeros_like(G), G)\n",
    "    \n",
    "    # 5. Calcular correlações máximas e mínimas\n",
    "    max_corr = tf.reduce_max(tf.abs(G))\n",
    "    min_corr = tf.reduce_min(tf.abs(G))\n",
    "    \n",
    "    # 6. Calcular proporção de pares abaixo de epsilon\n",
    "    below_eps = tf.reduce_sum(tf.cast(tf.abs(G) < eps, dtype=tf.float32))\n",
    "    total_pairs = tf.cast(tf.size(G) - tf.shape(G)[0], tf.float32)\n",
    "    prop = below_eps / total_pairs\n",
    "    \n",
    "    return min_corr.numpy(), max_corr.numpy(), prop.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# InfoGAN Trainer \n",
    "class InfoGANTrainer:\n",
    "    def __init__(self, G, DQ, latent_dist: Product, dataset: CelebADataset, batch_size=64,\n",
    "             info_coeff=1.0, log_dir='logs', ckpt_dir='ckpt', snapshot=1000, max_iter=100_000,\n",
    "             noise_dim=62, cat_dim=10, cont_dim=2):\n",
    "        self.G, self.DQ = G, DQ\n",
    "        self.latent_dist, self.dataset = latent_dist, dataset\n",
    "        self.bs = batch_size\n",
    "        self.info_coeff = info_coeff\n",
    "        self.snapshot = snapshot\n",
    "        self.max_iter = max_iter\n",
    "        self.noise_dim = noise_dim\n",
    "        self.cat_dim   = cat_dim\n",
    "        self.cont_dim  = cont_dim\n",
    "\n",
    "        self.log_dir, self.ckpt_dir = Path(log_dir), Path(ckpt_dir)\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            try:\n",
    "                tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "                tf.config.optimizer.set_jit(True)  # Ativa XLA\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Erro ao configurar GPU: {e}\")\n",
    "        else:\n",
    "            print(\"Executando em CPU\")\n",
    "            \n",
    "        mkdir_p(self.log_dir); mkdir_p(self.ckpt_dir)\n",
    "\n",
    "        self.d_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        self.g_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        self.q_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        \n",
    "        tf.config.optimizer.set_jit(True)  \n",
    "        #tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n",
    "\n",
    "        # Filtra os pesos da Q-head pelo nome\n",
    "        self.q_vars = [\n",
    "            v for v in self.DQ.trainable_variables\n",
    "            if 'q_cat_logits' in v.name or 'q_cont_params' in v.name\n",
    "        ]\n",
    "        \n",
    "        self.ckpt = tf.train.Checkpoint(\n",
    "            generator=self.G,\n",
    "            discriminator=self.DQ,\n",
    "            g_optimizer=self.g_opt,\n",
    "            d_optimizer=self.d_opt,\n",
    "            q_optimizer=self.q_opt\n",
    "        )\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(\n",
    "            self.ckpt,\n",
    "            directory=str(self.ckpt_dir),\n",
    "            max_to_keep=5\n",
    "        )\n",
    "        \n",
    "        # Métricas \n",
    "        self.metric_hist = {\n",
    "            'iter': [],\n",
    "            'FID': [],\n",
    "            'HSIC': [],\n",
    "            'MIG': [],\n",
    "            'SAP': [],\n",
    "            'Ortho_min': [],\n",
    "            'Ortho_max': [],\n",
    "            'Ortho_prop': [],\n",
    "            'D_loss': [],\n",
    "            'G_loss': [],\n",
    "            'MI_loss': []\n",
    "        }\n",
    "        \n",
    "        self.metric_path = self.log_dir / 'metricsGAN.csv'\n",
    "        \n",
    "        # Restaurar o último checkpoint se existir\n",
    "        self.start_iter = 0\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            self.ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Checkpoint restaurado: {self.ckpt_manager.latest_checkpoint}\")\n",
    "            # Extrair o número da iteração do nome do checkpoint\n",
    "            import re\n",
    "            match = re.search(r'ckpt-(\\d+)', self.ckpt_manager.latest_checkpoint)\n",
    "            if match:\n",
    "                self.start_iter = int(match.group(1))\n",
    "                print(f\"Continuando do passo {self.start_iter}\")\n",
    "\n",
    "    def _mi_loss(self, z_reg, q_cat_logits, q_cont_params):\n",
    "        # separa z_reg em z_cat (one-hot) e z_cont (gaussiano)\n",
    "        z_cat  = z_reg[:, :self.cat_dim]\n",
    "        z_cont = z_reg[:, self.cat_dim:]\n",
    "\n",
    "        # 1) Cross-entropy (categórico)\n",
    "        cat_loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=z_cat,\n",
    "                logits=q_cat_logits\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 2) Log-likelihood gaussiano (contínuo)\n",
    "        mean_pred    = q_cont_params[:, :self.cont_dim]\n",
    "        log_std_pred = q_cont_params[:, self.cont_dim:]\n",
    "        std_pred     = tf.exp(log_std_pred)\n",
    "\n",
    "        eps = (z_cont - mean_pred) / (std_pred + TINY)\n",
    "        logli_per_dim = -0.5 * (tf.math.log(2. * np.pi) + 2. * log_std_pred + tf.square(eps))\n",
    "        logli = tf.reduce_sum(logli_per_dim, axis=1)\n",
    "        cont_loss = -tf.reduce_mean(logli)  \n",
    "\n",
    "        return cat_loss + cont_loss\n",
    "\n",
    "    def _evaluate_metrics(self, n_samples=1000):\n",
    "        \"\"\"Calcula todas as métricas usando um conjunto de amostras\"\"\"\n",
    "        # Gerar amostras\n",
    "        real_flat = self.dataset.next_batch(n_samples)\n",
    "        z = self.latent_dist.sample_prior(n_samples)\n",
    "        gen_imgs = self.G(z, training=False).numpy()\n",
    "        \n",
    "        # 1. FID\n",
    "        #fid = fid_np(self.dataset.inverse_transform(real_flat),self.dataset.inverse_transform(gen_imgs.reshape(real_flat.shape))\n",
    "        \n",
    "        # 2. HSIC entre variáveis latentes\n",
    "        hsic_val = hsic(z[:, :self.noise_dim], z[:, self.noise_dim:]).numpy()\n",
    "        \n",
    "        # 3. MIG e SAP \n",
    "        \n",
    "        try:\n",
    "            mig_val = mig(z.numpy(), real_flat.numpy())\n",
    "            sap_val = sap_score(z.numpy(), real_flat.numpy())\n",
    "        except:\n",
    "            mig_val, sap_val = float('nan'), float('nan')\n",
    "        \n",
    "        # 4. Quase-ortogonalidade\n",
    "        ortho_min, ortho_max, ortho_prop = quase_orto(z)\n",
    "            \n",
    "        return {\n",
    "            'HSIC': hsic_val,\n",
    "            'MIG': mig_val,\n",
    "            'SAP': sap_val,\n",
    "            'Ortho_min': ortho_min,\n",
    "            'Ortho_max': ortho_max,\n",
    "            'Ortho_prop': ortho_prop\n",
    "        }\n",
    "            \n",
    "            \n",
    "    @tf.function\n",
    "    def _train_step(self, real_imgs):\n",
    "        # 1) Amostra z\n",
    "        z = self.latent_dist.sample_prior(self.bs)\n",
    "\n",
    "        # Atualiza Discriminador\n",
    "        with tf.GradientTape() as d_tape:\n",
    "            fake = self.G(z, training=True)\n",
    "            d_real, q_cat_logits, q_cont_params = self.DQ(real_imgs, training=True)\n",
    "            d_fake, _, _ = self.DQ(fake, training=True)\n",
    "            d_loss = -tf.reduce_mean(tf.math.log(d_real + TINY) + tf.math.log(1 - d_fake + TINY))\n",
    "\n",
    "        # Calcula gradientes apenas para o Discriminador (excluindo Q)\n",
    "        d_vars = [v for v in self.DQ.trainable_variables \n",
    "                  if 'q_cat_logits' not in v.name and 'q_cont_params' not in v.name]\n",
    "        d_grads = d_tape.gradient(d_loss, d_vars)\n",
    "        self.d_opt.apply_gradients(zip(d_grads, d_vars))\n",
    "\n",
    "        # Atualiza Gerador e Rede Q juntos\n",
    "        with tf.GradientTape(persistent=True) as g_tape:\n",
    "            fake = self.G(z, training=True)\n",
    "            d_fake, q_cat_logits, q_cont_params = self.DQ(fake, training=True)\n",
    "\n",
    "            # Loss adversarial\n",
    "            g_adv_loss = -tf.reduce_mean(tf.math.log(d_fake + TINY))\n",
    "\n",
    "            # Loss de informação mútua\n",
    "            z_reg = z[:, self.noise_dim:]\n",
    "            mi_loss = self._mi_loss(z_reg, q_cat_logits, q_cont_params)\n",
    "\n",
    "            g_loss = g_adv_loss + self.info_coeff * mi_loss\n",
    "\n",
    "        # Gradientes para o Gerador\n",
    "        g_grads = g_tape.gradient(g_loss, self.G.trainable_variables)\n",
    "        self.g_opt.apply_gradients(zip(g_grads, self.G.trainable_variables))\n",
    "\n",
    "        # Gradientes para a Rede Q (apenas MI loss)\n",
    "        q_grads = g_tape.gradient(mi_loss, self.q_vars)\n",
    "        self.q_opt.apply_gradients(zip(q_grads, self.q_vars))\n",
    "\n",
    "        del g_tape  \n",
    "\n",
    "        return d_loss, g_adv_loss, mi_loss\n",
    "\n",
    "    def train(self):\n",
    "        for step in tqdm(range(self.start_iter + 1, self.max_iter + 1), \n",
    "                        initial=self.start_iter + 1, \n",
    "                        total=self.max_iter):\n",
    "            real_flat = self.dataset.next_batch(self.bs)\n",
    "            real_imgs = real_flat.reshape((-1,) + self.dataset.image_shape)\n",
    "            d_loss, g_loss, mi = self._train_step(real_imgs)\n",
    "\n",
    "            if step % self.snapshot == 0:\n",
    "                # Avaliar todas as métricas\n",
    "                metrics = self._evaluate_metrics()\n",
    "                \n",
    "                # Atualizar histórico de métricas\n",
    "                self.metric_hist['iter'].append(step)\n",
    "                self.metric_hist['FID'].append(metrics['FID'])\n",
    "                self.metric_hist['HSIC'].append(metrics['HSIC'])\n",
    "                self.metric_hist['MIG'].append(metrics['MIG'])\n",
    "                self.metric_hist['SAP'].append(metrics['SAP'])\n",
    "                self.metric_hist['Ortho_min'].append(metrics['Ortho_min'])\n",
    "                self.metric_hist['Ortho_max'].append(metrics['Ortho_max'])\n",
    "                self.metric_hist['Ortho_prop'].append(metrics['Ortho_prop'])\n",
    "                self.metric_hist['D_loss'].append(float(d_loss))\n",
    "                self.metric_hist['G_loss'].append(float(g_loss))\n",
    "                self.metric_hist['MI_loss'].append(float(mi))\n",
    "                \n",
    "                # Salvar métricas\n",
    "                pd.DataFrame(self.metric_hist).to_csv(self.metric_path, index=False)\n",
    "                \n",
    "                # Salvar checkpoint\n",
    "                self.ckpt_manager.save(checkpoint_number=step)\n",
    "                \n",
    "                # Gerar amostras visuais\n",
    "                sample_dir = self.log_dir / f'samples_step_{step:06d}'\n",
    "                self.generate_samples(n_samples=16, output_dir=str(sample_dir))\n",
    "                \n",
    "                print(f\"\\nStep {step}:\")\n",
    "                \n",
    "                print(f\"  HSIC: {metrics['HSIC']:.4f}\")\n",
    "                print(f\"  MIG: {metrics['MIG']:.4f}\")\n",
    "                print(f\"  SAP: {metrics['SAP']:.4f}\")\n",
    "                print(f\"  Ortho: min={metrics['Ortho_min']:.4f}, max={metrics['Ortho_max']:.4f}, prop={metrics['Ortho_prop']:.4f}\")\n",
    "                print(f\"  Losses: D={d_loss:.4f}, G={g_loss:.4f}, MI={mi:.4f}\")\n",
    "                print(f\"  Checkpoint salvo em {self.ckpt_dir}/ckpt-{step}\")\n",
    "                print(f\"  Amostras salvas em {sample_dir}\")\n",
    "            \n",
    "            if step + 1 == self.max_iter:\n",
    "                # Gerar amostras\n",
    "                real_flat = self.dataset.next_batch(n_samples)\n",
    "                z = self.latent_dist.sample_prior(n_samples)\n",
    "                gen_imgs = self.G(z, training=False).numpy()\n",
    "\n",
    "                # 1. FID\n",
    "                fid = fid_np(\n",
    "                    self.dataset.inverse_transform(real_flat),\n",
    "                    self.dataset.inverse_transform(gen_imgs.reshape(real_flat.shape)))\n",
    "                print(f\"FID: {fid}\")\n",
    "\n",
    "\n",
    "\n",
    "    def load_latest_checkpoint(self):\n",
    "        \"\"\"Carrega o último checkpoint disponível\"\"\"\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            self.ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Checkpoint carregado: {self.ckpt_manager.latest_checkpoint}\")\n",
    "            return True\n",
    "        print(\"Nenhum checkpoint encontrado para carregar\")\n",
    "        return False\n",
    "\n",
    "    def generate_samples(self, n_samples: int = 8, output_dir: str = 'samples', prefix: str = 'sample'):\n",
    "        \"\"\"Gera e salva amostras como imagens PNG.\"\"\"\n",
    "        mkdir_p(output_dir)\n",
    "        z = self.latent_dist.sample_prior(n_samples)\n",
    "        gen = self.G(z, training=False).numpy()\n",
    "        imgs_uint8 = self.dataset.inverse_transform(gen.reshape(n_samples, -1))\n",
    "\n",
    "        for i, img in enumerate(imgs_uint8):\n",
    "            path = os.path.join(output_dir, f\"{prefix}_{i}.png\")\n",
    "            Image.fromarray(img).save(path)\n",
    "\n",
    "        print(f\"Geradas e salvas {n_samples} amostras em '{output_dir}/'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executando em CPU\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 1/100000 [00:00<?, ?it/s]/home/hanna/Github/mod_gen/.venv/lib/python3.9/site-packages/keras/src/optimizers/base_optimizer.py:855: UserWarning: Gradients do not exist for variables ['q_cat_logits/kernel', 'q_cat_logits/bias', 'q_cont_params/kernel', 'q_cont_params/bias'] when minimizing the loss. If using `model.compile()`, did you forget to provide a `loss` argument?\n",
      "  warnings.warn(\n",
      "  0%|          | 1/100000 [00:01<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "in user code:\n\n    File \"/tmp/ipykernel_2958359/4198870918.py\", line 184, in _train_step  *\n        self.q_opt.apply_gradients(zip(q_grads, self.q_vars))\n    File \"/home/hanna/Github/mod_gen/.venv/lib/python3.9/site-packages/keras/src/optimizers/base_optimizer.py\", line 462, in apply_gradients  **\n        grads, trainable_variables = zip(*grads_and_vars)\n\n    ValueError: not enough values to unpack (expected 2, got 0)\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 16\u001b[0m\n\u001b[1;32m     12\u001b[0m data \u001b[38;5;241m=\u001b[39m CelebADataset(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata/celeba\u001b[39m\u001b[38;5;124m'\u001b[39m, IMG_SHAPE)\n\u001b[1;32m     14\u001b[0m trainer \u001b[38;5;241m=\u001b[39m InfoGANTrainer(G, DQ, latent_dist, data, batch_size\u001b[38;5;241m=\u001b[39mBATCH, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100000\u001b[39m, snapshot\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, \n\u001b[1;32m     15\u001b[0m                              noise_dim\u001b[38;5;241m=\u001b[39mnoise_dim,cat_dim\u001b[38;5;241m=\u001b[39mcat_dim, cont_dim\u001b[38;5;241m=\u001b[39mcont_dim)\n\u001b[0;32m---> 16\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[6], line 196\u001b[0m, in \u001b[0;36mInfoGANTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    194\u001b[0m real_flat \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mnext_batch(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbs)\n\u001b[1;32m    195\u001b[0m real_imgs \u001b[38;5;241m=\u001b[39m real_flat\u001b[38;5;241m.\u001b[39mreshape((\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m,) \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset\u001b[38;5;241m.\u001b[39mimage_shape)\n\u001b[0;32m--> 196\u001b[0m d_loss, g_loss, mi \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mreal_imgs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    198\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m step \u001b[38;5;241m%\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msnapshot \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m    199\u001b[0m     \u001b[38;5;66;03m# Avaliar todas as métricas\u001b[39;00m\n\u001b[1;32m    200\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_evaluate_metrics()\n",
      "File \u001b[0;32m~/Github/mod_gen/.venv/lib/python3.9/site-packages/tensorflow/python/util/traceback_utils.py:153\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m    152\u001b[0m   filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m--> 153\u001b[0m   \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    155\u001b[0m   \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m/tmp/__autograph_generated_filei993mtfz.py:30\u001b[0m, in \u001b[0;36mouter_factory.<locals>.inner_factory.<locals>.tf___train_step\u001b[0;34m(self, real_imgs)\u001b[0m\n\u001b[1;32m     28\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mg_opt\u001b[38;5;241m.\u001b[39mapply_gradients, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mzip\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(g_grads), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mG\u001b[38;5;241m.\u001b[39mtrainable_variables), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     29\u001b[0m q_grads \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(g_tape)\u001b[38;5;241m.\u001b[39mgradient, (ag__\u001b[38;5;241m.\u001b[39mld(mi_loss), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mq_vars), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[0;32m---> 30\u001b[0m ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mq_opt\u001b[38;5;241m.\u001b[39mapply_gradients, (ag__\u001b[38;5;241m.\u001b[39mconverted_call(ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mzip\u001b[39m), (ag__\u001b[38;5;241m.\u001b[39mld(q_grads), ag__\u001b[38;5;241m.\u001b[39mld(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39mq_vars), \u001b[38;5;28;01mNone\u001b[39;00m, fscope),), \u001b[38;5;28;01mNone\u001b[39;00m, fscope)\n\u001b[1;32m     31\u001b[0m g_tape \u001b[38;5;241m=\u001b[39m ag__\u001b[38;5;241m.\u001b[39mUndefined(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mg_tape\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m~/Github/mod_gen/.venv/lib/python3.9/site-packages/keras/src/optimizers/base_optimizer.py:462\u001b[0m, in \u001b[0;36mBaseOptimizer.apply_gradients\u001b[0;34m(self, grads_and_vars)\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mapply_gradients\u001b[39m(\u001b[38;5;28mself\u001b[39m, grads_and_vars):\n\u001b[0;32m--> 462\u001b[0m     grads, trainable_variables \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mzip\u001b[39m(\u001b[38;5;241m*\u001b[39mgrads_and_vars)\n\u001b[1;32m    463\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply(grads, trainable_variables)\n\u001b[1;32m    464\u001b[0m     \u001b[38;5;66;03m# Return iterations for compat with tf.keras.\u001b[39;00m\n",
      "\u001b[0;31mValueError\u001b[0m: in user code:\n\n    File \"/tmp/ipykernel_2958359/4198870918.py\", line 184, in _train_step  *\n        self.q_opt.apply_gradients(zip(q_grads, self.q_vars))\n    File \"/home/hanna/Github/mod_gen/.venv/lib/python3.9/site-packages/keras/src/optimizers/base_optimizer.py\", line 462, in apply_gradients  **\n        grads, trainable_variables = zip(*grads_and_vars)\n\n    ValueError: not enough values to unpack (expected 2, got 0)\n"
     ]
    }
   ],
   "source": [
    "BATCH = 64\n",
    "IMG_SHAPE = (64, 64, 3)\n",
    "    \n",
    "#Parametros para latente (dimensão do vetor \\Re^(noise_dim + cat_dim + cont_dim))\n",
    "noise_dim, cat_dim, cont_dim = 62, 10, 2\n",
    "    \n",
    "# O espaço latente modelado pelo objeto Product que agrupa três distribuições:\n",
    "latent_dist = Product([Gaussian(noise_dim, fix_std=True),Categorical(cat_dim),Gaussian(cont_dim)])\n",
    "\n",
    "G = build_generator(noise_dim + cat_dim + cont_dim, IMG_SHAPE)\n",
    "DQ = build_discriminator_q(IMG_SHAPE, cat_dim, cont_dim)\n",
    "data = CelebADataset('data/celeba', IMG_SHAPE)\n",
    "\n",
    "trainer = InfoGANTrainer(G, DQ, latent_dist, data, batch_size=BATCH, max_iter=100000, snapshot=100, \n",
    "                             noise_dim=noise_dim,cat_dim=cat_dim, cont_dim=cont_dim)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
