{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFOGAN with metrics\n",
    "\n",
    "* souce: https://github.com/openai/InfoGAN/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow: 2.19.0\n",
      "GPU Available: [PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow:\", tf.__version__)\n",
    "print(\"GPU Available:\", tf.config.list_physical_devices('GPU'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Thu Jul 17 03:01:27 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 535.230.02             Driver Version: 535.230.02   CUDA Version: 12.2     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                 Persistence-M | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp   Perf          Pwr:Usage/Cap |         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA RTX A4000               Off | 00000000:55:00.0  On |                  Off |\n",
      "| 41%   36C    P8              29W / 140W |     54MiB / 16376MiB |      0%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A   2814199      G   /usr/lib/xorg/Xorg                           40MiB |\n",
      "|    0   N/A  N/A   2814248      G   /usr/bin/gnome-shell                          8MiB |\n",
      "+---------------------------------------------------------------------------------------+\n",
      "nvcc: NVIDIA (R) Cuda compiler driver\n",
      "Copyright (c) 2005-2023 NVIDIA Corporation\n",
      "Built on Fri_Jan__6_16:45:21_PST_2023\n",
      "Cuda compilation tools, release 12.0, V12.0.140\n",
      "Build cuda_12.0.r12.0/compiler.32267302_0\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi\n",
    "!nvcc --version\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install tensorflow>=2.13 scipy scikit-learn pandas pillow tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install prettytensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dateutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "InfoGAN em TensorFlow 2 / Keras – versão 2\n",
    "-----------------------------------------\n",
    "Compatível com Python ≥ 3.9 e TensorFlow ≥ 2.13 (eager por default).\n",
    "Principais mudanças em relação ao script original:\n",
    "• Todas as chamadas TF‑1.x (tf.random_normal, tf.multinomial, tf.pack, etc.)\n",
    "  foram substituídas pelas equivalentes em TF‑2.x.\n",
    "• Classes de distribuição (Gaussian, Categorical, Product) portadas para a nova API.\n",
    "• Ajuste no cálculo de MI: em vez de depender de `Product.reg_z`, selecionamos\n",
    "  as últimas dimensões do vetor latente (categorical + continuous).\n",
    "• Pequenas correções antipandas/NumPy para garantir execução em Windows + Anaconda.\n",
    "\n",
    "Requisitos:\n",
    " pip install tensorflow>=2.13 scipy scikit-learn pandas pillow tqdm\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "import os, math, json, errno, time\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.linalg import sqrtm\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from sklearn.metrics import mutual_info_score\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from PIL import Image\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Utils ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------\n",
    "TINY = 1e-8\n",
    "floatX = np.float32\n",
    "\n",
    "def mkdir_p(path: str):\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Dataset loader (CelebA + atributos) ----------------------------------------\n",
    "# ----------------------------------------------------------------------------\n",
    "class CelebAWithAttr:\n",
    "    \"\"\"Loader mínimo da CelebA com 40 atributos binários.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir: str = '.', image_shape=(64, 64, 3), split_ratio=0.9):\n",
    "        self.image_shape = image_shape\n",
    "        self.image_dim = int(np.prod(image_shape))\n",
    "        base = Path(root_dir)\n",
    "\n",
    "        # encontra arquivo de atributos\n",
    "        attr_files = list(base.glob('list_attr_celeba.*'))\n",
    "        attr_path = attr_files[0]\n",
    "\n",
    "        # leitura com pandas (auto-separador)\n",
    "        df = pd.read_csv(attr_path, sep=None, engine='python')\n",
    "        fname_col = df.columns[0]\n",
    "        self.attr_names = [c for c in df.columns if c != fname_col]\n",
    "\n",
    "        img_dir = base / 'img_align_celeba'\n",
    "        sub = img_dir / 'img_align_celeba'\n",
    "        if sub.is_dir():\n",
    "            img_dir = sub\n",
    "\n",
    "        self.files = df[fname_col].apply(lambda fn: img_dir / fn).values\n",
    "        attrs = df[self.attr_names].replace(-1, 0).values.astype(np.int8)\n",
    "        self.attrs = attrs\n",
    "\n",
    "        # split train\n",
    "        n_train = int(len(self.files) * split_ratio)\n",
    "        self.train_idx = np.arange(n_train)\n",
    "        np.random.shuffle(self.train_idx)\n",
    "        self.ptr = 0\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        if self.ptr + batch_size > len(self.train_idx):\n",
    "            np.random.shuffle(self.train_idx)\n",
    "            self.ptr = 0\n",
    "        sel = self.train_idx[self.ptr:self.ptr + batch_size]\n",
    "        self.ptr += batch_size\n",
    "        imgs, atts = [], []\n",
    "        for i in sel:\n",
    "            img = Image.open(self.files[i]).resize(self.image_shape[:2])\n",
    "            imgs.append(np.asarray(img, np.float32) / 127.5 - 1.0)\n",
    "            atts.append(self.attrs[i])\n",
    "        x = np.stack(imgs).reshape(batch_size, -1)\n",
    "        a = np.stack(atts)\n",
    "        return x, a\n",
    "\n",
    "    def inverse_transform(self, flat):\n",
    "        imgs = flat.reshape((-1,) + self.image_shape)\n",
    "        return ((imgs + 1.) * 127.5).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Distributions (TF‑2.x) -----------------------------------------------------\n",
    "# ----------------------------------------------------------------------------\n",
    "class Distribution:\n",
    "    @property\n",
    "    def dist_flat_dim(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def dim(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    @property\n",
    "    def effective_dim(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def logli(self, x_var, dist_info):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample(self, dist_info):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def sample_prior(self, batch_size):\n",
    "        return self.sample(self.prior_dist_info(batch_size))\n",
    "\n",
    "    def prior_dist_info(self, batch_size):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    # helpers usados pelo InfoGAN ------------------------------------------\n",
    "    def entropy(self, dist_info):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def marginal_entropy(self, dist_info):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def marginal_logli(self, x_var, dist_info):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def kl(self, p, q):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def dist_info_keys(self):\n",
    "        raise NotImplementedError\n",
    "\n",
    "    def activate_dist(self, flat_dist):\n",
    "        raise NotImplementedError\n",
    "\n",
    "\n",
    "class Categorical(Distribution):\n",
    "    def __init__(self, dim: int):\n",
    "        self._dim = dim\n",
    "\n",
    "    # --- propriedades ------------------------------------------------------\n",
    "    @property\n",
    "    def dim(self): return self._dim\n",
    "    @property\n",
    "    def dist_flat_dim(self): return self._dim\n",
    "    @property\n",
    "    def effective_dim(self): return 1\n",
    "    @property\n",
    "    def dist_info_keys(self): return ['prob']\n",
    "\n",
    "    # --- likelihood / KL ---------------------------------------------------\n",
    "    def logli(self, x_var, dist_info):\n",
    "        prob = dist_info['prob']\n",
    "        return tf.reduce_sum(tf.math.log(prob + TINY) * x_var, axis=1)\n",
    "\n",
    "    def kl(self, p, q):\n",
    "        p_prob, q_prob = p['prob'], q['prob']\n",
    "        return tf.reduce_sum(p_prob * (tf.math.log(p_prob + TINY) - tf.math.log(q_prob + TINY)), axis=1)\n",
    "\n",
    "    # --- sampling ----------------------------------------------------------\n",
    "    def sample(self, dist_info):\n",
    "        prob = dist_info['prob']\n",
    "        ids = tf.random.categorical(tf.math.log(prob + TINY), 1)[:, 0]\n",
    "        onehot = tf.eye(self.dim, dtype=tf.float32)\n",
    "        return tf.nn.embedding_lookup(onehot, ids)\n",
    "\n",
    "    def sample_prior(self, batch_size):\n",
    "        prob = tf.ones([batch_size, self.dim], dtype=floatX) / self.dim\n",
    "        return self.sample(dict(prob=prob))\n",
    "\n",
    "    # --- helpers -----------------------------------------------------------\n",
    "    def activate_dist(self, flat_dist):\n",
    "        return {'prob': tf.nn.softmax(flat_dist)}\n",
    "\n",
    "    def entropy(self, dist_info):\n",
    "        prob = dist_info['prob']\n",
    "        return -tf.reduce_sum(prob * tf.math.log(prob + TINY), axis=1)\n",
    "\n",
    "    def marginal_entropy(self, dist_info):\n",
    "        prob = dist_info['prob']\n",
    "        avg_prob = tf.tile(tf.reduce_mean(prob, axis=0, keepdims=True), [tf.shape(prob)[0], 1])\n",
    "        return self.entropy({'prob': avg_prob})\n",
    "\n",
    "    def marginal_logli(self, x_var, dist_info):\n",
    "        prob = dist_info['prob']\n",
    "        avg_prob = tf.tile(tf.reduce_mean(prob, axis=0, keepdims=True), [tf.shape(prob)[0], 1])\n",
    "        return self.logli(x_var, {'prob': avg_prob})\n",
    "\n",
    "    def nonreparam_logli(self, x_var, dist_info):\n",
    "        return self.logli(x_var, dist_info)\n",
    "\n",
    "\n",
    "class Gaussian(Distribution):\n",
    "    def __init__(self, dim: int, fix_std: bool = False):\n",
    "        self._dim = dim\n",
    "        self._fix_std = fix_std\n",
    "\n",
    "    @property\n",
    "    def dim(self): return self._dim\n",
    "    @property\n",
    "    def dist_flat_dim(self): return self._dim * 2\n",
    "    @property\n",
    "    def effective_dim(self): return self._dim\n",
    "    @property\n",
    "    def dist_info_keys(self): return ['mean', 'stddev']\n",
    "\n",
    "    # --- likelihood / KL ---------------------------------------------------\n",
    "    def logli(self, x_var, dist_info):\n",
    "        mean, std = dist_info['mean'], dist_info['stddev']\n",
    "        eps = (x_var - mean) / (std + TINY)\n",
    "        return tf.reduce_sum(-0.5 * np.log(2 * np.pi) - tf.math.log(std + TINY) - 0.5 * tf.square(eps), axis=1)\n",
    "\n",
    "    def kl(self, p, q):\n",
    "        μ1, σ1 = p['mean'], p['stddev']\n",
    "        μ2, σ2 = q['mean'], q['stddev']\n",
    "        num = tf.square(μ1 - μ2) + tf.square(σ1) - tf.square(σ2)\n",
    "        den = 2. * tf.square(σ2)\n",
    "        return tf.reduce_sum(num / (den + TINY) + tf.math.log(σ2 + TINY) - tf.math.log(σ1 + TINY), axis=1)\n",
    "\n",
    "    # --- sampling ----------------------------------------------------------\n",
    "    def sample(self, dist_info):\n",
    "        mean, std = dist_info['mean'], dist_info['stddev']\n",
    "        eps = tf.random.normal(tf.shape(mean))\n",
    "        return mean + eps * std\n",
    "\n",
    "    def sample_prior(self, batch_size):\n",
    "        return tf.random.normal([batch_size, self.dim])\n",
    "\n",
    "    # --- helpers -----------------------------------------------------------\n",
    "    def activate_dist(self, flat):\n",
    "        mean = flat[:, :self.dim]\n",
    "        if self._fix_std:\n",
    "            std = tf.ones_like(mean)\n",
    "        else:\n",
    "            std = tf.sqrt(tf.exp(flat[:, self.dim:]))\n",
    "        return {'mean': mean, 'stddev': std}\n",
    "\n",
    "    def entropy(self, dist_info):\n",
    "        std = dist_info['stddev']\n",
    "        return tf.reduce_sum(0.5 * np.log(2 * np.pi * np.e) + tf.math.log(std + TINY), axis=1)\n",
    "\n",
    "    def marginal_entropy(self, dist_info):\n",
    "        return self.entropy(dist_info)\n",
    "\n",
    "    def marginal_logli(self, x_var, dist_info):\n",
    "        return self.logli(x_var, dist_info)\n",
    "\n",
    "    def nonreparam_logli(self, x_var, dist_info):\n",
    "        return tf.zeros_like(x_var[:, 0])\n",
    "\n",
    "    def prior_dist_info(self, batch_size):\n",
    "        mean = tf.zeros([batch_size, self.dim])\n",
    "        std = tf.ones([batch_size, self.dim])\n",
    "        return {'mean': mean, 'stddev': std}\n",
    "\n",
    "# Espaço latente\n",
    "class Product(Distribution):\n",
    "    def __init__(self, dists: list[Distribution]):\n",
    "        self._dists = dists\n",
    "\n",
    "    # --- short‑cuts --------------------------------------------------------\n",
    "    @property\n",
    "    def dists(self): return list(self._dists)\n",
    "    @property\n",
    "    def dim(self): return sum(d.dim for d in self.dists)\n",
    "    @property\n",
    "    def effective_dim(self): return sum(d.effective_dim for d in self.dists)\n",
    "    @property\n",
    "    def dist_flat_dim(self): return sum(d.dist_flat_dim for d in self.dists)\n",
    "\n",
    "    def dims(self):\n",
    "        return [d.dim for d in self.dists]\n",
    "\n",
    "    def dist_flat_dims(self):\n",
    "        return [d.dist_flat_dim for d in self.dists]\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    def dist_info_keys(self):\n",
    "        keys = []\n",
    "        for idx, dist in enumerate(self.dists):\n",
    "            for k in dist.dist_info_keys:\n",
    "                keys.append(f'id_{idx}_{k}')\n",
    "        return keys\n",
    "\n",
    "    def split_dist_info(self, dist_info):\n",
    "        ret = []\n",
    "        for idx, dist in enumerate(self.dists):\n",
    "            cur = {k: dist_info[f'id_{idx}_{k}'] for k in dist.dist_info_keys}\n",
    "            ret.append(cur)\n",
    "        return ret\n",
    "\n",
    "    def join_dist_infos(self, infos):\n",
    "        ret = {}\n",
    "        for idx, dist, info in zip(itertools.count(), self.dists, infos):\n",
    "            for k in dist.dist_info_keys:\n",
    "                ret[f'id_{idx}_{k}'] = info[k]\n",
    "        return ret\n",
    "\n",
    "    def split_var(self, x):\n",
    "        cum = np.cumsum([d.dim for d in self.dists])\n",
    "        outs, start = [], 0\n",
    "        for end in cum:\n",
    "            outs.append(x[:, start:end])\n",
    "            start = end\n",
    "        return outs\n",
    "\n",
    "    def split_dist_flat(self, flat):\n",
    "        cum = np.cumsum([d.dist_flat_dim for d in self.dists])\n",
    "        outs, start = [], 0\n",
    "        for end in cum:\n",
    "            outs.append(flat[:, start:end])\n",
    "            start = end\n",
    "        return outs\n",
    "\n",
    "    # --- sampling ----------------------------------------------------------\n",
    "    def sample(self, dist_info):\n",
    "        parts = [tf.cast(d.sample(i), tf.float32) for d, i in zip(self.dists, self.split_dist_info(dist_info))]\n",
    "        return tf.concat(parts, axis=1)\n",
    "\n",
    "    def sample_prior(self, batch_size):\n",
    "        parts = [tf.cast(d.sample_prior(batch_size), tf.float32) for d in self.dists]\n",
    "        return tf.concat(parts, axis=1)\n",
    "\n",
    "    # --- likelihood / entropy / etc. --------------------------------------\n",
    "    def logli(self, x_var, dist_info):\n",
    "        return tf.add_n([d.logli(xi, di) for d, xi, di in zip(self.dists, self.split_var(x_var), self.split_dist_info(dist_info))])\n",
    "\n",
    "    def marginal_logli(self, x_var, dist_info):\n",
    "        return tf.add_n([d.marginal_logli(xi, di) for d, xi, di in zip(self.dists, self.split_var(x_var), self.split_dist_info(dist_info))])\n",
    "\n",
    "    def entropy(self, dist_info):\n",
    "        return tf.add_n([d.entropy(di) for d, di in zip(self.dists, self.split_dist_info(dist_info))])\n",
    "\n",
    "    def marginal_entropy(self, dist_info):\n",
    "        return tf.add_n([d.marginal_entropy(di) for d, di in zip(self.dists, self.split_dist_info(dist_info))])\n",
    "\n",
    "    def nonreparam_logli(self, x_var, dist_info):\n",
    "        return tf.add_n([d.nonreparam_logli(xi, di) for d, xi, di in zip(self.dists, self.split_var(x_var), self.split_dist_info(dist_info))])\n",
    "\n",
    "    def kl(self, p, q):\n",
    "        return tf.add_n([d.kl(pi, qi) for d, pi, qi in zip(self.dists, self.split_dist_info(p), self.split_dist_info(q))])\n",
    "\n",
    "    def activate_dist(self, flat):\n",
    "        ret = {}\n",
    "        for idx, d, f in zip(itertools.count(), self.dists, self.split_dist_flat(flat)):\n",
    "            info = d.activate_dist(f)\n",
    "            for k, v in info.items():\n",
    "                ret[f'id_{idx}_{k}'] = v\n",
    "        return ret\n",
    "\n",
    "    def prior_dist_info(self, batch_size):\n",
    "        infos = [d.prior_dist_info(batch_size) for d in self.dists]\n",
    "        return self.join_dist_infos(infos)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Modelos Keras --------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------\n",
    "\n",
    "#Gerador DCGAN-64: projeção → reshape → 4 transposed‐convs com batch norm + ReLU, final em tanh.\n",
    "def build_generator(z_dim: int, img_shape):\n",
    "    \"\"\"Gerador DCGAN‑64 clássico (4× upsampling → 64×64).\"\"\"\n",
    "    h, w, c = img_shape  # h==w==64\n",
    "\n",
    "    inp = layers.Input(shape=(z_dim,))\n",
    "\n",
    "    # 1) projeção + reshape → 4×4×512\n",
    "    x = layers.Dense(4 * 4 * 512, use_bias=False)(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Reshape((4, 4, 512))(x)\n",
    "\n",
    "    # 2) 8×8×256\n",
    "    x = layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # 3) 16×16×128\n",
    "    x = layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # 4) 32×32×64\n",
    "    x = layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # 5) 64×64×c\n",
    "    x = layers.Conv2DTranspose(c, kernel_size=4, strides=2, padding='same', activation='tanh')(x)\n",
    "\n",
    "    return Model(inp, x, name='Generator')\n",
    "\n",
    "\n",
    "def build_discriminator_q(img_shape, cat_dim, cont_dim):\n",
    "    inp = layers.Input(shape=img_shape)\n",
    "    x = layers.Conv2D(64, 4, 2, 'same')(inp)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    x = layers.Conv2D(128, 4, 2, 'same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(1024)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    # discriminator\n",
    "    d_out = layers.Dense(1, activation='sigmoid', name='d_out')(x)\n",
    "\n",
    "    # Q‐network: duas saídas\n",
    "    q_cat_logits = layers.Dense(cat_dim, name='q_cat_logits')(x)\n",
    "    q_cont_params = layers.Dense(cont_dim * 2, name='q_cont_params')(x)\n",
    "\n",
    "    return Model(inp, [d_out, q_cat_logits, q_cont_params], name='Discriminator_Q')\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Métricas\n",
    "\n",
    "_inception = tf.keras.applications.InceptionV3(include_top=False, pooling='avg', input_shape=(299, 299, 3))\n",
    "\n",
    "# ------------------------- FID helpers (stream‑safe) -------------------------\n",
    "FID_BATCH = 256  # nº de imagens por forward pass no Inception (evita OOM)\n",
    "\n",
    "\n",
    "def _get_inception_activations(img_uint8, bs: int = FID_BATCH):\n",
    "    \"\"\"Extrai ativações do pool-3 da Inception em minibatches para não estourar RAM.\"\"\"\n",
    "    acts = []\n",
    "    for i in range(0, len(img_uint8), bs):\n",
    "        batch = img_uint8[i:i + bs]\n",
    "        batch = tf.image.resize(batch, (299, 299))\n",
    "        batch = tf.keras.applications.inception_v3.preprocess_input(tf.cast(batch, tf.float32))\n",
    "        acts.append(_inception(batch, training=False))\n",
    "    # Retorna só o array numpy resultante\n",
    "    return tf.concat(acts, axis=0).numpy()\n",
    "\n",
    "\n",
    "def fid_np(real_uint8, gen_uint8):\n",
    "    act1, act2 = _get_inception_activations(real_uint8), _get_inception_activations(gen_uint8)\n",
    "    mu1, mu2 = act1.mean(0), act2.mean(0)\n",
    "    sigma1, sigma2 = np.cov(act1, rowvar=False), np.cov(act2, rowvar=False)\n",
    "    covmean = sqrtm(sigma1 @ sigma2)\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    return float(np.sum((mu1 - mu2) ** 2) + np.trace(sigma1 + sigma2 - 2 * covmean))\n",
    "\n",
    "# --- HSIC / MIG / SAP / quase-ortogonal -----------------------------------\n",
    "\n",
    "def _hsic(K, L):\n",
    "    n = K.shape[0]\n",
    "    H = np.eye(n) - np.ones((n, n)) / n\n",
    "    HKH, HLH = H @ K @ H, H @ L @ H\n",
    "    return np.trace(HKH @ HLH) / ((n - 1) ** 2)\n",
    "\n",
    "def metric_hsic(z: np.ndarray, attr: np.ndarray):\n",
    "    n, d = z.shape\n",
    "    hsic_vals = []\n",
    "    for k in range(d):\n",
    "        zk = z[:, k:k + 1]\n",
    "        K = np.exp(-squareform(pdist(zk, 'sqeuclidean')) / (np.median(zk) ** 2 + 1e-8))\n",
    "        for j in range(attr.shape[1]):\n",
    "            aj = attr[:, j:j + 1]\n",
    "            L = (aj == aj.T).astype(np.float32)\n",
    "            hsic_vals.append(_hsic(K, L))\n",
    "    return float(np.mean(hsic_vals))\n",
    "\n",
    "def metric_mutual_info(z, attr):\n",
    "    n_lat = z.shape[1]\n",
    "    mi = np.zeros((n_lat, attr.shape[1]))\n",
    "    for i in range(n_lat):\n",
    "        zi_disc = np.digitize(z[:, i], np.histogram(z[:, i], bins=20)[1][:-1])\n",
    "        for j in range(attr.shape[1]):\n",
    "            mi[i, j] = mutual_info_score(zi_disc, attr[:, j])\n",
    "    return mi\n",
    "\n",
    "def metric_mig(z, attr):\n",
    "    mi = metric_mutual_info(z, attr)\n",
    "    entropy_attr = np.array([mutual_info_score(attr[:, j], attr[:, j]) for j in range(attr.shape[1])])\n",
    "    sorted_mi = -np.sort(-mi, axis=0)\n",
    "    gap = (sorted_mi[0] - sorted_mi[1]) / (entropy_attr + 1e-12)\n",
    "    return float(np.mean(gap))\n",
    "\n",
    "def metric_sap(z, attr):\n",
    "    mi = metric_mutual_info(z, attr)\n",
    "    sorted_mi = -np.sort(-mi, axis=0)\n",
    "    return float(np.mean(sorted_mi[0] - sorted_mi[1]))\n",
    "\n",
    "def quasi_orthogonality(z):\n",
    "    zc = z - z.mean(0)\n",
    "    cov = np.cov(zc, rowvar=False)\n",
    "    off = cov - np.diag(np.diag(cov))\n",
    "    max_abs = np.max(np.abs(off))\n",
    "    return float(max_abs < 1e-5), float(max_abs)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# InfoGAN Trainer \n",
    "\n",
    "class InfoGANTrainer:\n",
    "    def __init__(self, G, DQ, latent_dist: Product, dataset: CelebAWithAttr, batch_size=64,\n",
    "                 info_coeff=1.0, log_dir='logs', ckpt_dir='ckpt', snapshot=1000, max_iter=100_000,\n",
    "                 noise_dim=62, cat_dim=10, cont_dim=2):\n",
    "        self.G, self.DQ = G, DQ\n",
    "        self.latent_dist, self.dataset = latent_dist, dataset\n",
    "        self.bs = batch_size\n",
    "        self.info_coeff = info_coeff\n",
    "        self.snapshot = snapshot\n",
    "        self.max_iter = max_iter\n",
    "        self.noise_dim = noise_dim\n",
    "        self.cat_dim   = cat_dim\n",
    "        self.cont_dim  = cont_dim\n",
    "\n",
    "        self.log_dir, self.ckpt_dir = Path(log_dir), Path(ckpt_dir)\n",
    "        mkdir_p(self.log_dir); mkdir_p(self.ckpt_dir)\n",
    "\n",
    "        self.d_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        self.g_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        # adicionamos um otimizador separado para a cabeça Q\n",
    "        self.q_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "\n",
    "        # filtramos os pesos da Q-head pelo nome\n",
    "        self.q_vars = [\n",
    "            v for v in self.DQ.trainable_variables\n",
    "            if 'q_cat_logits' in v.name or 'q_cont_params' in v.name\n",
    "        ]\n",
    "\n",
    "        self.metric_hist = {'iter': [], 'FID': [], 'HSIC': [], 'MIG': [], 'SAP': [], 'OrthoMax': []} if not self.log_dir.is_dir() else pd.DataFrame(self.log_dir / 'info_gan_metrics.csv').to_dict(orient='list')\n",
    "        self.metric_path = self.log_dir / 'info_gan_metrics.csv'\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    def _mi_loss(self, z_reg, q_cat_logits, q_cont_params):\n",
    "        # separa z_reg em z_cat (one‐hot) e z_cont (gaussiano)\n",
    "        z_cat  = z_reg[:, :self.cat_dim]\n",
    "        z_cont = z_reg[:, self.cat_dim:]\n",
    "\n",
    "        # --- 1) Cross‐entropy (categórico) -----------------------------\n",
    "        cat_loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=z_cat,\n",
    "                logits=q_cat_logits\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # --- 2) Log‐likelihood gaussiano (contínuo) ------------------\n",
    "        # previsão de mean e log‐std\n",
    "        mean_pred    = q_cont_params[:, :self.cont_dim]\n",
    "        log_std_pred = q_cont_params[:, self.cont_dim:]\n",
    "        std_pred     = tf.exp(log_std_pred)\n",
    "\n",
    "        # log‐likelihood de cada dimensão\n",
    "        eps = (z_cont - mean_pred) / (std_pred + TINY)\n",
    "        # log N(z;μ,σ) = −½·(log(2π) + 2·logσ + ε²)\n",
    "        logli_per_dim = -0.5 * (tf.math.log(2. * np.pi) + 2. * log_std_pred + tf.square(eps))\n",
    "        logli = tf.reduce_sum(logli_per_dim, axis=1)  # soma sobre dims\n",
    "\n",
    "        cont_loss = -tf.reduce_mean(logli)  # NLL\n",
    "\n",
    "        return cat_loss + cont_loss\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    @tf.function\n",
    "    def _train_step(self, real_imgs):\n",
    "        # 1) amostra z\n",
    "        z = self.latent_dist.sample_prior(self.bs)\n",
    "\n",
    "        \n",
    "        # Atualiza apenas o Discriminador (sem MI)\n",
    "        with tf.GradientTape() as d_tape:\n",
    "            fake = self.G(z, training=True)\n",
    "            d_real, _, _ = self.DQ(real_imgs, training=True)\n",
    "            d_fake, _, _ = self.DQ(fake,    training=True)\n",
    "            d_loss = -tf.reduce_mean(\n",
    "                tf.math.log(d_real + TINY) +\n",
    "                tf.math.log(1 - d_fake + TINY)\n",
    "            )\n",
    "        d_grads = d_tape.gradient(d_loss, self.DQ.trainable_variables)\n",
    "        self.d_opt.apply_gradients(zip(d_grads, self.DQ.trainable_variables))\n",
    "\n",
    "        \n",
    "        # Atualiza o Gerador (adversarial + MI)\n",
    "        \n",
    "        with tf.GradientTape() as g_tape:\n",
    "            fake = self.G(z, training=True)\n",
    "            d_fake, q_cat_logits, q_cont_params = self.DQ(fake, training=True)\n",
    "\n",
    "            # adversarial\n",
    "            g_adv_loss = -tf.reduce_mean(tf.math.log(d_fake + TINY))\n",
    "\n",
    "            # MI\n",
    "            z_reg = z[:, self.noise_dim:]\n",
    "            mi_loss = self._mi_loss(z_reg, q_cat_logits, q_cont_params)\n",
    "\n",
    "            g_loss = g_adv_loss + self.info_coeff * mi_loss\n",
    "\n",
    "        g_grads = g_tape.gradient(g_loss, self.G.trainable_variables)\n",
    "        self.g_opt.apply_gradients(zip(g_grads, self.G.trainable_variables))\n",
    "\n",
    "         \n",
    "        # Atualiza somente a cabeça Q (MI)\n",
    "        \n",
    "        with tf.GradientTape() as q_tape:\n",
    "            fake = self.G(z, training=False)  # G congelado aqui\n",
    "            _, q_cat_logits, q_cont_params = self.DQ(fake, training=True)\n",
    "            z_reg = z[:, self.noise_dim:]\n",
    "            mi_loss = self._mi_loss(z_reg, q_cat_logits, q_cont_params)\n",
    "\n",
    "        q_grads = q_tape.gradient(mi_loss, self.q_vars)\n",
    "        self.q_opt.apply_gradients(zip(q_grads, self.q_vars))\n",
    "\n",
    "        return d_loss, g_adv_loss, mi_loss\n",
    "\n",
    "\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    def _evaluate_metrics(self):\n",
    "        #Calcula FID (±1 k amostras) + métricas de disentanglement sem estourar memória.\"\"\"\n",
    "        N_EVAL = 1000  # amostras para FID / HSIC / etc.\n",
    "        real_flat, real_attr = self.dataset.next_batch(N_EVAL)\n",
    "        z = self.latent_dist.sample_prior(N_EVAL)\n",
    "        gen_imgs = self.G(z, training=False).numpy()\n",
    "\n",
    "        fid = fid_np(\n",
    "                self.dataset.inverse_transform(real_flat),\n",
    "                self.dataset.inverse_transform(gen_imgs.reshape(real_flat.shape))\n",
    "            )\n",
    "\n",
    "        z_np = z.numpy()\n",
    "        hsic = metric_hsic(z_np, real_attr)\n",
    "        mig = metric_mig(z_np, real_attr)\n",
    "        sap = metric_sap(z_np, real_attr)\n",
    "        _, omax = quasi_orthogonality(z_np)\n",
    "        return fid, hsic, mig, sap, omax\n",
    "\n",
    "    # ----------------------------------------------------------------------\n",
    "    def train(self):\n",
    "        for step in tqdm(range(1, self.max_iter + 1)):\n",
    "            real_flat, _ = self.dataset.next_batch(self.bs)\n",
    "            real_imgs = real_flat.reshape((-1,) + self.dataset.image_shape)\n",
    "            d_loss, g_loss, mi = self._train_step(real_imgs)\n",
    "\n",
    "            if step % self.snapshot == 0:\n",
    "                fid, hsic, mig, sap, omax = self._evaluate_metrics()\n",
    "                self.metric_hist['iter'].append(step)\n",
    "                self.metric_hist['FID'].append(fid)\n",
    "                self.metric_hist['HSIC'].append(hsic)\n",
    "                self.metric_hist['MIG'].append(mig)\n",
    "                self.metric_hist['SAP'].append(sap)\n",
    "                self.metric_hist['OrthoMax'].append(omax)\n",
    "                pd.DataFrame(self.metric_hist).to_csv(self.metric_path, index=False)\n",
    "                ckpt_path = str(self.ckpt_dir / f'ckpt_{step}')\n",
    "                self.ckpt.save(ckpt_path)\n",
    "                print(f\"Step {step}: FID {fid:.1f} | HSIC {hsic:.4f} | MIG {mig:.4f} | SAP {sap:.4f} | max|off‑diag| {omax:.3e}\")\n",
    "    \n",
    "    def generate_samples(self,\n",
    "                         n_samples: int = 8,\n",
    "                         output_dir: str = 'samples',\n",
    "                         prefix: str = 'sample'):\n",
    "        \"\"\"\n",
    "        Gera n amostras usando o G treinado e salva como PNG em output_dir.\n",
    "        Args:\n",
    "            n_samples: número de imagens a gerar (padrão 8).\n",
    "            output_dir: pasta onde os arquivos serão salvos.\n",
    "            prefix: prefixo do nome de cada arquivo (ex: 'sample_0.png').\n",
    "        \"\"\"\n",
    "        # garante que a pasta existe\n",
    "        mkdir_p(output_dir)\n",
    "\n",
    "        # amostra do espaço latente\n",
    "        z = self.latent_dist.sample_prior(n_samples)\n",
    "        # gera imagens\n",
    "        gen = self.G(z, training=False).numpy()\n",
    "        # converte do formato flat [-1,1] para uint8 [0,255]\n",
    "        imgs_uint8 = self.dataset.inverse_transform(gen.reshape(n_samples, -1))\n",
    "\n",
    "        # salva cada imagem\n",
    "        for i, img in enumerate(imgs_uint8):\n",
    "            path = os.path.join(output_dir, f\"{prefix}_{i}.png\")\n",
    "            Image.fromarray(img).save(path)\n",
    "\n",
    "        print(f\"Geradas e salvas {n_samples} amostras em '{output_dir}/'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mIndexError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 12\u001b[39m\n\u001b[32m     10\u001b[39m G = build_generator(noise_dim + cat_dim + cont_dim, IMG_SHAPE)\n\u001b[32m     11\u001b[39m DQ = build_discriminator_q(IMG_SHAPE, cat_dim, cont_dim)\n\u001b[32m---> \u001b[39m\u001b[32m12\u001b[39m data = \u001b[43mCelebAWithAttr\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43m.\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mIMG_SHAPE\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     14\u001b[39m trainer = InfoGANTrainer(G, DQ, latent_dist, data, batch_size=BATCH, max_iter=\u001b[32m300000\u001b[39m, snapshot=\u001b[32m1000\u001b[39m, \n\u001b[32m     15\u001b[39m                              noise_dim=noise_dim,cat_dim=cat_dim, cont_dim=cont_dim)\n\u001b[32m     16\u001b[39m trainer.train()\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[8]\u001b[39m\u001b[32m, line 52\u001b[39m, in \u001b[36mCelebAWithAttr.__init__\u001b[39m\u001b[34m(self, root_dir, image_shape, split_ratio)\u001b[39m\n\u001b[32m     50\u001b[39m \u001b[38;5;66;03m# encontra arquivo de atributos\u001b[39;00m\n\u001b[32m     51\u001b[39m attr_files = \u001b[38;5;28mlist\u001b[39m(base.glob(\u001b[33m'\u001b[39m\u001b[33mlist_attr_celeba.*\u001b[39m\u001b[33m'\u001b[39m))\n\u001b[32m---> \u001b[39m\u001b[32m52\u001b[39m attr_path = \u001b[43mattr_files\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# leitura com pandas (auto-separador)\u001b[39;00m\n\u001b[32m     55\u001b[39m df = pd.read_csv(attr_path, sep=\u001b[38;5;28;01mNone\u001b[39;00m, engine=\u001b[33m'\u001b[39m\u001b[33mpython\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[31mIndexError\u001b[39m: list index out of range"
     ]
    }
   ],
   "source": [
    "BATCH = 64\n",
    "IMG_SHAPE = (64, 64, 3)\n",
    "    \n",
    "#Parametros para latente (dimensão do vetor \\Re^(noise_dim + cat_dim + cont_dim))\n",
    "noise_dim, cat_dim, cont_dim = 62, 10, 2\n",
    "    \n",
    "# O espaço latente modelado pelo objeto Product que agrupa três distribuições:\n",
    "latent_dist = Product([Gaussian(noise_dim, fix_std=True),Categorical(cat_dim),Gaussian(cont_dim)])\n",
    "\n",
    "G = build_generator(noise_dim + cat_dim + cont_dim, IMG_SHAPE)\n",
    "DQ = build_discriminator_q(IMG_SHAPE, cat_dim, cont_dim)\n",
    "data = CelebAWithAttr('.', IMG_SHAPE)\n",
    "\n",
    "trainer = InfoGANTrainer(G, DQ, latent_dist, data, batch_size=BATCH, max_iter=300000, snapshot=1000, \n",
    "                             noise_dim=noise_dim,cat_dim=cat_dim, cont_dim=cont_dim)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mod-gen",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
