{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFOGAN with metrics\n",
    "\n",
    "* souce: https://github.com/openai/InfoGAN/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install prettytensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dateutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, math, json, errno, time\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.linalg import sqrtm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from PIL import Image\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Utils ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------\n",
    "TINY = 1e-8\n",
    "floatX = np.float32\n",
    "\n",
    "def mkdir_p(path: str):\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "# Dataset loader (CelebA)\n",
    "class CelebADataset:\n",
    "    \"\"\"Loader simplificado da CelebA sem atributos.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir: str = '.', image_shape=(64, 64, 3), split_ratio=0.9):\n",
    "        self.image_shape = image_shape\n",
    "        self.image_dim = int(np.prod(image_shape))\n",
    "        base = Path(root_dir)\n",
    "\n",
    "        # Encontra todas as imagens no diretório\n",
    "        img_dir = base / 'img_align_celeba'\n",
    "        sub = img_dir / 'img_align_celeba'\n",
    "        if sub.is_dir():\n",
    "            img_dir = sub\n",
    "            \n",
    "        self.files = sorted([f for f in img_dir.glob('*.jpg')])\n",
    "        \n",
    "        # Split train\n",
    "        n_train = int(len(self.files) * split_ratio)\n",
    "        self.train_idx = np.arange(n_train)\n",
    "        np.random.shuffle(self.train_idx)\n",
    "        self.ptr = 0\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        if self.ptr + batch_size > len(self.train_idx):\n",
    "            np.random.shuffle(self.train_idx)\n",
    "            self.ptr = 0\n",
    "        sel = self.train_idx[self.ptr:self.ptr + batch_size]\n",
    "        self.ptr += batch_size\n",
    "        imgs = []\n",
    "        for i in sel:\n",
    "            img = Image.open(self.files[i]).resize(self.image_shape[:2])\n",
    "            imgs.append(np.asarray(img, np.float32) / 127.5 - 1.0)\n",
    "        x = np.stack(imgs).reshape(batch_size, -1)\n",
    "        return x\n",
    "\n",
    "    def inverse_transform(self, flat):\n",
    "        imgs = flat.reshape((-1,) + self.image_shape)\n",
    "        return ((imgs + 1.) * 127.5).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "\n",
    "# Distributions \n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "TINY = 1e-8\n",
    "floatX = np.float32\n",
    "\n",
    "class Distribution:\n",
    "    \"\"\"Classe base abstrata para distribuições latentes no InfoGAN.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def dim(self) -> int:\n",
    "        \"\"\"Dimensão da variável aleatória (tamanho do vetor de saída).\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @property\n",
    "    def dist_flat_dim(self) -> int:\n",
    "        \"\"\"Dimensão do vetor plano de parâmetros da distribuição.\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @property\n",
    "    def effective_dim(self) -> int:\n",
    "        \"\"\"Dimensão efetiva para cálculo de mutual information.\"\"\"\n",
    "        return self.dim\n",
    "        \n",
    "    def logli(self, x_var: tf.Tensor, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Calcula o log-likelihood log p(x|θ) para cada amostra no batch.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def sample(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Gera amostras da distribuição parametrizada por dist_info.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def sample_prior(self, batch_size: int) -> tf.Tensor:\n",
    "        \"\"\"Gera amostras da distribuição prévia (prior).\"\"\"\n",
    "        return self.sample(self.prior_dist_info(batch_size))\n",
    "        \n",
    "    def prior_dist_info(self, batch_size: int) -> Dict[str, tf.Tensor]:\n",
    "        \"\"\"\n",
    "        Retorna os parâmetros da distribuição prévia (prior).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def entropy(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Calcula a entropia H[p(x|θ)] para cada amostra no batch.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def marginal_entropy(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Calcula a entropia marginal (média sobre o batch).\n",
    "        \"\"\"\n",
    "        # Implementação padrão: média dos parâmetros no batch\n",
    "        avg_dist_info = {\n",
    "            k: tf.tile(tf.reduce_mean(v, axis=0, keepdims=True), \n",
    "            [tf.shape(v)[0], *[1]*(len(v.shape)-1)])\n",
    "            for k, v in dist_info.items()\n",
    "        }\n",
    "        return self.entropy(avg_dist_info)\n",
    "        \n",
    "    def marginal_logli(self, x_var: tf.Tensor, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Log-likelihood usando parâmetros marginais (média no batch).\n",
    "        \"\"\"\n",
    "        avg_dist_info = {\n",
    "            k: tf.tile(tf.reduce_mean(v, axis=0, keepdims=True), \n",
    "            [tf.shape(v)[0], *[1]*(len(v.shape)-1)])\n",
    "            for k, v in dist_info.items()\n",
    "        }\n",
    "        return self.logli(x_var, avg_dist_info)\n",
    "        \n",
    "    def kl(self, p: Dict[str, tf.Tensor], q: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Calcula a divergência KL entre duas distribuições (KL(p||q)).\n",
    "        Implementação padrão usando entropia cruzada e entropia:\n",
    "        KL(p||q) = H(p,q) - H(p)\n",
    "        \"\"\"\n",
    "        cross_entropy = -self.logli(p['samples'], q)\n",
    "        entropy = self.entropy(p)\n",
    "        return cross_entropy - entropy\n",
    "        \n",
    "    def dist_info_keys(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Lista de chaves no dicionário de parâmetros da distribuição.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def activate_dist(self, flat_dist: tf.Tensor) -> Dict[str, tf.Tensor]:\n",
    "        \"\"\"\n",
    "        Converte um vetor plano de parâmetros em parâmetros da distribuição.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def nonreparam_logli(self, x_var: tf.Tensor, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Log-likelihood para distribuições sem reparameterization trick.\n",
    "        \"\"\"\n",
    "        return tf.zeros_like(x_var[:, 0])\n",
    "\n",
    "class Categorical(Distribution):\n",
    "    \"\"\"Distribuição categórica (one-hot) para variáveis latentes discretas.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int):\n",
    "        self._dim = dim\n",
    "        \n",
    "    @property\n",
    "    def dim(self) -> int:\n",
    "        return self._dim\n",
    "        \n",
    "    @property\n",
    "    def dist_flat_dim(self) -> int:\n",
    "        return self._dim\n",
    "        \n",
    "    @property\n",
    "    def effective_dim(self) -> int:\n",
    "        return 1  # Apesar de ter N categorias, a dimensão efetiva é 1\n",
    "        \n",
    "    def logli(self, x_var: tf.Tensor, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        prob = dist_info['prob']\n",
    "        return tf.reduce_sum(x_var * tf.math.log(prob + TINY), axis=1)\n",
    "        \n",
    "    def sample(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        prob = dist_info['prob']\n",
    "        # Gera amostras usando Gumbel-Softmax trick para diferenciabilidade\n",
    "        logits = tf.math.log(prob + TINY)\n",
    "        gumbel_noise = -tf.math.log(-tf.math.log(tf.random.uniform(tf.shape(logits), dtype=floatX)))\n",
    "        samples = tf.nn.softmax((logits + gumbel_noise) / 1.0)  # temperatura=1.0\n",
    "        return samples\n",
    "        \n",
    "    def prior_dist_info(self, batch_size: int) -> Dict[str, tf.Tensor]:\n",
    "        prob = tf.ones((batch_size, self.dim), dtype=floatX) / self.dim\n",
    "        return {'prob': prob}\n",
    "        \n",
    "    def entropy(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        prob = dist_info['prob']\n",
    "        return -tf.reduce_sum(prob * tf.math.log(prob + TINY), axis=1)\n",
    "        \n",
    "    def kl(self, p: Dict[str, tf.Tensor], q: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        p_prob, q_prob = p['prob'], q['prob']\n",
    "        return tf.reduce_sum(p_prob * (tf.math.log(p_prob + TINY) - tf.math.log(q_prob + TINY)), axis=1)\n",
    "        \n",
    "    def dist_info_keys(self) -> List[str]:\n",
    "        return ['prob']\n",
    "        \n",
    "    def activate_dist(self, flat_dist: tf.Tensor) -> Dict[str, tf.Tensor]:\n",
    "        return {'prob': tf.nn.softmax(flat_dist)}\n",
    "\n",
    "class Gaussian(Distribution):\n",
    "    \"\"\"Distribuição gaussiana para variáveis latentes contínuas.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, fix_std: bool = False):\n",
    "        self._dim = dim\n",
    "        self._fix_std = fix_std\n",
    "        \n",
    "    @property\n",
    "    def dim(self) -> int:\n",
    "        return self._dim\n",
    "        \n",
    "    @property\n",
    "    def dist_flat_dim(self) -> int:\n",
    "        return self._dim * 2 if not self._fix_std else self._dim\n",
    "        \n",
    "    def logli(self, x_var: tf.Tensor, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        mean, std = dist_info['mean'], dist_info['stddev']\n",
    "        z = (x_var - mean) / (std + TINY)\n",
    "        return tf.reduce_sum(-0.5 * (np.log(2 * np.pi) + tf.math.log(std + TINY) + 0.5 * tf.square(z)), axis=1)\n",
    "        \n",
    "    def sample(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        mean, std = dist_info['mean'], dist_info['stddev']\n",
    "        return mean + std * tf.random.normal(tf.shape(mean))\n",
    "        \n",
    "    def prior_dist_info(self, batch_size: int) -> Dict[str, tf.Tensor]:\n",
    "        mean = tf.zeros((batch_size, self.dim), dtype=floatX)\n",
    "        std = tf.ones((batch_size, self.dim), dtype=floatX)\n",
    "        return {'mean': mean, 'stddev': std}\n",
    "        \n",
    "    def entropy(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        std = dist_info['stddev']\n",
    "        return tf.reduce_sum(0.5 * np.log(2 * np.pi * np.e) + tf.math.log(std + TINY), axis=1)\n",
    "        \n",
    "    def kl(self, p: Dict[str, tf.Tensor], q: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        p_mean, p_std = p['mean'], p['stddev']\n",
    "        q_mean, q_std = q['mean'], q['stddev']\n",
    "        return tf.reduce_sum(\n",
    "            tf.math.log(q_std + TINY) - tf.math.log(p_std + TINY) + \n",
    "            (tf.square(p_std) + tf.square(p_mean - q_mean)) / (2 * tf.square(q_std + TINY)) - 0.5,\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "    def dist_info_keys(self) -> List[str]:\n",
    "        return ['mean', 'stddev']\n",
    "        \n",
    "    def activate_dist(self, flat_dist: tf.Tensor) -> Dict[str, tf.Tensor]:\n",
    "        mean = flat_dist[:, :self.dim]\n",
    "        if self._fix_std:\n",
    "            std = tf.ones_like(mean)\n",
    "        else:\n",
    "            std = tf.sqrt(tf.exp(flat_dist[:, self.dim:]))\n",
    "        return {'mean': mean, 'stddev': std}\n",
    "\n",
    "class Product(Distribution):\n",
    "    \"\"\"Produto de distribuições para combinar diferentes tipos de variáveis latentes.\"\"\"\n",
    "    \n",
    "    def __init__(self, dists: List[Distribution]):\n",
    "        self._dists = dists\n",
    "        \n",
    "    @property\n",
    "    def dim(self) -> int:\n",
    "        return sum(d.dim for d in self._dists)\n",
    "        \n",
    "    @property\n",
    "    def dist_flat_dim(self) -> int:\n",
    "        return sum(d.dist_flat_dim for d in self._dists)\n",
    "    @property\n",
    "    def dists(self):  \n",
    "        return self._dists\n",
    "    \n",
    "    @property\n",
    "    def effective_dim(self) -> int:\n",
    "        return sum(d.effective_dim for d in self._dists)\n",
    "        \n",
    "    def split_dist_info(self, dist_info: Dict[str, tf.Tensor]) -> List[Dict[str, tf.Tensor]]:\n",
    "        \"\"\"Divide um dicionário de parâmetros combinado em dicionários por distribuição.\"\"\"\n",
    "        split_infos = []\n",
    "        for i, dist in enumerate(self._dists):\n",
    "            info = {}\n",
    "            for key in dist.dist_info_keys():\n",
    "                info[key] = dist_info[f'dist{i}_{key}']\n",
    "            split_infos.append(info)\n",
    "        return split_infos\n",
    "        \n",
    "    def join_dist_infos(self, dist_infos: List[Dict[str, tf.Tensor]]) -> Dict[str, tf.Tensor]:\n",
    "        \"\"\"Combina dicionários de parâmetros em um único dicionário.\"\"\"\n",
    "        joint_info = {}\n",
    "        for i, (dist, info) in enumerate(zip(self._dists, dist_infos)):\n",
    "            for key in dist.dist_info_keys():\n",
    "                joint_info[f'dist{i}_{key}'] = info[key]\n",
    "        return joint_info\n",
    "        \n",
    "    def logli(self, x_var: tf.Tensor, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        split_infos = self.split_dist_info(dist_info)\n",
    "        dims = [d.dim for d in self._dists]\n",
    "        split_x = tf.split(x_var, dims, axis=1)\n",
    "        return tf.add_n([d.logli(x, i) for d, x, i in zip(self._dists, split_x, split_infos)])\n",
    "        \n",
    "    def sample(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        split_infos = self.split_dist_info(dist_info)\n",
    "        samples = [d.sample(i) for d, i in zip(self._dists, split_infos)]\n",
    "        return tf.concat(samples, axis=1)\n",
    "        \n",
    "    def prior_dist_info(self, batch_size: int) -> Dict[str, tf.Tensor]:\n",
    "        dist_infos = [d.prior_dist_info(batch_size) for d in self._dists]\n",
    "        return self.join_dist_infos(dist_infos)\n",
    "        \n",
    "    def entropy(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        split_infos = self.split_dist_info(dist_info)\n",
    "        return tf.add_n([d.entropy(i) for d, i in zip(self._dists, split_infos)])\n",
    "        \n",
    "    def kl(self, p: Dict[str, tf.Tensor], q: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        p_split = self.split_dist_info(p)\n",
    "        q_split = self.split_dist_info(q)\n",
    "        return tf.add_n([d.kl(pi, qi) for d, pi, qi in zip(self._dists, p_split, q_split)])\n",
    "        \n",
    "    def dist_info_keys(self) -> List[str]:\n",
    "        keys = []\n",
    "        for i, dist in enumerate(self._dists):\n",
    "            for key in dist.dist_info_keys():\n",
    "                keys.append(f'dist{i}_{key}')\n",
    "        return keys\n",
    "        \n",
    "    def activate_dist(self, flat_dist: tf.Tensor) -> Dict[str, tf.Tensor]:\n",
    "        dist_infos = []\n",
    "        sizes = [d.dist_flat_dim for d in self._dists]\n",
    "        split_flat = tf.split(flat_dist, sizes, axis=1)\n",
    "        for dist, flat in zip(self._dists, split_flat):\n",
    "            dist_infos.append(dist.activate_dist(flat))\n",
    "        return self.join_dist_infos(dist_infos)\n",
    "\n",
    "    def prior_dist_info(self, batch_size: int) -> Dict[str, tf.Tensor]:\n",
    "        infos = [d.prior_dist_info(batch_size) for d in self._dists]  \n",
    "        return self.join_dist_infos(infos)\n",
    "\n",
    "\n",
    "# Modelos Keras \n",
    "#Gerador DCGAN-64: projeção → reshape → 4 transposed‐convs com batch norm + ReLU, final em tanh.\n",
    "def build_generator(z_dim: int, img_shape):\n",
    "    \"\"\"Gerador DCGAN‑64 clássico (4× upsampling → 64×64).\"\"\"\n",
    "    h, w, c = img_shape  # h==w==64\n",
    "\n",
    "    inp = layers.Input(shape=(z_dim,))\n",
    "\n",
    "    # 1) projeção + reshape → 4×4×512\n",
    "    x = layers.Dense(4 * 4 * 512, use_bias=False)(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Reshape((4, 4, 512))(x)\n",
    "\n",
    "    # 2) 8×8×256\n",
    "    x = layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # 3) 16×16×128\n",
    "    x = layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # 4) 32×32×64\n",
    "    x = layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # 5) 64×64×c\n",
    "    x = layers.Conv2DTranspose(c, kernel_size=4, strides=2, padding='same', activation='tanh')(x)\n",
    "\n",
    "    return Model(inp, x, name='Generator')\n",
    "\n",
    "\n",
    "def build_discriminator_q(img_shape, cat_dim, cont_dim):\n",
    "    inp = layers.Input(shape=img_shape)\n",
    "    x = layers.Conv2D(64, 4, 2, 'same')(inp)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    x = layers.Conv2D(128, 4, 2, 'same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(1024)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    # discriminator\n",
    "    d_out = layers.Dense(1, activation='sigmoid', name='d_out')(x)\n",
    "\n",
    "    # Q‐network: duas saídas\n",
    "    q_cat_logits = layers.Dense(cat_dim, name='q_cat_logits')(x)\n",
    "    q_cont_params = layers.Dense(cont_dim * 2, name='q_cont_params')(x)\n",
    "\n",
    "    return Model(inp, [d_out, q_cat_logits, q_cont_params], name='Discriminator_Q')\n",
    "\n",
    "\n",
    "# FID\n",
    "_inception = tf.keras.applications.InceptionV3(include_top=False, pooling='avg', input_shape=(299, 299, 3))\n",
    "\n",
    "FID_BATCH = 1024  \n",
    "\n",
    "def _get_inception_activations(img_uint8, bs: int = FID_BATCH):\n",
    "    \"\"\"Extrai ativações do pool-3 da Inception em minibatches para não estourar RAM.\"\"\"\n",
    "    acts = []\n",
    "    for i in range(0, len(img_uint8), bs):\n",
    "        batch = img_uint8[i:i + bs]\n",
    "        batch = tf.image.resize(batch, (299, 299))\n",
    "        batch = tf.keras.applications.inception_v3.preprocess_input(tf.cast(batch, tf.float32))\n",
    "        acts.append(_inception(batch, training=False))\n",
    "    return tf.concat(acts, axis=0).numpy()\n",
    "\n",
    "def fid_np(real_uint8, gen_uint8):\n",
    "    act1, act2 = _get_inception_activations(real_uint8), _get_inception_activations(gen_uint8)\n",
    "    mu1, mu2 = act1.mean(0), act2.mean(0)\n",
    "    sigma1, sigma2 = np.cov(act1, rowvar=False), np.cov(act2, rowvar=False)\n",
    "    covmean = sqrtm(sigma1 @ sigma2)\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    return float(np.sum((mu1 - mu2) ** 2) + np.trace(sigma1 + sigma2 - 2 * covmean))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#métricas\n",
    "def hsic(x: tf.Tensor, y: tf.Tensor):\n",
    "    \"\"\"Calcula o Hilbert-Schmidt Independence Criterion (HSIC)\"\"\"\n",
    "    m = tf.cast(tf.shape(x)[0], tf.float32)\n",
    "    K = tf.matmul(x, x, transpose_b=True)\n",
    "    L = tf.matmul(y, y, transpose_b=True)\n",
    "    H = tf.eye(m) - tf.ones((m, m)) / m\n",
    "    KH = tf.matmul(K, H)\n",
    "    LH = tf.matmul(L, H)\n",
    "    return tf.linalg.trace(tf.matmul(KH, LH)) / (m * m)\n",
    "\n",
    "def mig(codes, factors):\n",
    "    \"\"\"Calcula o Mutual Information Gap (MIG)\"\"\"\n",
    "    # Simplificada\n",
    "    mi_matrix = np.zeros((factors.shape[1], codes.shape[1]))\n",
    "    for i in range(factors.shape[1]):\n",
    "        for j in range(codes.shape[1]):\n",
    "            mi_matrix[i,j] = mutual_info_score(factors[:,i], codes[:,j])\n",
    "    \n",
    "    sorted_mi = np.sort(mi_matrix, axis=0)[::-1]\n",
    "    return np.mean((sorted_mi[0,:] - sorted_mi[1,:]) / sorted_mi[0,:])\n",
    "\n",
    "def sap_score(codes, factors):\n",
    "    \"\"\"Calcula o Separated Attribute Predictability (SAP)\"\"\"\n",
    "    # Simplificada\n",
    "    scores = []\n",
    "    for i in range(factors.shape[1]):\n",
    "        pred_scores = []\n",
    "        for j in range(codes.shape[1]):\n",
    "            pred_scores.append(np.abs(pearsonr(factors[:,i], codes[:,j])[0]))\n",
    "        scores.append(np.mean(pred_scores))\n",
    "    return np.mean(scores)\n",
    "\n",
    "def quase_ortogo(z: tf.Tensor, eps=1e-5):\n",
    "    \"\"\"Calcula métricas de quase-ortogonalidade para códigos latentes\"\"\"\n",
    "    # 1. Normalizar os vetores\n",
    "    zn = tf.math.l2_normalize(z, axis=0)\n",
    "    \n",
    "    # 2. Verificar desvio padrão\n",
    "    std = tf.math.reduce_std(zn, axis=0)\n",
    "    if tf.reduce_any(std < 1e-6):\n",
    "        return False, float(\"nan\"), float(\"nan\"), float(\"nan\")\n",
    "    \n",
    "    # 3. Calcular matriz de Gram\n",
    "    G = tf.matmul(zn, zn, transpose_a=True)\n",
    "    \n",
    "    # 4. Zerar diagonal\n",
    "    mask = tf.eye(tf.shape(G)[0], dtype=tf.bool)\n",
    "    G = tf.where(mask, tf.zeros_like(G), G)\n",
    "    \n",
    "    # 5. Calcular correlações máximas e mínimas\n",
    "    max_corr = tf.reduce_max(tf.abs(G))\n",
    "    min_corr = tf.reduce_min(tf.abs(G))\n",
    "    \n",
    "    # 6. Calcular proporção de pares abaixo de epsilon\n",
    "    below_eps = tf.reduce_sum(tf.cast(tf.abs(G) < eps, dtype=tf.float32))\n",
    "    total_pairs = tf.cast(tf.size(G) - tf.shape(G)[0], tf.float32)\n",
    "    prop = below_eps / total_pairs\n",
    "    \n",
    "    return min_corr.numpy(), max_corr.numpy(), prop.numpy()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import mutual_info_score\n",
    "from scipy.stats import pearsonr\n",
    "\n",
    "# InfoGAN Trainer \n",
    "class InfoGANTrainer:\n",
    "    def __init__(self, G, DQ, latent_dist: Product, dataset: CelebADataset, batch_size=64,\n",
    "             info_coeff=1.0, log_dir='logs', ckpt_dir='ckpt', snapshot=1000, max_iter=100_000,\n",
    "             noise_dim=62, cat_dim=10, cont_dim=2):\n",
    "        self.G, self.DQ = G, DQ\n",
    "        self.latent_dist, self.dataset = latent_dist, dataset\n",
    "        self.bs = batch_size\n",
    "        self.info_coeff = info_coeff\n",
    "        self.snapshot = snapshot\n",
    "        self.max_iter = max_iter\n",
    "        self.noise_dim = noise_dim\n",
    "        self.cat_dim   = cat_dim\n",
    "        self.cont_dim  = cont_dim\n",
    "\n",
    "        self.log_dir, self.ckpt_dir = Path(log_dir), Path(ckpt_dir)\n",
    "        gpus = tf.config.list_physical_devices('GPU')\n",
    "        if gpus:\n",
    "            try:\n",
    "                tf.config.experimental.set_memory_growth(gpus[0], True)\n",
    "                tf.config.optimizer.set_jit(True)  # Ativa XLA\n",
    "            except RuntimeError as e:\n",
    "                print(f\"Erro ao configurar GPU: {e}\")\n",
    "        else:\n",
    "            print(\"Executando em CPU\")\n",
    "            \n",
    "        mkdir_p(self.log_dir); mkdir_p(self.ckpt_dir)\n",
    "\n",
    "        self.d_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        self.g_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        self.q_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        \n",
    "        tf.config.optimizer.set_jit(True)  \n",
    "        #tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n",
    "\n",
    "        # Filtra os pesos da Q-head pelo nome\n",
    "        self.q_vars = [\n",
    "            v for v in self.DQ.trainable_variables\n",
    "            if 'q_cat_logits' in v.name or 'q_cont_params' in v.name\n",
    "        ]\n",
    "        \n",
    "        self.ckpt = tf.train.Checkpoint(\n",
    "            generator=self.G,\n",
    "            discriminator=self.DQ,\n",
    "            g_optimizer=self.g_opt,\n",
    "            d_optimizer=self.d_opt,\n",
    "            q_optimizer=self.q_opt\n",
    "        )\n",
    "        self.ckpt_manager = tf.train.CheckpointManager(\n",
    "            self.ckpt,\n",
    "            directory=str(self.ckpt_dir),\n",
    "            max_to_keep=5\n",
    "        )\n",
    "        \n",
    "        # Métricas \n",
    "        self.metric_hist = {\n",
    "            'iter': [],\n",
    "            'FID': [],\n",
    "            'HSIC': [],\n",
    "            'MIG': [],\n",
    "            'SAP': [],\n",
    "            'Ortho_min': [],\n",
    "            'Ortho_max': [],\n",
    "            'Ortho_prop': [],\n",
    "            'D_loss': [],\n",
    "            'G_loss': [],\n",
    "            'MI_loss': []\n",
    "        }\n",
    "        \n",
    "        self.metric_path = self.log_dir / 'metricsGAN.csv'\n",
    "        \n",
    "        # Restaurar o último checkpoint se existir\n",
    "        self.start_iter = 0\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            self.ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Checkpoint restaurado: {self.ckpt_manager.latest_checkpoint}\")\n",
    "            # Extrair o número da iteração do nome do checkpoint\n",
    "            import re\n",
    "            match = re.search(r'ckpt-(\\d+)', self.ckpt_manager.latest_checkpoint)\n",
    "            if match:\n",
    "                self.start_iter = int(match.group(1))\n",
    "                print(f\"Continuando do passo {self.start_iter}\")\n",
    "\n",
    "    def _mi_loss(self, z_reg, q_cat_logits, q_cont_params):\n",
    "        # separa z_reg em z_cat (one-hot) e z_cont (gaussiano)\n",
    "        z_cat  = z_reg[:, :self.cat_dim]\n",
    "        z_cont = z_reg[:, self.cat_dim:]\n",
    "\n",
    "        # 1) Cross-entropy (categórico)\n",
    "        cat_loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=z_cat,\n",
    "                logits=q_cat_logits\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 2) Log-likelihood gaussiano (contínuo)\n",
    "        mean_pred    = q_cont_params[:, :self.cont_dim]\n",
    "        log_std_pred = q_cont_params[:, self.cont_dim:]\n",
    "        std_pred     = tf.exp(log_std_pred)\n",
    "\n",
    "        eps = (z_cont - mean_pred) / (std_pred + TINY)\n",
    "        logli_per_dim = -0.5 * (tf.math.log(2. * np.pi) + 2. * log_std_pred + tf.square(eps))\n",
    "        logli = tf.reduce_sum(logli_per_dim, axis=1)\n",
    "        cont_loss = -tf.reduce_mean(logli)  \n",
    "\n",
    "        return cat_loss + cont_loss\n",
    "\n",
    "    def _evaluate_metrics(self, n_samples=1000):\n",
    "        \"\"\"Calcula todas as métricas usando um conjunto de amostras\"\"\"\n",
    "        # Gerar amostras\n",
    "        real_flat = self.dataset.next_batch(n_samples)\n",
    "        z = self.latent_dist.sample_prior(n_samples)\n",
    "        gen_imgs = self.G(z, training=False).numpy()\n",
    "        \n",
    "        # 1. FID\n",
    "        #fid = fid_np(self.dataset.inverse_transform(real_flat),self.dataset.inverse_transform(gen_imgs.reshape(real_flat.shape))\n",
    "        \n",
    "        # 2. HSIC entre variáveis latentes\n",
    "        hsic_val = hsic(z[:, :self.noise_dim], z[:, self.noise_dim:]).numpy()\n",
    "        \n",
    "        # 3. MIG e SAP \n",
    "        \n",
    "        try:\n",
    "            mig_val = mig(z.numpy(), real_flat.numpy())\n",
    "            sap_val = sap_score(z.numpy(), real_flat.numpy())\n",
    "        except:\n",
    "            mig_val, sap_val = float('nan'), float('nan')\n",
    "        \n",
    "        # 4. Quase-ortogonalidade\n",
    "        ortho_min, ortho_max, ortho_prop = quase_orto(z)\n",
    "            \n",
    "        return {\n",
    "            'HSIC': hsic_val,\n",
    "            'MIG': mig_val,\n",
    "            'SAP': sap_val,\n",
    "            'Ortho_min': ortho_min,\n",
    "            'Ortho_max': ortho_max,\n",
    "            'Ortho_prop': ortho_prop\n",
    "        }\n",
    "            \n",
    "            \n",
    "    @tf.function\n",
    "    def _train_step(self, real_imgs):\n",
    "        # 1) Amostra z\n",
    "        z = self.latent_dist.sample_prior(self.bs)\n",
    "\n",
    "        # Atualiza Discriminador\n",
    "        with tf.GradientTape() as d_tape:\n",
    "            fake = self.G(z, training=True)\n",
    "            d_real, q_cat_logits, q_cont_params = self.DQ(real_imgs, training=True)\n",
    "            d_fake, _, _ = self.DQ(fake, training=True)\n",
    "            d_loss = -tf.reduce_mean(tf.math.log(d_real + TINY) + tf.math.log(1 - d_fake + TINY))\n",
    "\n",
    "        # Calcula gradientes apenas para o Discriminador (excluindo Q)\n",
    "        d_vars = [v for v in self.DQ.trainable_variables \n",
    "                  if 'q_cat_logits' not in v.name and 'q_cont_params' not in v.name]\n",
    "        d_grads = d_tape.gradient(d_loss, d_vars)\n",
    "        self.d_opt.apply_gradients(zip(d_grads, d_vars))\n",
    "\n",
    "        # Atualiza Gerador e Rede Q juntos\n",
    "        with tf.GradientTape(persistent=True) as g_tape:\n",
    "            fake = self.G(z, training=True)\n",
    "            d_fake, q_cat_logits, q_cont_params = self.DQ(fake, training=True)\n",
    "\n",
    "            # Loss adversarial\n",
    "            g_adv_loss = -tf.reduce_mean(tf.math.log(d_fake + TINY))\n",
    "\n",
    "            # Loss de informação mútua\n",
    "            z_reg = z[:, self.noise_dim:]\n",
    "            mi_loss = self._mi_loss(z_reg, q_cat_logits, q_cont_params)\n",
    "\n",
    "            g_loss = g_adv_loss + self.info_coeff * mi_loss\n",
    "\n",
    "        # Gradientes para o Gerador\n",
    "        g_grads = g_tape.gradient(g_loss, self.G.trainable_variables)\n",
    "        self.g_opt.apply_gradients(zip(g_grads, self.G.trainable_variables))\n",
    "\n",
    "        # Gradientes para a Rede Q (apenas MI loss)\n",
    "        q_grads = g_tape.gradient(mi_loss, self.q_vars)\n",
    "        self.q_opt.apply_gradients(zip(q_grads, self.q_vars))\n",
    "\n",
    "        del g_tape  \n",
    "\n",
    "        return d_loss, g_adv_loss, mi_loss\n",
    "\n",
    "    def train(self):\n",
    "        for step in tqdm(range(self.start_iter + 1, self.max_iter + 1), \n",
    "                        initial=self.start_iter + 1, \n",
    "                        total=self.max_iter):\n",
    "            real_flat = self.dataset.next_batch(self.bs)\n",
    "            real_imgs = real_flat.reshape((-1,) + self.dataset.image_shape)\n",
    "            d_loss, g_loss, mi = self._train_step(real_imgs)\n",
    "\n",
    "            if step % self.snapshot == 0:\n",
    "                # Avaliar todas as métricas\n",
    "                metrics = self._evaluate_metrics()\n",
    "                \n",
    "                # Atualizar histórico de métricas\n",
    "                self.metric_hist['iter'].append(step)\n",
    "                self.metric_hist['FID'].append(metrics['FID'])\n",
    "                self.metric_hist['HSIC'].append(metrics['HSIC'])\n",
    "                self.metric_hist['MIG'].append(metrics['MIG'])\n",
    "                self.metric_hist['SAP'].append(metrics['SAP'])\n",
    "                self.metric_hist['Ortho_min'].append(metrics['Ortho_min'])\n",
    "                self.metric_hist['Ortho_max'].append(metrics['Ortho_max'])\n",
    "                self.metric_hist['Ortho_prop'].append(metrics['Ortho_prop'])\n",
    "                self.metric_hist['D_loss'].append(float(d_loss))\n",
    "                self.metric_hist['G_loss'].append(float(g_loss))\n",
    "                self.metric_hist['MI_loss'].append(float(mi))\n",
    "                \n",
    "                # Salvar métricas\n",
    "                pd.DataFrame(self.metric_hist).to_csv(self.metric_path, index=False)\n",
    "                \n",
    "                # Salvar checkpoint\n",
    "                self.ckpt_manager.save(checkpoint_number=step)\n",
    "                \n",
    "                # Gerar amostras visuais\n",
    "                sample_dir = self.log_dir / f'samples_step_{step:06d}'\n",
    "                self.generate_samples(n_samples=16, output_dir=str(sample_dir))\n",
    "                \n",
    "                print(f\"\\nStep {step}:\")\n",
    "                \n",
    "                print(f\"  HSIC: {metrics['HSIC']:.4f}\")\n",
    "                print(f\"  MIG: {metrics['MIG']:.4f}\")\n",
    "                print(f\"  SAP: {metrics['SAP']:.4f}\")\n",
    "                print(f\"  Ortho: min={metrics['Ortho_min']:.4f}, max={metrics['Ortho_max']:.4f}, prop={metrics['Ortho_prop']:.4f}\")\n",
    "                print(f\"  Losses: D={d_loss:.4f}, G={g_loss:.4f}, MI={mi:.4f}\")\n",
    "                print(f\"  Checkpoint salvo em {self.ckpt_dir}/ckpt-{step}\")\n",
    "                print(f\"  Amostras salvas em {sample_dir}\")\n",
    "            \n",
    "            if step + 1 == self.max_iter:\n",
    "                # Gerar amostras\n",
    "                real_flat = self.dataset.next_batch(n_samples)\n",
    "                z = self.latent_dist.sample_prior(n_samples)\n",
    "                gen_imgs = self.G(z, training=False).numpy()\n",
    "\n",
    "                # 1. FID\n",
    "                fid = fid_np(\n",
    "                    self.dataset.inverse_transform(real_flat),\n",
    "                    self.dataset.inverse_transform(gen_imgs.reshape(real_flat.shape)))\n",
    "                print(f\"FID: {fid}\")\n",
    "\n",
    "\n",
    "\n",
    "    def load_latest_checkpoint(self):\n",
    "        \"\"\"Carrega o último checkpoint disponível\"\"\"\n",
    "        if self.ckpt_manager.latest_checkpoint:\n",
    "            self.ckpt.restore(self.ckpt_manager.latest_checkpoint)\n",
    "            print(f\"Checkpoint carregado: {self.ckpt_manager.latest_checkpoint}\")\n",
    "            return True\n",
    "        print(\"Nenhum checkpoint encontrado para carregar\")\n",
    "        return False\n",
    "\n",
    "    def generate_samples(self, n_samples: int = 8, output_dir: str = 'samples', prefix: str = 'sample'):\n",
    "        \"\"\"Gera e salva amostras como imagens PNG.\"\"\"\n",
    "        mkdir_p(output_dir)\n",
    "        z = self.latent_dist.sample_prior(n_samples)\n",
    "        gen = self.G(z, training=False).numpy()\n",
    "        imgs_uint8 = self.dataset.inverse_transform(gen.reshape(n_samples, -1))\n",
    "\n",
    "        for i, img in enumerate(imgs_uint8):\n",
    "            path = os.path.join(output_dir, f\"{prefix}_{i}.png\")\n",
    "            Image.fromarray(img).save(path)\n",
    "\n",
    "        print(f\"Geradas e salvas {n_samples} amostras em '{output_dir}/'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                                       | 1/300000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executando em CPU\n",
      "Checkpoint restaurado: ckpt\\ckpt_1000-1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                          | 39/300000 [00:58<128:18:48,  1.54s/it]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-11-0da6cddcd873>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     14\u001b[0m trainer = InfoGANTrainer(G, DQ, latent_dist, data, batch_size=BATCH, max_iter=300000, snapshot=1000, \n\u001b[0;32m     15\u001b[0m                              noise_dim=noise_dim,cat_dim=cat_dim, cont_dim=cont_dim)\n\u001b[1;32m---> 16\u001b[1;33m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-10-2d7dace3891f>\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    194\u001b[0m             \u001b[0mreal_flat\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnext_batch\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mbs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    195\u001b[0m             \u001b[0mreal_imgs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mreal_flat\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimage_shape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 196\u001b[1;33m             \u001b[0md_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mg_loss\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmi\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_train_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mreal_imgs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    197\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    198\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msnapshot\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    883\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    884\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 885\u001b[1;33m         \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    886\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    887\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m_call\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    915\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    916\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 917\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    918\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    919\u001b[0m       \u001b[1;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   3037\u001b[0m       (graph_function,\n\u001b[0;32m   3038\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[1;32m-> 3039\u001b[1;33m     return graph_function._call_flat(\n\u001b[0m\u001b[0;32m   3040\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[0;32m   3041\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1961\u001b[0m         and executing_eagerly):\n\u001b[0;32m   1962\u001b[0m       \u001b[1;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1963\u001b[1;33m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[0;32m   1964\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0;32m   1965\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    589\u001b[0m       \u001b[1;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    590\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 591\u001b[1;33m           outputs = execute.execute(\n\u001b[0m\u001b[0;32m    592\u001b[0m               \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    593\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda3\\lib\\site-packages\\tensorflow\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     57\u001b[0m   \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 59\u001b[1;33m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[0;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[0;32m     61\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "BATCH = 64\n",
    "IMG_SHAPE = (64, 64, 3)\n",
    "    \n",
    "#Parametros para latente (dimensão do vetor \\Re^(noise_dim + cat_dim + cont_dim))\n",
    "noise_dim, cat_dim, cont_dim = 62, 10, 2\n",
    "    \n",
    "# O espaço latente modelado pelo objeto Product que agrupa três distribuições:\n",
    "latent_dist = Product([Gaussian(noise_dim, fix_std=True),Categorical(cat_dim),Gaussian(cont_dim)])\n",
    "\n",
    "G = build_generator(noise_dim + cat_dim + cont_dim, IMG_SHAPE)\n",
    "DQ = build_discriminator_q(IMG_SHAPE, cat_dim, cont_dim)\n",
    "data = CelebADataset('.', IMG_SHAPE)\n",
    "\n",
    "trainer = InfoGANTrainer(G, DQ, latent_dist, data, batch_size=BATCH, max_iter=300000, snapshot=1000, \n",
    "                             noise_dim=noise_dim,cat_dim=cat_dim, cont_dim=cont_dim)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
