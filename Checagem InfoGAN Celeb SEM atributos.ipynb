{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# INFOGAN with metrics\n",
    "\n",
    "* souce: https://github.com/openai/InfoGAN/tree/master"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install prettytensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install progressbar"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install python-dateutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sem atributos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import os, math, json, errno, time\n",
    "from pathlib import Path\n",
    "import itertools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "from scipy.linalg import sqrtm\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import layers, Model\n",
    "from PIL import Image\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Utils ----------------------------------------------------------------------\n",
    "# ----------------------------------------------------------------------------\n",
    "TINY = 1e-8\n",
    "floatX = np.float32\n",
    "\n",
    "def mkdir_p(path: str):\n",
    "    Path(path).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Dataset loader (CelebA sem atributos) --------------------------------------\n",
    "# ----------------------------------------------------------------------------\n",
    "class CelebADataset:\n",
    "    \"\"\"Loader simplificado da CelebA sem atributos.\"\"\"\n",
    "\n",
    "    def __init__(self, root_dir: str = '.', image_shape=(64, 64, 3), split_ratio=0.9):\n",
    "        self.image_shape = image_shape\n",
    "        self.image_dim = int(np.prod(image_shape))\n",
    "        base = Path(root_dir)\n",
    "\n",
    "        # Encontra todas as imagens no diretório\n",
    "        img_dir = base / 'img_align_celeba'\n",
    "        sub = img_dir / 'img_align_celeba'\n",
    "        if sub.is_dir():\n",
    "            img_dir = sub\n",
    "            \n",
    "        self.files = sorted([f for f in img_dir.glob('*.jpg')])\n",
    "        \n",
    "        # Split train\n",
    "        n_train = int(len(self.files) * split_ratio)\n",
    "        self.train_idx = np.arange(n_train)\n",
    "        np.random.shuffle(self.train_idx)\n",
    "        self.ptr = 0\n",
    "\n",
    "    def next_batch(self, batch_size):\n",
    "        if self.ptr + batch_size > len(self.train_idx):\n",
    "            np.random.shuffle(self.train_idx)\n",
    "            self.ptr = 0\n",
    "        sel = self.train_idx[self.ptr:self.ptr + batch_size]\n",
    "        self.ptr += batch_size\n",
    "        imgs = []\n",
    "        for i in sel:\n",
    "            img = Image.open(self.files[i]).resize(self.image_shape[:2])\n",
    "            imgs.append(np.asarray(img, np.float32) / 127.5 - 1.0)\n",
    "        x = np.stack(imgs).reshape(batch_size, -1)\n",
    "        return x\n",
    "\n",
    "    def inverse_transform(self, flat):\n",
    "        imgs = flat.reshape((-1,) + self.image_shape)\n",
    "        return ((imgs + 1.) * 127.5).clip(0, 255).astype(np.uint8)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Distributions (TF-2.x) -----------------------------------------------------\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "from typing import Dict, List, Union\n",
    "\n",
    "TINY = 1e-8\n",
    "floatX = np.float32\n",
    "\n",
    "class Distribution:\n",
    "    \"\"\"Classe base abstrata para distribuições latentes no InfoGAN.\"\"\"\n",
    "    \n",
    "    @property\n",
    "    def dim(self) -> int:\n",
    "        \"\"\"Dimensão da variável aleatória (tamanho do vetor de saída).\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @property\n",
    "    def dist_flat_dim(self) -> int:\n",
    "        \"\"\"Dimensão do vetor plano de parâmetros da distribuição.\"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    @property\n",
    "    def effective_dim(self) -> int:\n",
    "        \"\"\"Dimensão efetiva para cálculo de mutual information.\"\"\"\n",
    "        return self.dim\n",
    "        \n",
    "    def logli(self, x_var: tf.Tensor, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Calcula o log-likelihood log p(x|θ) para cada amostra no batch.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def sample(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Gera amostras da distribuição parametrizada por dist_info.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def sample_prior(self, batch_size: int) -> tf.Tensor:\n",
    "        \"\"\"Gera amostras da distribuição prévia (prior).\"\"\"\n",
    "        return self.sample(self.prior_dist_info(batch_size))\n",
    "        \n",
    "    def prior_dist_info(self, batch_size: int) -> Dict[str, tf.Tensor]:\n",
    "        \"\"\"\n",
    "        Retorna os parâmetros da distribuição prévia (prior).\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def entropy(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Calcula a entropia H[p(x|θ)] para cada amostra no batch.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def marginal_entropy(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Calcula a entropia marginal (média sobre o batch).\n",
    "        \"\"\"\n",
    "        # Implementação padrão: média dos parâmetros no batch\n",
    "        avg_dist_info = {\n",
    "            k: tf.tile(tf.reduce_mean(v, axis=0, keepdims=True), \n",
    "            [tf.shape(v)[0], *[1]*(len(v.shape)-1)])\n",
    "            for k, v in dist_info.items()\n",
    "        }\n",
    "        return self.entropy(avg_dist_info)\n",
    "        \n",
    "    def marginal_logli(self, x_var: tf.Tensor, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Log-likelihood usando parâmetros marginais (média no batch).\n",
    "        \"\"\"\n",
    "        avg_dist_info = {\n",
    "            k: tf.tile(tf.reduce_mean(v, axis=0, keepdims=True), \n",
    "            [tf.shape(v)[0], *[1]*(len(v.shape)-1)])\n",
    "            for k, v in dist_info.items()\n",
    "        }\n",
    "        return self.logli(x_var, avg_dist_info)\n",
    "        \n",
    "    def kl(self, p: Dict[str, tf.Tensor], q: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Calcula a divergência KL entre duas distribuições (KL(p||q)).\n",
    "        Implementação padrão usando entropia cruzada e entropia:\n",
    "        KL(p||q) = H(p,q) - H(p)\n",
    "        \"\"\"\n",
    "        cross_entropy = -self.logli(p['samples'], q)\n",
    "        entropy = self.entropy(p)\n",
    "        return cross_entropy - entropy\n",
    "        \n",
    "    def dist_info_keys(self) -> List[str]:\n",
    "        \"\"\"\n",
    "        Lista de chaves no dicionário de parâmetros da distribuição.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def activate_dist(self, flat_dist: tf.Tensor) -> Dict[str, tf.Tensor]:\n",
    "        \"\"\"\n",
    "        Converte um vetor plano de parâmetros em parâmetros da distribuição.\n",
    "        \"\"\"\n",
    "        raise NotImplementedError\n",
    "        \n",
    "    def nonreparam_logli(self, x_var: tf.Tensor, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        \"\"\"\n",
    "        Log-likelihood para distribuições sem reparameterization trick.\n",
    "        \"\"\"\n",
    "        return tf.zeros_like(x_var[:, 0])\n",
    "\n",
    "class Categorical(Distribution):\n",
    "    \"\"\"Distribuição categórica (one-hot) para variáveis latentes discretas.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int):\n",
    "        self._dim = dim\n",
    "        \n",
    "    @property\n",
    "    def dim(self) -> int:\n",
    "        return self._dim\n",
    "        \n",
    "    @property\n",
    "    def dist_flat_dim(self) -> int:\n",
    "        return self._dim\n",
    "        \n",
    "    @property\n",
    "    def effective_dim(self) -> int:\n",
    "        return 1  # Apesar de ter N categorias, a dimensão efetiva é 1\n",
    "        \n",
    "    def logli(self, x_var: tf.Tensor, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        prob = dist_info['prob']\n",
    "        return tf.reduce_sum(x_var * tf.math.log(prob + TINY), axis=1)\n",
    "        \n",
    "    def sample(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        prob = dist_info['prob']\n",
    "        # Gera amostras usando Gumbel-Softmax trick para diferenciabilidade\n",
    "        logits = tf.math.log(prob + TINY)\n",
    "        gumbel_noise = -tf.math.log(-tf.math.log(tf.random.uniform(tf.shape(logits), dtype=floatX)))\n",
    "        samples = tf.nn.softmax((logits + gumbel_noise) / 1.0)  # temperatura=1.0\n",
    "        return samples\n",
    "        \n",
    "    def prior_dist_info(self, batch_size: int) -> Dict[str, tf.Tensor]:\n",
    "        prob = tf.ones((batch_size, self.dim), dtype=floatX) / self.dim\n",
    "        return {'prob': prob}\n",
    "        \n",
    "    def entropy(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        prob = dist_info['prob']\n",
    "        return -tf.reduce_sum(prob * tf.math.log(prob + TINY), axis=1)\n",
    "        \n",
    "    def kl(self, p: Dict[str, tf.Tensor], q: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        p_prob, q_prob = p['prob'], q['prob']\n",
    "        return tf.reduce_sum(p_prob * (tf.math.log(p_prob + TINY) - tf.math.log(q_prob + TINY)), axis=1)\n",
    "        \n",
    "    def dist_info_keys(self) -> List[str]:\n",
    "        return ['prob']\n",
    "        \n",
    "    def activate_dist(self, flat_dist: tf.Tensor) -> Dict[str, tf.Tensor]:\n",
    "        return {'prob': tf.nn.softmax(flat_dist)}\n",
    "\n",
    "class Gaussian(Distribution):\n",
    "    \"\"\"Distribuição gaussiana para variáveis latentes contínuas.\"\"\"\n",
    "    \n",
    "    def __init__(self, dim: int, fix_std: bool = False):\n",
    "        self._dim = dim\n",
    "        self._fix_std = fix_std\n",
    "        \n",
    "    @property\n",
    "    def dim(self) -> int:\n",
    "        return self._dim\n",
    "        \n",
    "    @property\n",
    "    def dist_flat_dim(self) -> int:\n",
    "        return self._dim * 2 if not self._fix_std else self._dim\n",
    "        \n",
    "    def logli(self, x_var: tf.Tensor, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        mean, std = dist_info['mean'], dist_info['stddev']\n",
    "        z = (x_var - mean) / (std + TINY)\n",
    "        return tf.reduce_sum(-0.5 * (np.log(2 * np.pi) + tf.math.log(std + TINY) + 0.5 * tf.square(z)), axis=1)\n",
    "        \n",
    "    def sample(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        mean, std = dist_info['mean'], dist_info['stddev']\n",
    "        return mean + std * tf.random.normal(tf.shape(mean))\n",
    "        \n",
    "    def prior_dist_info(self, batch_size: int) -> Dict[str, tf.Tensor]:\n",
    "        mean = tf.zeros((batch_size, self.dim), dtype=floatX)\n",
    "        std = tf.ones((batch_size, self.dim), dtype=floatX)\n",
    "        return {'mean': mean, 'stddev': std}\n",
    "        \n",
    "    def entropy(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        std = dist_info['stddev']\n",
    "        return tf.reduce_sum(0.5 * np.log(2 * np.pi * np.e) + tf.math.log(std + TINY), axis=1)\n",
    "        \n",
    "    def kl(self, p: Dict[str, tf.Tensor], q: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        p_mean, p_std = p['mean'], p['stddev']\n",
    "        q_mean, q_std = q['mean'], q['stddev']\n",
    "        return tf.reduce_sum(\n",
    "            tf.math.log(q_std + TINY) - tf.math.log(p_std + TINY) + \n",
    "            (tf.square(p_std) + tf.square(p_mean - q_mean)) / (2 * tf.square(q_std + TINY)) - 0.5,\n",
    "            axis=1\n",
    "        )\n",
    "        \n",
    "    def dist_info_keys(self) -> List[str]:\n",
    "        return ['mean', 'stddev']\n",
    "        \n",
    "    def activate_dist(self, flat_dist: tf.Tensor) -> Dict[str, tf.Tensor]:\n",
    "        mean = flat_dist[:, :self.dim]\n",
    "        if self._fix_std:\n",
    "            std = tf.ones_like(mean)\n",
    "        else:\n",
    "            std = tf.sqrt(tf.exp(flat_dist[:, self.dim:]))\n",
    "        return {'mean': mean, 'stddev': std}\n",
    "\n",
    "class Product(Distribution):\n",
    "    \"\"\"Produto de distribuições para combinar diferentes tipos de variáveis latentes.\"\"\"\n",
    "    \n",
    "    def __init__(self, dists: List[Distribution]):\n",
    "        self._dists = dists\n",
    "        \n",
    "    @property\n",
    "    def dim(self) -> int:\n",
    "        return sum(d.dim for d in self._dists)\n",
    "        \n",
    "    @property\n",
    "    def dist_flat_dim(self) -> int:\n",
    "        return sum(d.dist_flat_dim for d in self._dists)\n",
    "    @property\n",
    "    def dists(self):  \n",
    "        return self._dists\n",
    "    \n",
    "    @property\n",
    "    def effective_dim(self) -> int:\n",
    "        return sum(d.effective_dim for d in self._dists)\n",
    "        \n",
    "    def split_dist_info(self, dist_info: Dict[str, tf.Tensor]) -> List[Dict[str, tf.Tensor]]:\n",
    "        \"\"\"Divide um dicionário de parâmetros combinado em dicionários por distribuição.\"\"\"\n",
    "        split_infos = []\n",
    "        for i, dist in enumerate(self._dists):\n",
    "            info = {}\n",
    "            for key in dist.dist_info_keys():\n",
    "                info[key] = dist_info[f'dist{i}_{key}']\n",
    "            split_infos.append(info)\n",
    "        return split_infos\n",
    "        \n",
    "    def join_dist_infos(self, dist_infos: List[Dict[str, tf.Tensor]]) -> Dict[str, tf.Tensor]:\n",
    "        \"\"\"Combina dicionários de parâmetros em um único dicionário.\"\"\"\n",
    "        joint_info = {}\n",
    "        for i, (dist, info) in enumerate(zip(self._dists, dist_infos)):\n",
    "            for key in dist.dist_info_keys():\n",
    "                joint_info[f'dist{i}_{key}'] = info[key]\n",
    "        return joint_info\n",
    "        \n",
    "    def logli(self, x_var: tf.Tensor, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        split_infos = self.split_dist_info(dist_info)\n",
    "        dims = [d.dim for d in self._dists]\n",
    "        split_x = tf.split(x_var, dims, axis=1)\n",
    "        return tf.add_n([d.logli(x, i) for d, x, i in zip(self._dists, split_x, split_infos)])\n",
    "        \n",
    "    def sample(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        split_infos = self.split_dist_info(dist_info)\n",
    "        samples = [d.sample(i) for d, i in zip(self._dists, split_infos)]\n",
    "        return tf.concat(samples, axis=1)\n",
    "        \n",
    "    def prior_dist_info(self, batch_size: int) -> Dict[str, tf.Tensor]:\n",
    "        dist_infos = [d.prior_dist_info(batch_size) for d in self._dists]\n",
    "        return self.join_dist_infos(dist_infos)\n",
    "        \n",
    "    def entropy(self, dist_info: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        split_infos = self.split_dist_info(dist_info)\n",
    "        return tf.add_n([d.entropy(i) for d, i in zip(self._dists, split_infos)])\n",
    "        \n",
    "    def kl(self, p: Dict[str, tf.Tensor], q: Dict[str, tf.Tensor]) -> tf.Tensor:\n",
    "        p_split = self.split_dist_info(p)\n",
    "        q_split = self.split_dist_info(q)\n",
    "        return tf.add_n([d.kl(pi, qi) for d, pi, qi in zip(self._dists, p_split, q_split)])\n",
    "        \n",
    "    def dist_info_keys(self) -> List[str]:\n",
    "        keys = []\n",
    "        for i, dist in enumerate(self._dists):\n",
    "            for key in dist.dist_info_keys():\n",
    "                keys.append(f'dist{i}_{key}')\n",
    "        return keys\n",
    "        \n",
    "    def activate_dist(self, flat_dist: tf.Tensor) -> Dict[str, tf.Tensor]:\n",
    "        dist_infos = []\n",
    "        sizes = [d.dist_flat_dim for d in self._dists]\n",
    "        split_flat = tf.split(flat_dist, sizes, axis=1)\n",
    "        for dist, flat in zip(self._dists, split_flat):\n",
    "            dist_infos.append(dist.activate_dist(flat))\n",
    "        return self.join_dist_infos(dist_infos)\n",
    "\n",
    "    def prior_dist_info(self, batch_size: int) -> Dict[str, tf.Tensor]:\n",
    "        infos = [d.prior_dist_info(batch_size) for d in self._dists]  # Usando _dists\n",
    "        return self.join_dist_infos(infos)\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Modelos Keras --------------------------------------------------------------\n",
    "# (Mantido igual ao original)\n",
    "# ----------------------------------------------------------------------------\n",
    "#Gerador DCGAN-64: projeção → reshape → 4 transposed‐convs com batch norm + ReLU, final em tanh.\n",
    "def build_generator(z_dim: int, img_shape):\n",
    "    \"\"\"Gerador DCGAN‑64 clássico (4× upsampling → 64×64).\"\"\"\n",
    "    h, w, c = img_shape  # h==w==64\n",
    "\n",
    "    inp = layers.Input(shape=(z_dim,))\n",
    "\n",
    "    # 1) projeção + reshape → 4×4×512\n",
    "    x = layers.Dense(4 * 4 * 512, use_bias=False)(inp)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "    x = layers.Reshape((4, 4, 512))(x)\n",
    "\n",
    "    # 2) 8×8×256\n",
    "    x = layers.Conv2DTranspose(256, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # 3) 16×16×128\n",
    "    x = layers.Conv2DTranspose(128, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # 4) 32×32×64\n",
    "    x = layers.Conv2DTranspose(64, kernel_size=4, strides=2, padding='same', use_bias=False)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.ReLU()(x)\n",
    "\n",
    "    # 5) 64×64×c\n",
    "    x = layers.Conv2DTranspose(c, kernel_size=4, strides=2, padding='same', activation='tanh')(x)\n",
    "\n",
    "    return Model(inp, x, name='Generator')\n",
    "\n",
    "\n",
    "def build_discriminator_q(img_shape, cat_dim, cont_dim):\n",
    "    inp = layers.Input(shape=img_shape)\n",
    "    x = layers.Conv2D(64, 4, 2, 'same')(inp)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    x = layers.Conv2D(128, 4, 2, 'same')(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dense(1024)(x)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    # discriminator\n",
    "    d_out = layers.Dense(1, activation='sigmoid', name='d_out')(x)\n",
    "\n",
    "    # Q‐network: duas saídas\n",
    "    q_cat_logits = layers.Dense(cat_dim, name='q_cat_logits')(x)\n",
    "    q_cont_params = layers.Dense(cont_dim * 2, name='q_cont_params')(x)\n",
    "\n",
    "    return Model(inp, [d_out, q_cat_logits, q_cont_params], name='Discriminator_Q')\n",
    "\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# Métricas (Simplificadas - apenas FID) --------------------------------------\n",
    "# ----------------------------------------------------------------------------\n",
    "_inception = tf.keras.applications.InceptionV3(include_top=False, pooling='avg', input_shape=(299, 299, 3))\n",
    "\n",
    "FID_BATCH = 256  # nº de imagens por forward pass no Inception (evita OOM)\n",
    "\n",
    "def _get_inception_activations(img_uint8, bs: int = FID_BATCH):\n",
    "    \"\"\"Extrai ativações do pool-3 da Inception em minibatches para não estourar RAM.\"\"\"\n",
    "    acts = []\n",
    "    for i in range(0, len(img_uint8), bs):\n",
    "        batch = img_uint8[i:i + bs]\n",
    "        batch = tf.image.resize(batch, (299, 299))\n",
    "        batch = tf.keras.applications.inception_v3.preprocess_input(tf.cast(batch, tf.float32))\n",
    "        acts.append(_inception(batch, training=False))\n",
    "    return tf.concat(acts, axis=0).numpy()\n",
    "\n",
    "def fid_np(real_uint8, gen_uint8):\n",
    "    act1, act2 = _get_inception_activations(real_uint8), _get_inception_activations(gen_uint8)\n",
    "    mu1, mu2 = act1.mean(0), act2.mean(0)\n",
    "    sigma1, sigma2 = np.cov(act1, rowvar=False), np.cov(act2, rowvar=False)\n",
    "    covmean = sqrtm(sigma1 @ sigma2)\n",
    "    if np.iscomplexobj(covmean):\n",
    "        covmean = covmean.real\n",
    "    return float(np.sum((mu1 - mu2) ** 2) + np.trace(sigma1 + sigma2 - 2 * covmean))\n",
    "\n",
    "# ----------------------------------------------------------------------------\n",
    "# InfoGAN Trainer (Adaptado para dataset sem atributos) -----------------------\n",
    "# ----------------------------------------------------------------------------\n",
    "class InfoGANTrainer:\n",
    "    def __init__(self, G, DQ, latent_dist: Product, dataset: CelebADataset, batch_size=64,\n",
    "             info_coeff=1.0, log_dir='logs', ckpt_dir='ckpt', snapshot=1000, max_iter=100_000,\n",
    "             noise_dim=62, cat_dim=10, cont_dim=2):\n",
    "        self.G, self.DQ = G, DQ\n",
    "        self.latent_dist, self.dataset = latent_dist, dataset\n",
    "        self.bs = batch_size\n",
    "        self.info_coeff = info_coeff\n",
    "        self.snapshot = snapshot\n",
    "        self.max_iter = max_iter\n",
    "        self.noise_dim = noise_dim\n",
    "        self.cat_dim   = cat_dim\n",
    "        self.cont_dim  = cont_dim\n",
    "\n",
    "        self.log_dir, self.ckpt_dir = Path(log_dir), Path(ckpt_dir)\n",
    "        mkdir_p(self.log_dir); mkdir_p(self.ckpt_dir)\n",
    "\n",
    "        self.d_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        self.g_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        self.q_opt = tf.keras.optimizers.Adam(2e-4, beta_1=0.5)\n",
    "        \n",
    "        tf.config.optimizer.set_jit(True)  \n",
    "        tf.config.experimental.set_memory_growth(tf.config.list_physical_devices('GPU')[0], True)\n",
    "\n",
    "        # Filtra os pesos da Q-head pelo nome\n",
    "        self.q_vars = [\n",
    "            v for v in self.DQ.trainable_variables\n",
    "            if 'q_cat_logits' in v.name or 'q_cont_params' in v.name\n",
    "        ]\n",
    "\n",
    "        # Métricas simplificadas (apenas FID)\n",
    "        self.metric_hist = {'iter': [], 'FID': []}\n",
    "        self.metric_path = self.log_dir / 'metrics.csv'\n",
    "\n",
    "    def _mi_loss(self, z_reg, q_cat_logits, q_cont_params):\n",
    "        # separa z_reg em z_cat (one-hot) e z_cont (gaussiano)\n",
    "        z_cat  = z_reg[:, :self.cat_dim]\n",
    "        z_cont = z_reg[:, self.cat_dim:]\n",
    "\n",
    "        # 1) Cross-entropy (categórico)\n",
    "        cat_loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(\n",
    "                labels=z_cat,\n",
    "                logits=q_cat_logits\n",
    "            )\n",
    "        )\n",
    "\n",
    "        # 2) Log-likelihood gaussiano (contínuo)\n",
    "        mean_pred    = q_cont_params[:, :self.cont_dim]\n",
    "        log_std_pred = q_cont_params[:, self.cont_dim:]\n",
    "        std_pred     = tf.exp(log_std_pred)\n",
    "\n",
    "        eps = (z_cont - mean_pred) / (std_pred + TINY)\n",
    "        logli_per_dim = -0.5 * (tf.math.log(2. * np.pi) + 2. * log_std_pred + tf.square(eps))\n",
    "        logli = tf.reduce_sum(logli_per_dim, axis=1)\n",
    "        cont_loss = -tf.reduce_mean(logli)  # NLL\n",
    "\n",
    "        return cat_loss + cont_loss\n",
    "\n",
    "    \n",
    "    @tf.function\n",
    "    def _train_step(self, real_imgs):\n",
    "        # 1) Amostra z\n",
    "        z = self.latent_dist.sample_prior(self.bs)\n",
    "\n",
    "        # Atualiza Discriminador\n",
    "        with tf.GradientTape() as d_tape:\n",
    "            fake = self.G(z, training=True)\n",
    "            d_real, q_cat_logits, q_cont_params = self.DQ(real_imgs, training=True)\n",
    "            d_fake, _, _ = self.DQ(fake, training=True)\n",
    "            d_loss = -tf.reduce_mean(tf.math.log(d_real + TINY) + tf.math.log(1 - d_fake + TINY))\n",
    "\n",
    "        # Calcula gradientes apenas para o Discriminador (excluindo Q)\n",
    "        d_vars = [v for v in self.DQ.trainable_variables \n",
    "                  if 'q_cat_logits' not in v.name and 'q_cont_params' not in v.name]\n",
    "        d_grads = d_tape.gradient(d_loss, d_vars)\n",
    "        self.d_opt.apply_gradients(zip(d_grads, d_vars))\n",
    "\n",
    "        # Atualiza Gerador e Rede Q juntos\n",
    "        with tf.GradientTape(persistent=True) as g_tape:\n",
    "            fake = self.G(z, training=True)\n",
    "            d_fake, q_cat_logits, q_cont_params = self.DQ(fake, training=True)\n",
    "\n",
    "            # Loss adversarial\n",
    "            g_adv_loss = -tf.reduce_mean(tf.math.log(d_fake + TINY))\n",
    "\n",
    "            # Loss de informação mútua\n",
    "            z_reg = z[:, self.noise_dim:]\n",
    "            mi_loss = self._mi_loss(z_reg, q_cat_logits, q_cont_params)\n",
    "\n",
    "            g_loss = g_adv_loss + self.info_coeff * mi_loss\n",
    "\n",
    "        # Gradientes para o Gerador\n",
    "        g_grads = g_tape.gradient(g_loss, self.G.trainable_variables)\n",
    "        self.g_opt.apply_gradients(zip(g_grads, self.G.trainable_variables))\n",
    "\n",
    "        # Gradientes para a Rede Q (apenas MI loss)\n",
    "        q_grads = g_tape.gradient(mi_loss, self.q_vars)\n",
    "        self.q_opt.apply_gradients(zip(q_grads, self.q_vars))\n",
    "\n",
    "        del g_tape  # Importante quando persistent=True\n",
    "\n",
    "        return d_loss, g_adv_loss, mi_loss\n",
    "\n",
    "    def _evaluate_metrics(self):\n",
    "        \"\"\"Calcula apenas o FID (±1k amostras)\"\"\"\n",
    "        N_EVAL = 1000\n",
    "        real_flat = self.dataset.next_batch(N_EVAL)\n",
    "        z = self.latent_dist.sample_prior(N_EVAL)\n",
    "        gen_imgs = self.G(z, training=False).numpy()\n",
    "\n",
    "        fid = fid_np(\n",
    "            self.dataset.inverse_transform(real_flat),\n",
    "            self.dataset.inverse_transform(gen_imgs.reshape(real_flat.shape))\n",
    "        )\n",
    "        return fid\n",
    "\n",
    "    def train(self):\n",
    "        for step in tqdm(range(1, self.max_iter + 1)):\n",
    "            real_flat = self.dataset.next_batch(self.bs)\n",
    "            real_imgs = real_flat.reshape((-1,) + self.dataset.image_shape)\n",
    "            d_loss, g_loss, mi = self._train_step(real_imgs)\n",
    "\n",
    "            if step % self.snapshot == 0:\n",
    "                fid = self._evaluate_metrics()\n",
    "                self.metric_hist['iter'].append(step)\n",
    "                self.metric_hist['FID'].append(fid)\n",
    "                pd.DataFrame(self.metric_hist).to_csv(self.metric_path, index=False)\n",
    "                print(f\"Step {step}: FID {fid:.1f}\")\n",
    "\n",
    "    def generate_samples(self, n_samples: int = 8, output_dir: str = 'samples', prefix: str = 'sample'):\n",
    "        \"\"\"Gera e salva amostras como imagens PNG.\"\"\"\n",
    "        mkdir_p(output_dir)\n",
    "        z = self.latent_dist.sample_prior(n_samples)\n",
    "        gen = self.G(z, training=False).numpy()\n",
    "        imgs_uint8 = self.dataset.inverse_transform(gen.reshape(n_samples, -1))\n",
    "\n",
    "        for i, img in enumerate(imgs_uint8):\n",
    "            path = os.path.join(output_dir, f\"{prefix}_{i}.png\")\n",
    "            Image.fromarray(img).save(path)\n",
    "\n",
    "        print(f\"Geradas e salvas {n_samples} amostras em '{output_dir}/'.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-0da6cddcd873>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mdata\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mCelebADataset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'.'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mIMG_SHAPE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 14\u001b[1;33m trainer = InfoGANTrainer(G, DQ, latent_dist, data, batch_size=BATCH, max_iter=300000, snapshot=1000, \n\u001b[0m\u001b[0;32m     15\u001b[0m                              noise_dim=noise_dim,cat_dim=cat_dim, cont_dim=cont_dim)\n\u001b[0;32m     16\u001b[0m \u001b[0mtrainer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-8-36e2bc5ef446>\u001b[0m in \u001b[0;36m__init__\u001b[1;34m(self, G, DQ, latent_dist, dataset, batch_size, info_coeff, log_dir, ckpt_dir, snapshot, max_iter, noise_dim, cat_dim, cont_dim)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    463\u001b[0m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_jit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 464\u001b[1;33m         \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexperimental\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_memory_growth\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlist_physical_devices\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'GPU'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    465\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    466\u001b[0m         \u001b[1;31m# Filtra os pesos da Q-head pelo nome\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "BATCH = 64\n",
    "IMG_SHAPE = (64, 64, 3)\n",
    "    \n",
    "#Parametros para latente (dimensão do vetor \\Re^(noise_dim + cat_dim + cont_dim))\n",
    "noise_dim, cat_dim, cont_dim = 62, 10, 2\n",
    "    \n",
    "# O espaço latente modelado pelo objeto Product que agrupa três distribuições:\n",
    "latent_dist = Product([Gaussian(noise_dim, fix_std=True),Categorical(cat_dim),Gaussian(cont_dim)])\n",
    "\n",
    "G = build_generator(noise_dim + cat_dim + cont_dim, IMG_SHAPE)\n",
    "DQ = build_discriminator_q(IMG_SHAPE, cat_dim, cont_dim)\n",
    "data = CelebADataset('.', IMG_SHAPE)\n",
    "\n",
    "trainer = InfoGANTrainer(G, DQ, latent_dist, data, batch_size=BATCH, max_iter=300000, snapshot=1000, \n",
    "                             noise_dim=noise_dim,cat_dim=cat_dim, cont_dim=cont_dim)\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
